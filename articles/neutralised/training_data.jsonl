{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Event-driven architectures and event-based compute are distinct concepts that are often used together in serverless applications on AWS, but they are not the same thing.\n\nEvent-driven architectures involve decoupled, asynchronous processing of events across an application. In this architectural style, components of the system communicate by emitting and consuming events, rather than through direct method calls or shared memory. This approach promotes loose coupling, scalability, and flexibility, as components can be added, removed, or modified without affecting the entire system.\n\nOn the other hand, event-based compute refers to the execution model of AWS Lambda functions, where they are triggered by events rather than running continuously. Lambda functions are invoked in response to specific events, such as API calls, database updates, or file uploads, and they execute their code in a stateless, ephemeral environment. This event-driven execution model has significant implications for how code is written and how applications are designed in a serverless context.\n\nWhile event-driven architectures and event-based compute are often used together in serverless applications, they are separate concerns. An application can employ an event-driven architecture without necessarily using event-based compute (e.g., using message queues or event streams with traditional servers), and an application can use event-based compute without adopting a fully event-driven architecture (e.g., using Lambda functions for specific tasks but maintaining a more traditional, request-response flow for other parts of the system).\n\nThe patterns and practices used for event-driven architectures may not directly apply to event-based compute, and vice versa. Developers need to understand the nuances and implications of both concepts to effectively design and implement serverless applications on AWS.\n```"}, {"role": "assistant", "content": "# Event-Driven Architectures vs. Event-Based Compute in Serverless Applications\n\nAlex DeBrie  \nFounder, DeBrie Advisory\n\nI recently delivered serverless training to some engineers, and there was confusion between two concepts that come up in discussions of serverless architectures. On one hand, I describe AWS Lambda as event-based compute, which has significant implications for how you write the code and design the architecture in your serverless applications. On the other hand, many serverless applications use an event-driven architecture that relies on decoupled, asynchronous processing of events across your application. These two concepts -- event-driven architectures and event-based compute -- sound similar and are often used together in serverless applications on AWS, but they're not the same thing. Further, the patterns you use for one will not necessarily apply if you're not using the other.\n## KEY POINT\n\nThe event-based nature of Lambda compute is what fundamentally distinguishes Lambda from other computing paradigms and is what leads to the unique constraints and demands of serverless application developers.\n\nIn this post, we'll look at both event-driven architecture and event-based compute. We'll examine the key characteristics of each as well as the implications for you in your applications."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```AWS Lambda is known as event-based compute, and it greatly affects the way you create codes and design the architecture of serverless applications. Many serverless apps utilize an event-driven structure, relying on the uncoupled, asynchronous processing of events throughout the application. Though event-driven architectures and event-based compute may seem similar and are often jointly applied in serverless AWS applications, they're distinct. Moreover, the methods used for one won't necessarily work if not using the other.\n\nEvent-driven architectures are popular nowadays. They're defined by services that communicate asynchronously through events, thus setting them apart from other architectural paradigms. \n\nIf you've used a front end client contacting a back-end GraphQL API, a backend service calling another service through a REST API or RPC, you've utilized the request-response pattern. This connection method is quite common from one client to a service, typified by a synchronous flow where the client waits for a full feedback from the service about the request's outcome. \n\nThe synchronous path is straightforward because you attain immediate remarks on what ensued with your request. But depending on synchronous communication might have drawbacks as well: it might lower the overall performance of your app, especially when dealing with slower tasks like sending emails or generating reports. The availability of your services reduces too. With asynchronous patterns, these hindrances lessen. The services can still communicate, but won't anticipate an instant response. \n\nThe downstream service takes its time to process the communication without holding up that of the upstream service. Although this contributes to issues soft debugging, eventual consistency and more, it minimizes the downsides of synchronous communication. Consult Talia Nassi's post on the benefits of transitioning to event-driven architectures to understand more about request-response versus event-driven.\n```"}, {"role": "assistant", "content": "On one hand, I describe AWS Lambda as event-based compute, which has significant implications for how you write the code and design the architecture in your serverless applications. On the other hand, many serverless applications use an event-driven architecture that relies on decoupled, asynchronous processing of events across your application. These two concepts -- event-driven architectures and event-based compute -- sound similar and are often used together in serverless applications on AWS, but they're not the same thing. Further, the patterns you use for one will not necessarily apply if you're not using the other.\n## What is an event-driven architecture\n\nEvent-driven architectures are all the rage, so we'll start there.\n\nEvent-driven architectures are characterized by services that communicate (1) asynchronously (2) via events. These two elements distinguish them from other architecture patterns.\n### Event-driven architectures communicate asynchronously\n\nIf you've worked with a frontend client calling a backend GraphQL API or a backend service calling another service via a REST API or RPC, you've used the request-response pattern. This is a common pattern for communication from one client to a service. It is a synchronous flow where the client will wait for a full response from the service indicating the result of the request. A synchronous flow is simple as you get direct feedback on what happened from your request. However, relying on synchronous communication has costs as well. It can reduce the overall performance of your application, particularly when working with slower tasks like sending emails or generating reports. Further, the availability of your services goes down. If Service A relies on synchronous responses from Service B to operate, the uptime for Service A cannot be higher than that of Service B. Service B's downtime becomes Service A's downtime. With asynchronous patterns, these problems are reduced. Services can still communicate, but there's no expectation of an immediate response. The downstream service can process the communication on its own schedule without tying up the upstream service. This adds its own challenges around debugging, eventual consistency, and more, but it does reduce the downsides of synchronous communication. For more on request-response vs. event-driven, check out Former AWS Developer Advocate Talia Nassi's post on the benefits of moving to event-driven architectures."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Services can operate independently without relying on immediate responses from other services through asynchronous communication patterns. Service A does not have to wait for Service B's response to continue functioning. Instead, services broadcast events that other services can subscribe to and react upon as needed. This decouples producers from consumers, allowing services to process events on their own schedules. While this approach introduces challenges like debugging and eventual consistency, it reduces the downsides of synchronous communication where a service's downtime directly impacts dependent services. Event producers publish occurrences within their service, and event consumers subscribe to relevant events and update their state or trigger workflows accordingly. The producer does not know or care about the specific consumers, similar to a news broadcaster reporting events regardless of who is tuned in.\n```"}, {"role": "assistant", "content": "If Service A relies on synchronous responses from Service B to operate, the uptime for Service A cannot be higher than that of Service B. Service B's downtime becomes Service A's downtime. With asynchronous patterns, these problems are reduced. Services can still communicate, but there's no expectation of an immediate response. The downstream service can process the communication on its own schedule without tying up the upstream service. This adds its own challenges around debugging, eventual consistency, and more, but it does reduce the downsides of synchronous communication. For more on request-response vs. event-driven, check out Former AWS Developer Advocate Talia Nassi's post on the benefits of moving to event-driven architectures.\n### Event-driven architectures communicate via events\n\nUnsurprisingly, the \"events\" bit is the more unique part of event-driven architectures. In event-driven architectures, services will broadcast events that will be consumed by and reacted upon by other services. Thus, events require two types of parties: Event producers, which publish events describing something that happened within the service (a User was created, an Order was delivered, or a Login Attempt failed). Event consumers, which subscribe to published events and react to them (updating local state, incrementing aggregates, triggering workflows, etc.). A key feature of true event-driven architectures is that the producers and consumers are completely decoupled -- a producer shouldn't know or care who is consuming its events and how the consumers use those events in their service.. An event producer is like a broadcaster on the nightly news. The broadcaster will say the news that happened regardless of whether anyone is tuned in. Contrast this with a more traditional message-driven architecture in which one component in a system may insert a message to a message queue (like SQS or RabbitMQ) for processing. A message-driven pattern is also asynchronous, like an event-driven pattern. However, the producer of the message is purposefully sending the work to a specific customer. The use of the queue helps with resiliency and with faster response times from the initial component, but it's not truly \"event-driven\" in the normal sense of the term given the tight connection between the producer of the message and the consumer."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```- A news broadcaster delivers updates regardless of whether anyone's watching.\n- In a message-driven setup, one system part sends work to a specific recipient via a message queue like SQS or RabbitMQ.\n- It's asynchronous like event-driven, but the sender purposefully targets a consumer.\n- Using queues improves resilience and speeds up the initial component's response.\n- However, it's not purely \"event-driven\" due to the tight producer-consumer link.\n```"}, {"role": "assistant", "content": "The broadcaster will say the news that happened regardless of whether anyone is tuned in. Contrast this with a more traditional message-driven architecture in which one component in a system may insert a message to a message queue (like SQS or RabbitMQ) for processing. A message-driven pattern is also asynchronous, like an event-driven pattern. However, the producer of the message is purposefully sending the work to a specific customer. The use of the queue helps with resiliency and with faster response times from the initial component, but it's not truly \"event-driven\" in the normal sense of the term given the tight connection between the producer of the message and the consumer."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The presence of a news broadcaster does not depend on the audience, they will report news regardless. This is comparable to standard message-driven models like SQS or RabbitMQ where a system component pushes a message into a queue for processing. While this too is asynchronous and similar to an event-driven system, there's a distinct disparity: in the message-driven model, the message is deliberately sent to a specific recipient. While this indirect communication through a queue aids in resiliency and expedient responses, a strong synchrony between message sender and receiver deviates this model from normal event-driven frame.\n\nDiscussing event-based compute:\n\nDefining features of event-based computation:\n\n- The life of a computational entity is intrinsically linked to an event being processed.\n- The computation cluster acts on one event at any given time.\n\nTo provide an understandable context, consider setting up a Lambda function. Post drafting code, storage to AWS after converting to a ZIP file, and defining RabbitMQ settings\u2014 none of this automatically activates your code, unlike similar configurations on EC2 or Fargate. So, despite structuring your Lambda function, no instances of operation exist yet as the function unrealized.\n\nLambda operations commence upon its integration with an external event source. Most commonly, API Gateway, SQS, EventBridge, and Kinesis/ DynamoDB Streams integrate with Lambda. laying out event origination and linking up functional services leads to the function's activation. Additionally, converted Lambda function instances process triggered events under your customization and recap the intended response to the event initiator.\n\nTo optimize, Lambda service may preserve function instances functional in case of several triggering events occurring within a brief span. The logistics of these operations are mostly inaccessible. Noteworthy: traditional function instances waiting upon request arrivals for processing their tasks or continually updating retrieval messages align differently from designated compute segments idly attending to a sole critical event. Even a Fargate role lineup contrasts against Lambda functions since their execution lacks any consciousness revolving around attribute triggering events.\n```"}, {"role": "assistant", "content": "The broadcaster will say the news that happened regardless of whether anyone is tuned in. Contrast this with a more traditional message-driven architecture in which one component in a system may insert a message to a message queue (like SQS or RabbitMQ) for processing. A message-driven pattern is also asynchronous, like an event-driven pattern. However, the producer of the message is purposefully sending the work to a specific customer. The use of the queue helps with resiliency and with faster response times from the initial component, but it's not truly \"event-driven\" in the normal sense of the term given the tight connection between the producer of the message and the consumer.\n## What is event-based compute\n\nNow that we understand a bit about event-driven architectures, let's turn to event-based compute to see how it differs.\n\nThere are two key characteristics of event-based compute:\n\n1. First, the existence of a compute instance is intimately tied to the occurrence of an event to be processed.\n2. Second, the compute acts on a single event at a time.\n\nThat's sort of abstract, so let's make it more concrete.\n\nImagine you create a Lambda function. You write a bit of code, create a ZIP file, upload it to AWS, and set up the configuration in the Lambda service. Configuring this doesn't actually start your code, like it might with an EC2 instance or a Fargate container. By default, there will be no actual instances of your Lambda compute running. Your Lambda function has potential, but it hasn't realized it yet.\n\nTo make your function a reality, you need to hook it up to an event source. There are a ton of services that integrate with Lambda. The most popular sources are probably API Gateway (HTTP requests), SQS (queue processing), EventBridge (event bus), and Kinesis / DynamoDB Streams (stream processing).\n\nOnce you've configured your event source and an event has flowed through the configured service, then your function will spring to life. The Lambda service will create an instance of your function and pass the triggering event to it for processing by your function. Your function will process the event as desired and return a response back to the event trigger.\n\nFor optimization purposes, the Lambda service may keep your function instance running for a bit to serve other events that occur in a short time period. However, the specifics of this is (mostly) out of your control.\n\nImportantly, the purpose of an instance of running compute is to handle a single, specific event. This distinguishes Lambda from traditional instances or containers which are created to be available to handle requests as they arrive or to poll for messages from a queue or stream. It even distinguishes Lambda from something like creating a Fargate task on a schedule. While the creation of the task is based on an event, the task doesn't naturally have awareness of the event that created it while executing."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```- DynamoDB is a fully managed, highly scalable, multi-tenant cloud database service by AWS.\n- It was inspired by the concepts from the Dynamo Paper, an internal database used by Amazon to handle large-scale retail operations.\n- The new DynamoDB Paper discusses the product-level learnings and technical improvements over the years.\n- Key product-level takeaways:\n    - Consistent performance is crucial for users.\n    - Fully managed services are preferred over self-managed ones.\n    - User data distribution is often uneven, contrary to expectations.\n- The Dynamo Paper highlighted that strong consistency is not always necessary, and relaxing consistency requirements can improve availability and reduce write latency.\n- The DynamoDB Paper provides insights into how user needs and existing practices were reviewed to develop the service further.\n```"}, {"role": "assistant", "content": "# Key Takeaways from the DynamoDB Paper\n\nIn 2007, a group of engineers from Amazon published The Dynamo Paper, which described an internal database used by Amazon to handle the enormous scale of its retail operation. This paper helped launch the NoSQL movement and led to the creation of NoSQL databases like Apache Cassandra, MongoDB, and, of course, AWS's own fully managed service, DynamoDB.\n\nFifteen years later, the folks at Amazon have released a new paper about DynamoDB. Most of the names have changed (except for AWS VP Swami Sivasubramanian, who appears on both!), but it's a fascinating look at how the core concepts from Dynamo were updated and altered to provide a fully managed, highly scalable, multi-tenant cloud database service.\n\nIn this post, I want to discuss my key takeaways from the new DynamoDB Paper. There are two main areas I found interesting from my review:\n- The product-level, \"user needs\" learnings of the DynamoDB service; and\n- The technical improvements over the years to further develop the service.\n## Product-level takeaways from the DynamoDB Paper\n\nBoth the Dynamo paper and the DynamoDB paper describe some incredible technical concepts, but I'm equally impressed by the discussion of user needs. In both papers, there is a deep review of existing practices to see what is important and what should be re-thought around core user needs.\n\nThe Dynamo paper also noted that the traditional guarantee of strong consistency, while critical in some circumstances, was not necessary for all applications. In many cases, the enhanced availability and reduced write latency achieved by relaxing consistency requirements was well worth the tradeoff.\n\nThere are three important notes on user needs that I took from the paper:\n1. The importance of consistent performance\n2. Fully managed is better than self-managed\n3. User data isn't as evenly distributed as you want"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The two papers conducted an in-depth analysis of existing practices to evaluate their significance and determine if a re-evaluation was necessary to better align with core user requirements.\n\nThe Dynamo paper highlighted that while strong consistency is critical in certain scenarios, it may not be essential for all applications. In many cases, the improved availability and reduced write latency achieved by relaxing consistency requirements could be a worthwhile trade-off.\n\nThree key points regarding user needs were derived from the paper:\n\n1. The importance of consistent performance\n2. Fully managed solutions are preferable to self-managed ones\n3. User data distribution is often uneven\n\nThe importance of consistent performance:\nThe DynamoDB paper repeatedly emphasized that for many users, consistent performance at any scale is often more crucial than median request service times. In other words, a narrower range between median and tail latency is preferable to reducing median (or even p90 or p95) latency.\n\nFully managed over self-managed:\nFor those familiar with cloud computing and serverless architectures, the focus is on building key business differentiators while offloading undifferentiated tasks to third-party services, allowing for a fully managed solution.\n\nThe paper's observations and user needs analysis provide valuable insights into the trade-offs and considerations involved in designing and implementing distributed systems.\n```"}, {"role": "assistant", "content": "In both papers, there is a deep review of existing practices to see what is important and what should be re-thought around core user needs.\n\nThe Dynamo paper also noted that the traditional guarantee of strong consistency, while critical in some circumstances, was not necessary for all applications. In many cases, the enhanced availability and reduced write latency achieved by relaxing consistency requirements was well worth the tradeoff.\n\nThere are three important notes on user needs that I took from the paper:\n1. The importance of consistent performance\n2. Fully managed is better than self-managed\n3. User data isn't as evenly distributed as you want\n### The importance of consistent performance\n\nOne point that the DynamoDB paper hammers over and over is that, for many users, \"consistent performance at any scale is often more important than median request service times.\" Stated differently, it's better to have a narrower range between median and tail latency than it is to reduce median (or even p90 or p95) latency.\n### Fully managed over self-managed\n\nIf you're reading this blog, you're probably drinking the cloud Kool-Aid and may even be fully into the serverless world. In the serverless world, we're as focused as possible building the key differentiators of our business while offloading the undifferentiated heavy lifting to others."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```### Opting for fully managed over self-managed\n\nIf you're referencing this article, you are likely quite familiar with cloud operations and might even be deeply immersed in the serverless environment. In the serverless domain, we aim to devote our efforts to creating unique aspects of our business and delegate the routine backend operations to external entities.\n### The critical role of regular outcomes\n\nOne of the recurring themes in the DynamoDB documentation is that a lot of users often value trustworthy performance at any level over median request response times. To put it plainly, having managed results between median and tail latency is more valuable than lowering median or p90 or p95 latency.\n### Abnormal distribution of user data\n\nThe key takeaway for the user is that we can only interact and deal with the consumers we have at hand, not those we aspire to work with. Ideally, users would exert consistent and forecasted traffic that evenly distributed data access across all sections of a table's key map. Yet, reality presents a different scenario. The initial Dynamo documentation discussed the use of constant hashing in data dissemination across individual divisions that are typically nearby 10GB in volume. Further, differing from the primary Dynamo model, DynamoDB is a multiple-user system. Your divisions will share storage space with divisions from other DynamoDB user datasets.\n```"}, {"role": "assistant", "content": "### Fully managed over self-managed\n\nIf you're reading this blog, you're probably drinking the cloud Kool-Aid and may even be fully into the serverless world. In the serverless world, we're as focused as possible building the key differentiators of our business while offloading the undifferentiated heavy lifting to others.\n### The importance of consistent performance\n\nOne point that the DynamoDB paper hammers over and over is that, for many users, \"consistent performance at any scale is often more important than median request service times.\" Stated differently, it's better to have a narrower range between median and tail latency than it is to reduce median (or even p90 or p95) latency.\n### User data isn't as evenly distributed as you want\n\nThe final user takeaway is that you have to work with the users you're given, not the users you want. In an ideal world, users would have steady, predictable traffic that spread data access evenly across a table's keyspace. The reality is much different. The original Dynamo paper used the concept of consistent hashing to distribute your data across independent partitions of roughly 10GB in size. (Partitions are discussed in more depth below). It uses the partition key of your items to place data across the partitions, which allows for predictable performance and linear horizontal scaling. Further, unlike the original Dynamo system, DynamoDB is a multi-tenant system. Your partitions are co-located with partitions from tables of other DynamoDB users."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Users ideally should have consistent and predictable traffic that would evenly distribute data access amongst the keyspace of a table. However, this is often not the case in practice. In the original Dynamo paper, consistent hashing was implemented to allocate your data across self-governing partitions, each approximately 10GB in size. Partitions are further discussed later. The partition key of your items is used to place data across these partitions, enabling predictable performance and linear horizontal scaling. Moreover, there is a notable difference between DynamoDB and the original Dynamo system\u2014DynamoDB is a multi-tenant system, whereby your partitions share space with those from tables of other DynamoDB users. \n\n### Key Insights from the DynamoDB Paper\n\nAlthough the product-related learnings are noteworthy, the core focus of this paper is its technical aspect. The DynamoDB team's work on a large scale is commendable and many of the technical insights can be knowledgeable, not just for technological perfectionists but for those who are not necessarily operating at the same scale as DynamoDB.\n```"}, {"role": "assistant", "content": "In an ideal world, users would have steady, predictable traffic that spread data access evenly across a table's keyspace. The reality is much different. The original Dynamo paper used the concept of consistent hashing to distribute your data across independent partitions of roughly 10GB in size. (Partitions are discussed in more depth below). It uses the partition key of your items to place data across the partitions, which allows for predictable performance and linear horizontal scaling. Further, unlike the original Dynamo system, DynamoDB is a multi-tenant system. Your partitions are co-located with partitions from tables of other DynamoDB users. \n## Technical takeaways from the DynamoDB Paper\n\nThe product-level learnings are fascinating, but this is ultimately a technical paper. The work the DynamoDB team is doing at massive scale is impressive, and many of the technical learnings apply even to those without DynamoDB's scale."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Users may have varying data access patterns that do not distribute evenly across a table's keyspace. DynamoDB utilizes consistent hashing to distribute data across independent partitions of around 10GB size. The partition key determines the partition an item is placed in, allowing for predictable performance and linear scaling. DynamoDB is a multi-tenant system where partitions from different users' tables are co-located.\n\nDynamoDB employs log replicas to improve durability and availability during instance failures. Data is split into partitions, which are independent 10GB storage segments. The partition key assigns items to a partition, enabling horizontal scaling while keeping related items together. DynamoDB runs a fleet of storage nodes handling partitions from multiple user tables. A partition consists of three instances across availability zones, forming a replication group. One instance acts as the leader, handling writes by committing them locally and to at least one replica before responding to the client. This enhances durability in case of node failure.\n\nEach partition has a B-tree containing indexed data and a write-ahead log (WAL) storing ordered updates. WALs improve write durability and latency as appending to them is faster than updating the B-tree, which involves random I/O and potential page rewrites.\n```"}, {"role": "assistant", "content": "In an ideal world, users would have steady, predictable traffic that spread data access evenly across a table's keyspace. The reality is much different. The original Dynamo paper used the concept of consistent hashing to distribute your data across independent partitions of roughly 10GB in size. (Partitions are discussed in more depth below). It uses the partition key of your items to place data across the partitions, which allows for predictable performance and linear horizontal scaling. Further, unlike the original Dynamo system, DynamoDB is a multi-tenant system. Your partitions are co-located with partitions from tables of other DynamoDB users. \n### Using log replicas to improve durability and availability\n\nOne of the more interesting points was how DynamoDB uses something called log replicas to assist during periods of instance failure. \n\n```plaintext\nStart of DynamoDB storage background section\n```\nUnder the hood, DynamoDB is splitting your data into partitions, which are independent storage segments of roughly 10GB in size. DynamoDB uses the partition key to assign your items to a given partition, which allows DynamoDB to scale horizontally as your database grows while still keeping related items together. DynamoDB is running a massive fleet of storage nodes which are handling partitions from many different user tables. An individual partition is actually a set of three partition instances in different availability zones which form a replication group. One of the instances is the leader for a given partition and is responsible for handling all writes. When a write comes in, the leader writes it locally and ensures it is commited to at least one additional replica before returning to the client. This increases durability in the event of failure, as the loss of one node will not result in loss of data. On each storage partition are two data structures -- the B-tree that contains the indexed data on the partition along with a write-ahead log (WAL) that contains an ordered list of updates applied to that partition. A write-ahead log is a commonly used tactic in databases to enhance the durability and latency of write operations. Updating the B-tree is slower as it involves random I/O and may include re-writing multiple pages on disk, whereas updating the write-ahead log is an append-only operation that is much faster (P.S. the write-ahead log is basically the concept behind Kafka and related systems!). \n\n```plaintext\nEnd of DynamoDB storage background section\n```"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The Serverless Framework provides a standardized structure for applications, abstracts appropriate aspects while avoiding over-abstraction, and helps constrain potentially harmful impulses. It offers a balance between simplicity and flexibility that aligns well with building serverless applications on AWS.\n\nSome key advantages of the Serverless Framework include:\n\n1. Consistent application structure across projects, promoting familiarity and ease of understanding.\n2. Appropriate abstraction of common serverless tasks, reducing boilerplate and repetitive configuration.\n3. Avoidance of over-abstraction, allowing direct access and control over AWS services when needed.\n4. Discouragement of unnecessary complexity or over-engineering by providing constraints and opinionated defaults.\n\nWhile the AWS CDK has gained popularity, the Serverless Framework's approach resonates with a preference for simplicity and a focus on building serverless applications efficiently on AWS.\n```"}, {"role": "assistant", "content": "# Why I (Still) Like the Serverless Framework over the CDK\n\nOver the past year or two, I've seen the AWS CDK turn a lot of my friends into converts. And in the very recent past, a few of these people have written up their (mostly positive) experiences with the CDK. Some of these articles describe similar concerns I have, but none of them quite nails my thoughts on the matter. I thought I'd throw my hat in the ring and describe why I still prefer the Serverless Framework over the CDK.\n## My Background and Preferences\n\nLet's start with some facts that factor into my preferences. Your situation might not overlap with mine and, thus, some of my points may not be applicable to you. I worked for Serverless, Inc., creators of the Serverless Framework, for about two and a half years. This is relevant! I learned a ton about AWS + serverless while working there, and I built a lot of knowledge around the Serverless Framework in particular. Second, I am primarily building \"little-s\" serverless applications on AWS. Finally, I have a pretty strong bias for boring in my programming.\n## Why I like the Serverless Framework\n\nAt a high level, there are four reasons I prefer the Serverless Framework:\n\n1. It provides a standard structure for every application\n2. It abstracts the right things\n3. It doesn't abstract the wrong things\n4. It constrains our bad impulses"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The Serverless Framework and AWS CDK are both tools for defining and deploying serverless applications on AWS. They serve as pre-processors for AWS CloudFormation, but differ in their approach.\n\nThe Serverless Framework utilizes a declarative YAML configuration file called serverless.yml to describe the application. It has blocks for defining Lambda functions, event sources, and other AWS resources. The key abstraction is the functions block, which simplifies the process of creating Lambda functions and connecting them to event sources. This approach follows a 'convention over configuration' pattern.\n\nIn contrast, CDK applications involve a mixture of application code and infrastructure code within the same codebase. The infrastructure definitions are written in a programming language like TypeScript or Python. This approach requires developers to understand the team's specific implementation of the CDK, in addition to the CDK itself, which can make onboarding new developers more challenging.\n\nThe text highlights concerns around maintainability and developer knowledge transfer for CDK applications, as well as a preference for a more standardized structure provided by the Serverless Framework.\n```"}, {"role": "assistant", "content": "It abstracts the right things\n3. It doesn't abstract the wrong things\n4. It constrains our bad impulses\n## My Background and Preferences\n\nLet's start with some facts that factor into my preferences. Your situation might not overlap with mine and, thus, some of my points may not be applicable to you. I worked for Serverless, Inc., creators of the Serverless Framework, for about two and a half years. This is relevant! I learned a ton about AWS + serverless while working there, and I built a lot of knowledge around the Serverless Framework in particular. Second, I am primarily building \"little-s\" serverless applications on AWS. Finally, I have a pretty strong bias for boring in my programming.\n### A standard structure for every application\n\nBoth tools are basically pre-processors for AWS CloudFormation. Their core difference is in the process they use to get to CloudFormation. With the Serverless Framework, you're writing a single, declarative YAML configuration file called serverless.yml that will describe your application. It has a number of top-level blocks, including functions for creating Lambda functions and resources for provisioning other AWS resources. The key abstraction is really in the functions block, as it focuses on simplifying the process around creating Lambda functions and hooking up event sources to these functions. This is the core of most serverless applications, and simplifying this process is a big win.\n\nIf you're from the Rails ecosystem, you might think of the Serverless Framework's approach as 'convention over configuration'.\n\nIn contrast, every CDK repo I've looked at turns into a murder mystery. I'm digging through application code that is mixed with infrastructure code, trying to find my function handlers, my service code, anything that will help me solve The Case of the Missing IAM Permission. Some people have discussed the long-term issues around CDK maintainability -- what if people aren't updating your constructs or staying on top of CDK version updates? Those issues are real, but I'm way more worried about how you maintain developer knowledge around a specific CDK application. There are real hurdles to onboarding a new developer into your custom structure. They don't just need to know the CDK, they need to know your team's specific implementation of the CDK. The knowledge of your application that is built up within your team is hidden, and it will result in slower ramp-up times for new developers."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Some individuals have raised concerns about the sustainability of CDK over time. There are potential issues such as lack of updates for constructs or not adhering to the CDK version updates. These problems are valid, but the larger problem seems to be the retention of developer knowledge relating to a particular CDK application. Bringing a new developer up-to-speed on your specific structure poses significant challenges. A new developer needs to not only understand CDK but also your team's unique use of CDK. This specialized knowledge remains concealed within your team and could potentially lead to elongated adjustment periods for new developers.\n\nTwo of the more challenging aspects of deploying serverless applications are: encapsulating your functions, and setting up the events that activate those functions. In this context, the Serverless Framework is appreciably more efficacious in its abstractions. Recognizing that functions and events are the backbones of serverless applications, the Serverless Framework has accentuated these two facets in their core abstraction.\n\nHere is an illustrative functions block taken from a serverless.yml file:\n\n```\nfunctions:\n  createUser:\n    handler: src/handlers/createUser.handler\n    events:\n      - http: POST /users\n  processQueue:\n    handler: src/handlers/processQueue.handler\n    events:\n      - sqs:\n          arn: !GetAtt [MyQueue, Arn]\n          batchSize: 10\n  fanout:\n    handler: src/handlers/fanout.handler\n    events:\n      - sns:\n          arn: !Ref MyTopic\n          topicName: MyTopic\n```\n\nExecuting the command `serverless deploy` prompts the serverless framework to compile and configure a function on AWS for each function listed in the functions block. This comprises createUser, processQueue, and fanout. For each function, the serverless framework auto-installs dependencies, conducts a build process, packs the directory, uploads it to S3, and establishes the function alongside its basic configuration with the Lambda service. All this takes place with minimal involvement on the user end, unless the user has special requirements.\n```"}, {"role": "assistant", "content": "Some people have discussed the long-term issues around CDK maintainability -- what if people aren't updating your constructs or staying on top of CDK version updates? Those issues are real, but I'm way more worried about how you maintain developer knowledge around a specific CDK application. There are real hurdles to onboarding a new developer into your custom structure. They don't just need to know the CDK, they need to know your team's specific implementation of the CDK. The knowledge of your application that is built up within your team is hidden, and it will result in slower ramp-up times for new developers.\n### The Serverless Framework abstracts the right things\n\nI mentioned in the last section that both the Serverless Framework and the CDK are abstractions over CloudFormation. In my opinion, the Serverless Framework does a better job of abstracting the right things to make it easier to work with serverless applications. There are two hard things about deploying serverless applications:\n\n- Packaging up your functions\n- Configuring the events that will trigger your functions\n\nThe Serverless Framework has rightly identified that functions and events are core to serverless applications and has made them the core abstraction. Let's look at another example functions block in a serverless.yml file:\n\n```\nfunctions:\n  createUser:\n    handler: src/handlers/createUser.handler\n    events:\n      - http: POST /users\n  processQueue:\n    handler: src/handlers/processQueue.handler\n    events:\n      - sqs:\n          arn: !GetAtt [MyQueue, Arn]\n          batchSize: 10\n  fanout:\n    handler: src/handlers/fanout.handler\n    events:\n      - sns:\n          arn: !Ref MyTopic\n          topicName: MyTopic\n```\n\nWhen I run `serverless deploy` to deploy my stack, it's going to build and configure a function on AWS for each function in my functions block (here, there are three: createUser, processQueue, and fanout). For each function, it may install your dependencies, run a build process, zip up the directory, upload it to S3, and register the function and its basic configuration with the Lambda service. You don't really need to know much about that packaging process, unless you have specific needs."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Each function can install dependencies, execute the build process, compress the directory, upload it to S3, and register the function and its basic configuration with the Lambda service. There is no need to understand the details of this packaging process unless specific needs exist. \n### Problems with abstraction in the CDK\n\nContrary to the previous segment, it is also suggested that the CDK abstracts the wrong aspects. However, there is an admission to be made. In the previous section, the event configuration made using the Serverless Framework was presented as simpler than it actually is. In the SQS and SNS integrations, additional infrastructure is referenced, specifically an SQS queue and an SNS topic:\n\n```\nfunctions:\n  createUser:\n    handler: src/handlers/createUser.handler\n    events:\n      - http: POST /users\n  processQueue:\n    handler: src/handlers/processQueue.handler\n    events:\n      - sqs:\n          arn: !GetAtt [MyQueue, Arn]\n          batchSize: 10\n  fanout:\n    handler: src/handlers/fanout.handler\n    events:\n      - sns:\n          arn: !Ref MyTopic\n          topicName: MyTopic\n```\n\nWhilst the Serverless Framework considers functions and events as the main constituents of your serverless applications, it also acknowledges the necessity for further supporting infrastructure to achieve anything substantial. And so, the resources block of your serverless.yml comes into play.\n\nYou can include any additional AWS resource required directly via CloudFormation in this resources block. The resources block for our SQS- and SNS-functioning application would look as this:\n\n```\nresources:\n  MyQueue:\n    Type: AWS::SQS::Queue\n    Properties:\n      QueueName: my-queue\n\n  MyTopic:\n    Type: AWS::SNS::Topic\n    Properties:\n      TopicName: my-topic\n```\n\nThe appealing aspect of this is that you are effectively writing CloudFormation. As a result, you are gaining proficiency in the tool you'll be using extensively.\n```"}, {"role": "assistant", "content": "For each function, it may install your dependencies, run a build process, zip up the directory, upload it to S3, and register the function and its basic configuration with the Lambda service. You don't really need to know much about that packaging process, unless you have specific needs.\n### The CDK abstracts the wrong things\n\nOn the flip side of the previous section, I also think that the CDK abstracts the wrong things. But first, I have a confession. In the previous section, I made the Serverless Framework event configuration look slightly easier than it is. For both the SQS and SNS integrations, we're referring to additional infrastructure in our application -- an SQS queue and an SNS topic:\n\n```\nfunctions:\n  createUser:\n    handler: src/handlers/createUser.handler\n    events:\n      - http: POST /users\n  processQueue:\n    handler: src/handlers/processQueue.handler\n    events:\n      - sqs:\n          arn: !GetAtt [MyQueue, Arn]\n          batchSize: 10\n  fanout:\n    handler: src/handlers/fanout.handler\n    events:\n      - sns:\n          arn: !Ref MyTopic\n          topicName: MyTopic\n```\n\nWhile the Serverless Framework looks at functions and events as the core of your serverless applications, it also realizes you will need additional supporting infrastructure to do anything meaningful. That's where the resources block of your serverless.yml comes in.\n\nIn the resources block, you can configure any additional AWS resource you want via straight CloudFormation. For our SQS- and SNS-enabled application above, our resources block would look as follows:\n\n```\nresources:\n  MyQueue:\n    Type: AWS::SQS::Queue\n    Properties:\n      QueueName: my-queue\n\n  MyTopic:\n    Type: AWS::SNS::Topic\n    Properties:\n      TopicName: my-topic\n```\n\nThe part I love about this is that you're actually writing CloudFormation. You're learning the tool that you'll be using under the hood."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```# Examination of DynamoDB Costs\n\nThis article will outline the thought process for contemplating DynamoDB costs. It will cover an understanding of DynamoDB pricing, including offering examples of how these considerations impact decisions on DynamoDB user expenses. It will not offer a comprehensive deep dive into DynamoDB costs and for more detailed strategies on cost-saving patterns with DynamoDB, Khawaja Sham's extensive thread provides further information.\n\n## Understanding DynamoDB Pricing Practices\n\nThe cost structure of DynamoDB consists primarily of three major components. The service charges for read consumption, write consumption, and storage used. This model presents a foreseeable pattern of potential charges.\n\nUsers can get preoccupied with different pricing modes, such as on-demand versus provisioned, or table storage types such as Standard or Standard-IA. However, it is fundamental to understand that the primary cost components are read, write, and storage charges.\n\nThat foresight absolves the need for puzzlement over concerns like how many concurrent operations a db.m6gd.large instance can handle or how much data can fit in a 1TB gp3 storage volume, for example. It leaves the user to mainly focus on the cost impact of data read, data written, and storage consumed.\n\nA few nuances in the DynamoDB pricing structure are noteworthy. Read Capacity Units (RCUs) and Write Capacity Units (WCUs) are charged in increments. Any unused increments in these units are nonetheless chargeable. Strongly consistent read requests and eventually consistent read requests are available as choices for the user. Moreover, DynamoDB enables reading several items with a single request through a DynamoDB Query.\n```"}, {"role": "assistant", "content": "# How you should think about DynamoDB costs\n\nIn this post, I'll walk through my approach to reasoning about DynamoDB costs. I'll start with an overview about how DynamoDB pricing works -- feel free to skip this if you're already familiar. From there, I'll walk through a few examples of how I use this to make decisions about DynamoDB costs.\n\nThis post isn't comprehensive on DynamoDB costs -- it's more about how to approach this and make cost modeling part of your design process. If you want some other tips on DynamoDB costs, check out Khawaja Sham's epic thread on patterns for saving costs with DynamoDB. It has a ton of wide-ranging thoughts on DynamoDB costs.\n## How DynamoDB pricing works\n\nLet's start with a basic overview of how DynamoDB pricing works. You can get bogged down in the different pricing modes (on-demand vs. provisioned?) or table storage classes (Standard or Standard-IA?), but that is secondary to understanding the core dimensions on which DynamoDB charges.\n\nBasically, DynamoDB is charging for three things:\n- Read consumption\n- Write consumption\n- Storage\n\nThe great thing about this is its predictability! You don't have to wonder about how many concurrent operations a db.m6gd.large instance can handle. You don't have to worry about how much data you can store in a 1TB gp3 storage volume. You just have to worry about how much data you're reading, writing, and storing.\n\nA few other quick notes / quirks about DynamoDB pricing:\n- Notice that RCUs and WCUs are step functions -- you're charged for each increment, even if you don't use the full increment.\n- For reads, you have the option between a strongly consistent read request and an eventually consistent read request.\n- A DynamoDB Query allows you to read multiple items in a single request."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```There's no need for concern about the storage capacity of a 1TB gp3 volume, instead, consideration needs to be given to the amount of data being read, written and stored.\n\nAdditional aspects related to DynamoDB pricing include:\n- RCUs and WCUs operate based on step functions, meaning charges are implicated for each increment, even if it's not fully utilised.\n- For read operations, there's a choice between a strongly consistent read request or an eventually consistent read request.\n- A single request through DynamoDB Query can fetch multiple items.\n## WCU usage is doubled by DynamoDB Transactions\n\nDue to DynamoDB Transactions using two-phase commit mechanisms underneath, every stage of the process incurs a charge.\n```"}, {"role": "assistant", "content": "You don't have to worry about how much data you can store in a 1TB gp3 storage volume. You just have to worry about how much data you're reading, writing, and storing.\n\nA few other quick notes / quirks about DynamoDB pricing:\n- Notice that RCUs and WCUs are step functions -- you're charged for each increment, even if you don't use the full increment.\n- For reads, you have the option between a strongly consistent read request and an eventually consistent read request.\n- A DynamoDB Query allows you to read multiple items in a single request.\n## Using DynamoDB Transactions doubles your WCU usage\n\nIt's using two-phase commit under the hood, so basically think of being charged for each phase."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```DynamoDB is a NoSQL database service provided by Amazon Web Services (AWS). It uses a key-value data model and offers high availability, scalability, and performance. The text provides some insights regarding the pricing and usage considerations for DynamoDB.\n\nWhen using DynamoDB transactions, which involve two-phase commit operations, the Write Capacity Unit (WCU) usage is effectively doubled. This is because each transaction requires two write operations, one for the prepare phase and another for the commit phase.\n\nThe text also highlights a few points about DynamoDB pricing:\n\n1. Read Capacity Units (RCUs) and WCUs are charged in increments, even if the full increment is not utilized.\n2. For read operations, there is a choice between strongly consistent reads and eventually consistent reads, with different pricing implications.\n3. DynamoDB Query allows reading multiple items in a single request.\n\nThe text then discusses the choice between provisioned and on-demand billing modes for DynamoDB. It suggests using on-demand billing for initial calculations, as it has a fixed price for reads and writes, making cost estimation simpler. However, for high-traffic workloads, provisioned capacity may be a worthwhile optimization, potentially saving up to 42% on costs compared to on-demand billing, assuming 50% utilization.\n\nThe trade-off is that with provisioned capacity, users are responsible for scaling up and down based on traffic patterns and handling unexpected spikes in traffic. On-demand billing provides operational simplicity and avoids throttling but can be more expensive for high-traffic workloads.\n```"}, {"role": "assistant", "content": "## Using DynamoDB Transactions doubles your WCU usage\n\nIt's using two-phase commit under the hood, so basically think of being charged for each phase.\n You don't have to worry about how much data you can store in a 1TB gp3 storage volume. You just have to worry about how much data you're reading, writing, and storing.\n\nA few other quick notes / quirks about DynamoDB pricing:\n- Notice that RCUs and WCUs are step functions -- you're charged for each increment, even if you don't use the full increment.\n- For reads, you have the option between a strongly consistent read request and an eventually consistent read request.\n- A DynamoDB Query allows you to read multiple items in a single request.\n## Choosing between provisioned and on-demand\n\nIn the examples below, we'll do some calculations to see how you can think about DynamoDB pricing. For my examples, I always calculate using DynamoDB On-Demand billing mode. On-Demand billing has a fixed price for reads and writes, so it's easy to calculate the cost of a particular access pattern. I don't have to think about things like utilization over time or providing a buffer to handle unexpected bursts of traffic. This greatly simplifies the math and gives you a directionally accurate look at your potential costs.\n\nHowever, this doesn't mean you should never think about your billing mode! Once you've run the initial numbers, then you can fine-tune your calculations by looking at the difference between provisioned and on-demand.\n\nAt a first cut, if the on-demand numbers are negligible, you should probably stick with that. On-demand is going to be a hands-off solution that (almost completely) avoids throttling. If the potential bill won't break the bank, take the operational simplicity and move on.\n\nHowever, when you're talking about real money, then you can consider whether provisioned capacity would be a worthwhile optimization. In general, on-demand billing is 6.94x as expensive as fully-utilized provisioned capacity.\n\nThis seems like a lot, but note the key qualifier there -- fully-utilized. You're not going to get anywhere near full utilization, so saving 85% is not a realistic goal. However, it's not unrealistic to think you could get to 50% utilization, which would save you 42% on your bill. For a high-traffic DynamoDB workload, that's a pretty significant savings!\n\nJust remember what you're taking on. You're responsible for scaling up and down to account for traffic patterns. If traffic grows over time, you're responsible to account for the new highs. If you have a spike in traffic, you're responsible for scaling up to handle it. If you have a spike in traffic that you don't expect, you're responsible for handling it."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Achieving 85% utilization is unrealistic, but reaching 50% utilization, which would result in a 42% cost reduction, is attainable for a high-traffic DynamoDB workload, leading to significant savings. However, it is important to consider the responsibilities involved, such as scaling up and down to accommodate traffic patterns, handling traffic growth over time, and managing unexpected traffic spikes.\n\nRegarding the example of tracking view counts, the cost of incrementing a counter for each page visit can be estimated by considering the item size and traffic volume. For instance, with a 1KB item size, each write would consume 1 Write Capacity Unit (WCU), resulting in a cost of $1.25 per million page views under on-demand billing. This pricing may or may not be acceptable, depending on the specific requirements.\n\nIf the underlying record is larger, such as 10KB, the cost would increase significantly, with each million page views costing $12.50. In such cases, employing a vertical sharding technique, where the page view counter is separated into a smaller, focused item, can potentially save 90% of the write costs by acting only on the smaller item.\n```"}, {"role": "assistant", "content": "You're not going to get anywhere near full utilization, so saving 85% is not a realistic goal. However, it's not unrealistic to think you could get to 50% utilization, which would save you 42% on your bill. For a high-traffic DynamoDB workload, that's a pretty significant savings!\n\nJust remember what you're taking on. You're responsible for scaling up and down to account for traffic patterns. If traffic grows over time, you're responsible to account for the new highs. If you have a spike in traffic, you're responsible for scaling up to handle it. If you have a spike in traffic that you don't expect, you're responsible for handling it.\n## Doin' the math pt. 1: Tracking view counts\n\nAlright, enough background chatter. Let's get into some examples.\n\nThe first example is what spurred this blog post. Someone wrote me an email about potential DynamoDB costs in tracking view counts. Every time a user visits one of his pages, he wants to increment a counter for that page.\n\nHe was worried about the cost of this and asked what I thought about batching page views in Redis for the short term, then occasionally writing them back to DynamoDB.\n\nThis adds some cost (Redis instance!) and some complexity (multiple sources of data! syncing back to DynamoDB!), but he was thinking about doing it out of fear of a large DynamoDB bill.\n\nThe first thing I told him -- \"Do the math!\" Look at how big your item sizes are, then try to roughly estimate your traffic. I love spreadsheets, so I usually throw this in a spreadsheet and fiddle with the numbers to see what I'm dealing with.\n\nIn this situation, imagine his page record was pretty small -- under 1KB. If so, each write will be 1 WCU. Thus, he can estimate that his DynamoDB write costs will be $1.25 per million page views (on-demand billing is $1.25 per million WCUs).\n\nThis pricing may not be acceptable to you, but it gives you a sense of what you're dealing with. For him, it helped him realize that it wasn't something to optimize yet. He should spend his time on other things.\n\nPS: If you are thinking 'how do I know my item size?', generate an example record in your application and then paste it into Zac Charles' DynamoDB Item Size Calculator. I love this thing.\n\nNote that I discussed an enhanced version of this during my 2022 talk at AWS re:Invent. I talked about 'vertical sharding', which can be a nice use of single-table design principles to split a single entity into multiple records to save on costs. For this example, imagine our underlying record was much larger -- 10KB instead of 1KB. Now, each million page views will cost us $12.50 even though we're just incrementing a counter.\n\nInstead, we could separate our record into two items -- the page view counter and the actual item data. We can place them in the same item collection to fetch them in a single Query request when needed, but now a page view increment is only acting on the small, focused item. This would save 90% of our write costs for this example."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```- The text discusses 'vertical sharding', which involves splitting a single entity into multiple records to reduce costs in a single-table design.\n- It provides an example where the underlying record is 10KB instead of 1KB. In this case, each million page views would cost $12.50 even though it's just incrementing a counter.\n- As an alternative, the record could be separated into two items - the page view counter and the actual item data. These could be placed in the same item collection for fetching in a single Query request when needed.\n- This approach would save 90% of the write costs for the given example.\n\n- The text then moves on to discuss secondary indexes in DynamoDB and the associated trade-offs.\n- It explains that each write to DynamoDB is charged 1 WCU per KB of data, rounded up, and this applies to each target (base table and any secondary indexes).\n- Having multiple secondary indexes (e.g., 4-5) can quickly increase write costs. The example of the view count shows that with 5 secondary indexes, it would cost 6 WCU per page view instead of 1 WCU.\n- While there are ways to reduce costs for necessary Global Secondary Indexes (GSIs), sometimes it may be better to avoid a GSI altogether.\n- The text suggests that in DynamoDB, it's generally recommended to filter and sort records via the key attributes. However, there are certain times when it may be better to over-read data instead of using a GSI.\n```"}, {"role": "assistant", "content": "I talked about 'vertical sharding', which can be a nice use of single-table design principles to split a single entity into multiple records to save on costs. For this example, imagine our underlying record was much larger -- 10KB instead of 1KB. Now, each million page views will cost us $12.50 even though we're just incrementing a counter.\n\nInstead, we could separate our record into two items -- the page view counter and the actual item data. We can place them in the same item collection to fetch them in a single Query request when needed, but now a page view increment is only acting on the small, focused item. This would save 90% of our write costs for this example.\n## Doin' the math pt. 2: Do I really need that secondary index?\n\nAlright, that's an easy example. Let's look at another example that involves some tradeoffs between different approaches to an access pattern within DynamoDB.\n\nWe saw earlier that each write to DynamoDB is charged 1 WCU per KB of data, rounded up. However, this applies to each target to which the write will be applied. I use the word 'target' to refer to not just the base table but any secondary indexes to which the item will be written.\n\nIt's not uncommon to have 4-5 secondary indexes on a DynamoDB table. When this happens, your write costs can quickly add up. Think of our view count example above. If we had 5 secondary indexes, we'd be paying 6 WCU per page view instead of 1 WCU!\n\nThere are lots of ways to reduce costs for GSIs that you truly need (check out Khawaja's tweet about saving money on GSIs during his awesome thread on DynamoDB savings generally), but sometimes you may be better off avoiding a GSI altogether.\n\nLet's see an example.\n\nIn DynamoDB, I tell people that you almost always want to filter and sort the exact records you need via the key attributes. The key word here is 'almost'. There are certain times when it may be better to over-read your data."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```DynamoDB costs can accumulate quickly, particularly when using multiple secondary indexes. It is advisable to carefully evaluate the necessity of each index and consider alternative approaches when appropriate.\n\nIn certain scenarios, it may be more cost-effective to fetch and filter data within the application code rather than creating a secondary index. This approach can be beneficial when dealing with small data sets or infrequent access patterns. \n\nFor example, in a SaaS application where teams are typically small, with fewer than 10 users per team and user records being relatively small (1KB), fetching all users for a team and filtering them in the application code can be more economical than maintaining a separate secondary index. Even in the worst case of fetching 100 users (100KB), the read cost would be around 12.5 read capacity units (RCUs), which is less expensive than the write costs associated with maintaining a secondary index.\n\nIt is important to strike a balance between utilizing DynamoDB's key attributes for efficient querying and avoiding unnecessary secondary indexes for long-tail access patterns, especially when dealing with small data sets. While the default approach is to filter and sort data using key attributes, there are situations where over-reading and filtering data in the application code can be a more cost-effective solution.\n```"}, {"role": "assistant", "content": "When this happens, your write costs can quickly add up. Think of our view count example above. If we had 5 secondary indexes, we'd be paying 6 WCU per page view instead of 1 WCU!\n\nThere are lots of ways to reduce costs for GSIs that you truly need (check out Khawaja's tweet about saving money on GSIs during his awesome thread on DynamoDB savings generally), but sometimes you may be better off avoiding a GSI altogether.\n\nLet's see an example.\n\nIn DynamoDB, I tell people that you almost always want to filter and sort the exact records you need via the key attributes. The key word here is 'almost'. There are certain times when it may be better to over-read your data.\n## How you should think about DynamoDB costs | DeBrie Advisory\n\nLet's work with a simple example. Imagine you're selling a SaaS service that is purchased by teams. By nature of the industry you're in, it's natural that teams will be small. It would be extremely rare that over 100 people would belong to a single team. In the vast majority of cases, teams will be fewer than 10 users. Each User record within the team is pretty small -- 1KB of data. However, you also have a few access patterns on the User -- maybe a point lookup to fetch a User by username, and two range queries -- one to fetch all the Users with a given role, and one to fetch all Users on the team ordered by the date added to the team.\n\nYou could set up a GSI for each of these secondary access patterns. However, you could also just handle the range queries by fetching all the Users on the team and filtering them in your application code. Even in the worst case scenario, you'll be fetching 100KB of data. That's 12.5 RCUs (100KB / 4 = 25 * 0.5 for eventually consistent reads = 12.5 RCUs). For most cases, it will be much smaller than that -- likely less than a single RCU.\n\nFurther, RCUs are not only 4 times bigger than WCUs (4KB vs. 1KB), they're also 5 times cheaper ($0.25 per million as opposed to $1.25 per million for WCUs). Based on some back-of-the-napkin math, it might be better to skip the GSI and over-read data to handle long-tail access patterns. This is particularly true when the overall result set is pretty small. Again, you should still focus on using your key attributes as much as possible. However, there are times when you can relax that default assumption to save on write costs."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```GraphQL, DynamoDB, and Single-table Design\n\nThe topic of the compatibility of GraphQL and single-table design in DynamoDB has recently been discussed again, sparked by a tweet from Rick Houlihan, leading to a follow-up tweet suggesting that the post about single-table design, where it was mentioned that GraphQL might not be suitable for single-table design with DynamoDB, should be updated. Twitter is not an ideal medium for nuanced discussions, and the question of whether to use single-table design with GraphQL depends largely on the reasons for choosing to use GraphQL. In summary, single-table design can be used with GraphQL, and some knowledgeable individuals are doing so. However, it is not necessarily incorrect to choose not to use single-table design, depending on specific needs and preferences. Additionally, Amplify made the appropriate choice in opting for a table-per-model default.\n\nThe benefits of GraphQL and its compatibility with DynamoDB and single-table design are explored. The choice made by Amplify regarding the default table structure is also discussed.\n```"}, {"role": "assistant", "content": "# GraphQL, DynamoDB, and Single-table Design\n\nRecently, the topic of the compatibility of GraphQL and single-table design in DynamoDB came up again, sparked by a tweet from Rick Houlihan that led to a follow-up tweet indicating that I should update my post about single-table design where I mentioned that GraphQL might be an area where you don't want to use single-table design with DynamoDB. Twitter is a bad medium for nuance, and I think the question of whether to use single-table design with GraphQL is a nuanced one that depends a lot on why you are choosing to use GraphQL. If you want the TL;DR Twitter version of this post, it is: You absolutely can use single-table design with GraphQL, and I know some very smart people who are doing so. However, I don't think it's wrong to chose not to, depending on your specific needs + preferences. Further, I think Amplify made the right choice in opting for a table-per-model default. Post-publishing note: if you want to see some good practical tips on how to use single-table design with AppSync + GraphQL, check out this video from Adam Elmore on his setup. He also has some good conceptual points on when he does (and doesn't) use single-table design principles in his application. If you want the full details, check out the post below. We will cover: What are the benefits of GraphQL? DynamoDB and GraphQL Did Amplify make the right choice? Let's goooooo."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```It's not incorrect to choose not to use it, depending on individual requirements and preferences. Additionally, Amplify's decision to default to a table-per-model approach seems appropriate. For practical tips on using single-table design with AppSync and GraphQL, you can refer to Adam Elmore's video that discusses his methodology. It's not just pragmatic advice; he offers valuable insight into the circumstances in which he employs (and forgoes) single-table design principles. For a comprehensive understanding, continue reading below. The focus will be on understanding the advantages of GraphQL, its combination with DynomoDB, and whether Amplify's choice was suitable. \n\n## Understanding the Advantages of GraphQL\n\nFirst, we need to appreciate the possible benefits of GraphQL to understand why individuals opt for it. Keep in mind, however, that these are potential benefits. The decision to implement GraphQL could be driven by all, some, or maybe just one of these factors, depending on importance. Moreover, usage might ignore other likely benefits. The importance of these choices depends on the team and product's context and unique needs. There are a few additional benefits of GraphQL that are not listed here, like the ability to specify the exact fields to save on network bandwidth. This does not imply these features are insignificant, but rather, they don't significantly impact the single-table versus multi-table discussion.\n```"}, {"role": "assistant", "content": "However, I don't think it's wrong to chose not to, depending on your specific needs + preferences. Further, I think Amplify made the right choice in opting for a table-per-model default. Post-publishing note: if you want to see some good practical tips on how to use single-table design with AppSync + GraphQL, check out this video from Adam Elmore on his setup. He also has some good conceptual points on when he does (and doesn't) use single-table design principles in his application. If you want the full details, check out the post below. We will cover: What are the benefits of GraphQL? DynamoDB and GraphQL Did Amplify make the right choice? Let's goooooo.\n## What are the benefits of GraphQL?\n\nI want to start off by understanding the potential benefits of GraphQL in order to understand why people are choosing to use GraphQL. Note that I refer to these as potential benefits. People may adopt GraphQL for all of these reasons, or there may be one or two reasons that are more important. Further, they may use GraphQL in a way that eschews one or more of the other potential benefits. I don't think these decisions are necessarily wrong -- it depends a lot on context and the particular needs of a team and product. Finally, there are a few benefits of GraphQL that I don't mention here, such as the ability to specify the exact fields you want to save on network bandwidth. It doesn't mean these aren't important for some situations. Rather, they're not that relevant to the single-table vs. multi-table debate."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The potential benefits of adopting GraphQL are discussed, with the caveat that teams may prioritize certain advantages over others based on their specific needs and context. One key advantage highlighted is the type safety provided by the API schema, which ensures that the data returned from the backend adheres to a defined, typed structure. This addresses the common issue faced by frontend developers when working with JSON-based REST APIs, where the shape of the returned data can be unreliable, necessitating defensive coding. By having a clearly defined schema available from the backend, frontend clients of a GraphQL service can have greater confidence in the structure of the data they receive, reducing the need for extensive error handling. While other potential benefits of GraphQL are acknowledged, such as network bandwidth savings, the focus is on the type safety aspect as it relates to the debate between single-table and multi-table data access patterns.\n```"}, {"role": "assistant", "content": "Note that I refer to these as potential benefits. People may adopt GraphQL for all of these reasons, or there may be one or two reasons that are more important. Further, they may use GraphQL in a way that eschews one or more of the other potential benefits. I don't think these decisions are necessarily wrong -- it depends a lot on context and the particular needs of a team and product. Finally, there are a few benefits of GraphQL that I don't mention here, such as the ability to specify the exact fields you want to save on network bandwidth. It doesn't mean these aren't important for some situations. Rather, they're not that relevant to the single-table vs. multi-table debate.\n### API type safety\n\nThe first reason people like GraphQL is the type safety from the API. Single-page applications make requests to a backend server to retrieve data to display in the application. On a shopping application, the requested data could be a list of products in a category or the contents of a customer's shopping cart. On GitHub, the requested data could be a GitHub repository or a list of issues for the repository. In a standard JSON-based REST API, it's likely there is a schema for this data, at least theoretically. The problem comes in reality -- many frontend developers find that the returned data shape is unreliable, and developers need to implement lots of defensive coding to ensure they have the data properties they are expecting. GraphQL helps with this problem by having a defined, typed schema that is available from the backend. Further, most GraphQL implementations will ensure that a response from the backend adheres to this schema before sending to the frontend. This makes it significantly easier as a frontend client of a GraphQL-based service as you have greater confidence over the returned data."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```In a conventional JSON-based REST API, theoretically there may be a data schema. Applied in practical terms, however, many frontend developers encounter that the return data's structure can often be unreliable, requiring them to employ a number of safety measures to verify the data they are expecting. GraphQL addresses this issue due to its designated and typed schema provided by the backend. Moreover, the majority of GraphQL executions assure that a response from the backend complies with this schema prior to forwarding it to the frontend, enhancing the trustworthiness of the received data if you are a frontend client of a GraphQL-based service.\n\nRegarding the reduction of network requests from the frontend, GraphQL provides another advantage. Consider the scenarios presented previously permeating a single-page application. Implementing a shopping cart for a single-page application with a regular REST backend could necessitate multiple calls to render the shopping cart page: one request to fetch the current items in the cart and numerous requests to gather up-to-date data about each item. This presents a multitude of problems, not least the dreaded N + 1 issue which ORMs can add extra strain on the database due to their inefficient queries. The same issue applies to single-page applications using a RESTful backend, where the frontend must issue numerous requests to properly display the page.\n\nAn added complexity is the sequencing of the requests - the individual item details cannot be fetched until the shopping cart content is retrieved, leading to a cascade of linked requests and a perceptible sluggishness of the application.\n\nGraphQL considerably mitigates this issue by allowing the frontend client to retrieve a complete data map with just a single request. A single GraphQL query can gather the contents of the shopping cart and the specific details of each item. As soon as this request completes, the page can be displayed without waiting for further requests. However, it should be noted that GraphQL doesn't necessarily entirely remove the N + 1 problem. It is possible that the issue could be shifted to the backend; though that doesn't belittle the fact that GraphQL successfully eliminates the frontend application and developer from bearing the problem.\n```"}, {"role": "assistant", "content": "In a standard JSON-based REST API, it's likely there is a schema for this data, at least theoretically. The problem comes in reality -- many frontend developers find that the returned data shape is unreliable, and developers need to implement lots of defensive coding to ensure they have the data properties they are expecting. GraphQL helps with this problem by having a defined, typed schema that is available from the backend. Further, most GraphQL implementations will ensure that a response from the backend adheres to this schema before sending to the frontend. This makes it significantly easier as a frontend client of a GraphQL-based service as you have greater confidence over the returned data.\n### Fewer network requests from the frontend\n\nA second reason that people like GraphQL is to reduce the number of network requests from the frontend to the backend. Let's return to our previous examples with a single-page application. If I'm implementing a shopping cart for my single-page application with a typical REST backend, I might need to make a number of calls to render the shopping cart page: One call to fetch the current N items in the shopping cart N calls to fetch up-to-date details about each item, including the current price and availability This is the dreaded N + 1 problem. The N + 1 problem is commonly used as an argument against using ORMs as evidence they can make suboptimal queries that add additional load on your database. However, we're often making this same problem on the frontend! By using single-page applications with a RESTful backend implementation, our frontend needs to make a number of requests to render the page. Further, there may be a sequential nature to this, as I can't make the N calls to fetch the item details until my first request to retrieve the shopping cart contents. Now I have a waterfall of sequential requests that make my application seem sluggish. GraphQL reduces this problem by allowing a frontend client to make a single request that retrieves a graph of data. In a single GraphQL query, I can retrieve both my shopping cart and all of the items in the cart. As soon as this request finishes, I can paint the entire page without waiting for subsequent requests. Notice that this doesn't necessarily eliminate the N + 1 problem entirely. As we'll see below, it could move the N + 1 problem to the backend. The only thing GraphQL does here is eliminate the N + 1 problem from the frontend application and developer."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The application's performance appears to be hindered by a series of sequential requests, resulting in a sluggish user experience. GraphQL aims to address this issue by enabling a frontend client to make a single request that retrieves a graph of data. Within a single GraphQL query, it is possible to retrieve both the shopping cart and all of the items contained within it. Once this request is fulfilled, the entire page can be rendered without waiting for subsequent requests. However, it is important to note that GraphQL does not necessarily eliminate the N + 1 problem entirely. As will be discussed below, it could potentially shift the N + 1 problem to the backend. The primary advantage of GraphQL in this context is that it eliminates the N + 1 problem from the frontend application and developer's perspective.\n```"}, {"role": "assistant", "content": "Now I have a waterfall of sequential requests that make my application seem sluggish. GraphQL reduces this problem by allowing a frontend client to make a single request that retrieves a graph of data. In a single GraphQL query, I can retrieve both my shopping cart and all of the items in the cart. As soon as this request finishes, I can paint the entire page without waiting for subsequent requests. Notice that this doesn't necessarily eliminate the N + 1 problem entirely. As we'll see below, it could move the N + 1 problem to the backend. The only thing GraphQL does here is eliminate the N + 1 problem from the frontend application and developer."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```- There's an issue with multiple requests from the frontend causing sluggishness.\n- GraphQL allows the frontend to fetch all required data in a single request, reducing round trips.\n- In one GraphQL query, you can retrieve the shopping cart and its items, enabling rendering the entire page without waiting.\n- However, this doesn't completely eliminate the N+1 problem, it just moves it to the backend.\n- GraphQL provides flexible querying options for clients compared to traditional REST APIs.\n- The Backends for Frontends (BFF) pattern also aims to reduce frontend-backend calls, but it's less flexible than GraphQL.\n- BFFs are designed for specific frontend experiences, so changing the frontend requires backend changes too.\n- With GraphQL, you publish a schema, and clients can make any request against it without backend changes.\n- GraphQL backends use resolvers to retrieve specific data, which can lead to N+1 queries on the backend.\n- For example, a shopping cart query may fetch cart details and then make separate requests for each item's details.\n```"}, {"role": "assistant", "content": "Now I have a waterfall of sequential requests that make my application seem sluggish. GraphQL reduces this problem by allowing a frontend client to make a single request that retrieves a graph of data. In a single GraphQL query, I can retrieve both my shopping cart and all of the items in the cart. As soon as this request finishes, I can paint the entire page without waiting for subsequent requests. Notice that this doesn't necessarily eliminate the N + 1 problem entirely. As we'll see below, it could move the N + 1 problem to the backend. The only thing GraphQL does here is eliminate the N + 1 problem from the frontend application and developer.\n### Flexible querying patterns\n\nThe third and final benefit of GraphQL for the point of this post is that GraphQL provides more flexible querying options for clients. We saw above that GraphQL can eliminate the N + 1 problem from the frontend. Another alternative to reduce calls from the frontend to the backend is the Backends for Frontends (BFF) architecture pattern. With this pattern, backend APIs are implemented specifically for the frontends that will be calling them. For example, we may have an endpoint specifically for rendering our shopping cart page that assembles both the cart contents and updated details about the specific items on the backend before returning a complete response to the frontend. Once fully implemented, the BFF pattern looks a lot like GraphQL. I can get the full graph of data I need in a single request rather than by making multiple calls to my backend. The main difference between BFFs and GraphQL is in flexibility. BFFs are designed specifically for my frontend experiences, and thus changing my frontend experience requires changing my backend as well. This can mean pulling in additional teams, dealing with backward compatibility issues, and other factors. As a result, BFFs can be slower to evolve. In contrast, GraphQL has provides significantly more flexibility. With GraphQL, you publish a schema that describes the shape of your data. A client can make any request against that schema they need, and the GraphQL backend implementation will do the work to assemble the data. Thus, changing requirements on the frontend (usually) do not require backend changes as well. It's as simple as rewriting the GraphQL query for a page to change the desired data. Generally, GraphQL backends use isolated, focused resolvers that retrieve specific pieces of data. This can result in N + 1 queries on the backend. For example, imagine our shopping cart query below:\n\n```graphql\n{\n  cart(username: \"alexdebrie\") {\n    id\n    lastUpdated\n    items {\n      id\n      quantity\n      name\n      sku\n      price\n}\n```"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The GraphQL strategy involves letting out a schema that outlines the structure of your data. Clients are free to make any necessary requests against this schema, and the backend implementation of GraphQL will handle the assembly of the requisite data. Typically, frontend modifications do not necessitate backend changes, and the process can be as simple as reworking the GraphQL query relevant to a particular page to alter the required data. In general terms, GraphQL backends rely on separate, concentrated resolvers that obtain specific data components. This may generate N+1 question patterns on the backend. Consider our example of a shopping cart query:\n\n```graphql\n{\n  cart(username: \"alexdebrie\") {\n    id\n    lastUpdated\n    items {\n      id\n      quantity\n      name\n      sku\n      price\n}\n```\n## Points to Note\n\nObserving the above benefits, one conclusions that may be drawn is that GraphQL simplifies tasks for frontend developers. It allows them to be less careful about the returned data from an API. It eliminates the need for a cascade of network requests and the troubles with piecemeal errors. Importantly, they can make updates to the frontend without any backend support. The resolvers are so designed that they perform universally, regardless the initial query making it easy to format a response simply through feeding in a modified GraphQL query.\n\nThis underpins the higher level of GraphQL adoption among engineering teams that afford more influence to frontend developers. The concept itself is not erroneous. Most engineering teams inherently favor a specific kind of developer and optimize their systems accordingly. In my observation, REST API's simplify tasks for backend developers at the expense of making them more complex for clients, often these are the frontend developers.\n\n## Interaction between DynamoDB and GraphQL\n\nGaining more insights into the benefits of GraphQL, let's shift our focus to the interaction dynamics between DynamoDB and GraphQL. To start with, we need to know the fundamental facts about DynamoDB.\n```"}, {"role": "assistant", "content": "With GraphQL, you publish a schema that describes the shape of your data. A client can make any request against that schema they need, and the GraphQL backend implementation will do the work to assemble the data. Thus, changing requirements on the frontend (usually) do not require backend changes as well. It's as simple as rewriting the GraphQL query for a page to change the desired data. Generally, GraphQL backends use isolated, focused resolvers that retrieve specific pieces of data. This can result in N + 1 queries on the backend. For example, imagine our shopping cart query below:\n\n```graphql\n{\n  cart(username: \"alexdebrie\") {\n    id\n    lastUpdated\n    items {\n      id\n      quantity\n      name\n      sku\n      price\n}\n```\n## Key takeaways\n\nIf you look closely at the benefits above, you'll notice that GraphQL makes life quite a bit easier for frontend developers. They don't have to be as defensive about the data they get back from an API. They don't have to make a waterfall of network requests and deal with partial errors. And they can iterate on the frontend without requiring backend assistance. Because the resolvers are implemented generically, independent of the originating query, it's easy to reshape a response simply by changing the GraphQL query input.\n\nBecause of this, I've generally seen more GraphQL adoption in engineering teams where frontend developers have more sway. There is nothing inherently wrong with this. Most engineering teams have some inherent bias toward a particular type of developer and optimize their systems accordingly. I think REST APIs make life a lot easier for backend developers -- by simplifyng the amount of data on any particular request -- by pushing a lot of that complexity to clients (often the frontend developers).\n## DynamoDB and GraphQL\n\nNow that we know a bit about some of the benefits of GraphQL, let's take a look at how DynamoDB and GraphQL interact. First, we'll cover some basics about DynamoDB."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Frontend developers can make changes without needing help from backend developers, due to the fact that resolvers are implemented in a versatile nature, detached from the initiating query. Altering the GraphQL query input easily reshapes responses. \n\nTypically, GraphQL is more widely used in engineering teams where there is more influence held by frontend developers. There is no fundamental problem with this situation. Most engineering teams lean towards a certain type of developer and will adjust their systems to suit that preference. It's my opinion that REST APIs make tasks considerably less complex for backend developers. They offload a lot of the intricacies towards clients, which are often frontend developers, by reducing the data quantity within each specific request.\n```"}, {"role": "assistant", "content": "And they can iterate on the frontend without requiring backend assistance. Because the resolvers are implemented generically, independent of the originating query, it's easy to reshape a response simply by changing the GraphQL query input.\n\nBecause of this, I've generally seen more GraphQL adoption in engineering teams where frontend developers have more sway. There is nothing inherently wrong with this. Most engineering teams have some inherent bias toward a particular type of developer and optimize their systems accordingly. I think REST APIs make life a lot easier for backend developers -- by simplifyng the amount of data on any particular request -- by pushing a lot of that complexity to clients (often the frontend developers)."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The text discusses several aspects of GraphQL and DynamoDB. Regarding GraphQL, it highlights how the resolvers are implemented generically, independent of the originating query, allowing for easy reshaping of responses by modifying the GraphQL query input. It notes that this has led to more GraphQL adoption among engineering teams where frontend developers have a stronger influence, as it simplifies data handling for them compared to REST APIs, which can make life easier for backend developers by limiting the amount of data per request.\n\nMoving on to DynamoDB, the text provides background information on this NoSQL database from AWS. It emphasizes DynamoDB's optimization for extreme predictability and consistency of performance, achieved by restricting how it can be used and enforcing the use of a primary key for most access patterns. This primary key structure, combined with the lack of features like JOINs and aggregations across multiple records, ensures single-digit millisecond response times on individual items, regardless of database size.\n\nThe text further explains that DynamoDB has clear, strict limits, such as a maximum item size of 400KB and a limit on the number of concurrent reads and writes against a subset of data. These limitations are in place to maintain consistency and predictability, turning performance into a binary decision of success or failure rather than allowing for slowly degrading application performance under load.\n\nDue to these choices made by the DynamoDB team, data modeling in DynamoDB differs from relational databases. Instead of designing data in an abstract, normalized way and then implementing queries, DynamoDB requires considering access patterns first and designing the table accordingly. This often results in denormalized data structures tailored to specific requirements.\n\nThe text introduces the concept of single-table design, where related data is assembled in a single DynamoDB table to avoid multiple requests and the N+1 problem. By using a Query operation, both the repository and the issue items can be fetched in a single request, turning an N+1 query into a single query without the CPU overhead of a SQL join operation.\n```"}, {"role": "assistant", "content": "And they can iterate on the frontend without requiring backend assistance. Because the resolvers are implemented generically, independent of the originating query, it's easy to reshape a response simply by changing the GraphQL query input.\n\nBecause of this, I've generally seen more GraphQL adoption in engineering teams where frontend developers have more sway. There is nothing inherently wrong with this. Most engineering teams have some inherent bias toward a particular type of developer and optimize their systems accordingly. I think REST APIs make life a lot easier for backend developers -- by simplifyng the amount of data on any particular request -- by pushing a lot of that complexity to clients (often the frontend developers).\n### Background on DynamoDB\n\nI'm not going to go deep on the details of DynamoDB, but I do want to discuss some high-level points in order to help the discussion later.\n\nDynamoDB is a NoSQL database from AWS. It optimizes for extreme predictability and consistency of performance. It wants to provide the exact same performance for a query when you have 1 MB of data as when you have 1 GB, 1 TB, or even 1 PB of data. To do that, DynamoDB intentionally restricts how you can use it. It forces you to use a primary key for most of your access patterns. This primary key is a combination of a hash key and a B-tree to allow for fast, predictable performance when operating on an individual item or when retrieving a range of contiguous items. Because of this primary key structure, you should see single-digit millisecond response times on individual items regardless of your database size.\n\nAdditionally, DynamoDB removes certain features provided by other databases. You can't do JOINs in DynamoDB. You can't do aggregations across multiple records in DynamoDB. Both of these operations can operate on an unbounded number of items with unpredictable performance characteristics. DynamoDB will not let you write a bad query and will not provide features that allow you to do so.\n\nFinally, DynamoDB has clear, strict limits in certain areas. It won't let you create an item over 400KB as a way to force you into smaller, more targeted items. More importantly, it has a limit on the number of concurrent reads and writes against a subset of your data. If you exceed that limit for an individual second, your request will be throttled and you'll need to try again. Again, this is in pursuit of consistency and predictability. It turns performance into a binary decision -- single-digit response that either succeeded or failed -- rather than having slowly degrading application performance under load.\n\nBecause of the choices made by the DynamoDB team in order to provide that consistency, you need to model your data differently. Most importantly, you have to think about how you will actually use your data.\n\nIn a relational database, you design your data in an abstract, normalized way. Then you implement the queries and potentially add indexes to assist in performance.\n\nWith DynamoDB, this pattern is flipped. You consider your access patterns first, then design your table to handle those access patterns. Your data probably won't be normalized as it would in a relational database. It's going to be handcrafted for your specific requirements.\n\nLet's think back to our GitHub repository example before. Because DynamoDB does not have joins, fetching a repository and the ten most recent issues in the repository would take at least two requests to the database if you had two different tables. Depending on how you modeled the primary keys for the tables, it could be two sequential requests, which will increase latency. We're back to the old N + 1 problem we discussed before (or at least the 1 + 1 problem :)).\n\nThis is where single-table design comes in. If you know you have an access pattern where you need the repository and the ten most recent issues, you can assemble the items next to each other in a single DynamoDB table. Then, you can use a Query operation to fetch both the repository and the issue items in a single request. You've turned your N + 1 query into a single query, and you've done it in a way that doesn't burn through CPU like you may with a SQL join operation."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Ensuring security for CI/CD pipelines is generally carried out through the use of dedicated roles rather than IAM users - one role per pipeline. The person or entity allowed to assume these roles has become a matter for consideration. Several 3rd-party providers, including GitHub Actions, offer broad support for identity federation, meaning there's no longer a necessity to store IAM credentials in CI/CD tools, significantly reducing the risk of security breaches.\n\nHowever, despite these precautions, vulnerabilities still exist. For instance, pipelines can be compromised via supply chain attacks, such as through corruption of relied-on Docker images within the CI/CD pipeline, or violation of static analysis tools like eslint. Consequently, the ideal degree of limitation on permissions for a CI/CD role is a recurring concern at workshops such as Production-Ready Serverless. \n\nA typical initial response might be to restrict the CI/CD role to its absolute minimum requirements as part of adhering to the principle of least privilege. Let's examine two typical strategies for achieving this limitation:\n\n1. Beginning with no permissions and continually adding them until the role can deploy the application.\n2. First bestowing the AdministratorAccess policy and allowing the role universal scope. Then, tracking the role's activities using CloudTrail over a certain timeframe. Finally, compiling a specifically tailored IAM permissions set based on the gathered CloudTrail data.\n\nWhile the first method seems exhaustive and inefficient and is suggested to be avoided; Much to their discredit, both strategies possess significant long-term disadvantages.\n```"}, {"role": "assistant", "content": "# How to secure CI/CD roles without burning production to the ground\n\nBy now, most of us have moved away from using IAM users for CI/CD pipelines. Instead, we\u2019d use dedicated CI/CD roles, one for each pipeline. This forces us to consider who can assume this role. Identity federation is widely supported by 3rd-party providers such as GitHub Actions [1]. So, no more putting IAM credentials in CI/CD tools and worry that they might be compromised in a security breach [2].\n\nHowever, attackers can still compromise the pipeline through supply chain attacks. For example, by compromising a Docker image we depend on in our CI/CD pipeline. Or by compromising static analysis tools such as eslint [3]. So, the question of \u201c How best to limit CI/CD role\u2019s permissions?\u201d has come up many times during my Production-Ready Serverless [4] workshop. Your instinct might be to lock down the CI/CD role to just what it needs. Because we all want to follow the principle of least privilege. There are different ways to achieve this. Here are two common approaches:\n\n1. Start with a blank slate and add permissions to the role until it\u2019s able to deploy your application.\n2. Start with the AdministratorAccess policy and allow the role to do everything. Use CloudTrail to track the actions performed by the role over some time. Then use the CloudTrail data to create a tailored set of IAM permissions.\n\nThe first approach is very time-consuming and laborious and is best avoided. But in truth, both approaches have significant drawbacks in the long run."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```One might think to limit the CI/CD role to its necessity in complying with the principle of least privilege. This goal can be attained through various means, here are two prevalent methods:\n\n1. Begin with zero permissions and progressively grant the role permissions until it can deploy the application.\n2. Initiate with an AdministratorAccess policy permitting the role to perform all actions. The actions carried out by the role over a period can be logged using CloudTrail, and this data can be utilized to develop a customized IAM permissions set.\n\nThe initial method is highly labor-intensive and time-consuming, hence best to be avoided. However, both methods are flawed and pivotal drawbacks emerge in the long run. \n\n## Unexpected Rollbacks\n\nRollbacks in CloudFormation seldom occur, particularly for individuals who do not manually script CloudFormation templates. Therefore, when a deployment fails for the first time, it exposes that the CI/CD role lacks many permissions. This situation can create complications leading to stagnation of your stack in the ROLLBACK_FAILED state. You can only resume once the permissions deficiencies are addressed. Essentially, establishing a least privileged CI/CD role demands double effort as it is also necessary to account for all rollback permissions.\n```"}, {"role": "assistant", "content": "Your instinct might be to lock down the CI/CD role to just what it needs. Because we all want to follow the principle of least privilege. There are different ways to achieve this. Here are two common approaches:\n\n1. Start with a blank slate and add permissions to the role until it\u2019s able to deploy your application.\n2. Start with the AdministratorAccess policy and allow the role to do everything. Use CloudTrail to track the actions performed by the role over some time. Then use the CloudTrail data to create a tailored set of IAM permissions.\n\nThe first approach is very time-consuming and laborious and is best avoided. But in truth, both approaches have significant drawbacks in the long run.\n## Rollback surprises\n\nFor most people, CloudFormation rollbacks don\u2019t happen often. Especially for those of us who don\u2019t write CloudFormation templates by hand. So when a deployment fails for the first time, you find out that the CI/CD role is missing a whole bunch of permissions! This can leave you in an awkward position where your stack is stuck in the ROLLBACK_FAILED state. And you are stuck until you resolve the missing permissions. In essence, crafting a least privileged CI/CD role takes twice as much work as you think. Because you also need to factor in all the permissions for rollbacks."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```## Unanticipated rollback frustrations \n\nCloudFormation rollbacks do not commonly occur for several individuals, more so for those who do not manually write CloudFormation templates. As such, when a deployment error occurs for the initial time, there emerges the revelation that there are numerous permissions lacking in the CI/CD role, an uncomfortable scenario. This situation can result in the stack being mired in a ROLLBACK_FAILED state, leaving you with a challenge until the permissions' discrepancies are addressed. Ultimately, the conception of a least privilege CI/CD role is double the anticipated effort, as there's a need to consider all the permissions for potential rollbacks.\n## Impact on productivity \n\nArchitectures dynamism is fundamental, where it needs to adapt to the application and business demands. The introduction of any new service necessitates even a revision of the CI/CD role. As an example, prior to utilizing OpenSearch or Bedrock, necessary adjustments to the CI/CD role are required. Where the application team does not have oversight over the CI/CD role within organizations, this process can significantly impact productivity. It can result in frequent interactions between teams, impeding productivity. Moreover, all services may undergo this process, as each has their distinct CI/CD pipeline. This ongoing friction is essentially a burden on productivity, causing issues such as stifling creativity and the lagging integration of new services into the tech stack. All because any new service integrates needs changes to the CI/CD permissions, leading to unnecessary bureaucratic processes.\n```"}, {"role": "assistant", "content": "## Rollback surprises\n\nFor most people, CloudFormation rollbacks don\u2019t happen often. Especially for those of us who don\u2019t write CloudFormation templates by hand. So when a deployment fails for the first time, you find out that the CI/CD role is missing a whole bunch of permissions! This can leave you in an awkward position where your stack is stuck in the ROLLBACK_FAILED state. And you are stuck until you resolve the missing permissions. In essence, crafting a least privileged CI/CD role takes twice as much work as you think. Because you also need to factor in all the permissions for rollbacks.\n## A tax on productivity\n\nYour architecture is not static. It needs to evolve with your application and your business needs. Every time you introduce a new service, you also need to revise the CI/CD role. Want to start using OpenSearch? Not before you update the CI/CD role! Want to start using Bedrock? Not before you \u2026 you get the point. In organizations where the application team doesn\u2019t own the CI/CD role, this productivity tax can be steep. It requires many back-and-forth between teams and hits productivity hard. And you likely have to do these for every service because every service has its own CI/CD pipeline. This friction is a constant tax on your productivity. It hampers innovation and delays the adoption of new services into the tech stack. Because introducing a new service means new CI/CD permissions. Who in their right mind wants to deal with all that bureaucracy?"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```In order to utilize OpenSearch, it is necessary to update the CI/CD role. The same requirement applies to the use of Bedrock. Particularly in larger businesses where the application team does not oversee the CI/CD role, this can create a significant drawback to productivity. It often necessitates extensive discussions between teams, slowing down work progress. Additionally, it is typically required for each individual service, as each service has its own CI/CD pipeline. This bureaucratic process imposes a permanent burden on productivity. It stymies innovation and impedes the implementation of new services into the technical stack, as the initiation of a new service calls for new CI/CD permissions.\n\nAdopting least privileged CI/CD roles for security reasoning can create a false sense of safety. These roles serve to minimize the potential damage of a security breach. By safeguarding the CI/CD pipeline, it ultimately limits what a hacker can accomplish. The CI/CD role, nonetheless, usually requires the deployment of IAM roles and Lambda functions. Given a security breach, a least privileged CI/CD role does not provide sufficient protection against a hacker. The attackers could manipulate the CI/CD role to set up confused deputies in order to act according to their instructions. For instance, an attacker may use the CI/CD role to create IAM roles for them to use or Lambda functions to carry out harmful actions. These dangers can be somewhat lessened using permission boundaries, but that demands a considerable amount of work and precision. Once again, it would be needed for every CI/CD role.\n```"}, {"role": "assistant", "content": "Want to start using OpenSearch? Not before you update the CI/CD role! Want to start using Bedrock? Not before you \u2026 you get the point. In organizations where the application team doesn\u2019t own the CI/CD role, this productivity tax can be steep. It requires many back-and-forth between teams and hits productivity hard. And you likely have to do these for every service because every service has its own CI/CD pipeline. This friction is a constant tax on your productivity. It hampers innovation and delays the adoption of new services into the tech stack. Because introducing a new service means new CI/CD permissions. Who in their right mind wants to deal with all that bureaucracy?\n## They are not as safe as you think\n\nThe reason why we pursue least privileged CI/CD roles is to reduce the blast radius of a security breach. If an attacker compromises our CI/CD pipeline, we want to limit what the attacker can do. However, the CI/CD role likely needs to deploy IAM roles and Lambda functions. In the event of a compromise, a least privileged CI/CD role is not enough to stop the attacker. Because the attackers can use the CI/CD role to create confused deputies to act on their behalf. For example, attackers can use the CI/CD role to create IAM roles for them to assume. And they can also create Lambda functions to carry out malicious activities. We can mitigate these risks to some extent with permission boundaries. But that requires lots more work and it\u2019s hard to get right. And again, you have to do it for every CI/CD role."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```If our CI/CD pipeline gets breached, we want to lessen the potential damage the attacker can inflict. Often, the CI/CD role needs to implement IAM roles and Lambda functions, so if there's a breach, a least privileged CI/CD role may not be effective enough in preventing the attacker's actions. Attackers may exploit the CI/CD role by creating confused deputies, for instance, they could make IAM roles that they then assume control of, or design Lambda functions with harmful actions. A certain amount of risk reduction can be achieved with permission boundaries, but this demands more effort and precision, and it needs to be applied to every CI/CD role.\n\nChoosing a permissive role that's tough to misuse is recommended, instead of strictly adhering to the least privileged CI/CD roles, which with time yields less return on investment. This is because a significant amount of output is needed to manage improbable security risks. Listed here are some suggestions to securitize a CI/CD pipeline.\n\n- Assign a distinct AWS account to each environment. Everyone should practice this, as it provides protection between environments. The damage of a breach is limited to within the account boundaries. If your dev environment is infiltrated, it implies your production data still remains inaccessible to the eavesdropper.\n  \n- Particularly for sizable organizations, further measures may be helpful. For instance, allocating at least one account for every team in each environment can prevent the impact of one team's mistakes on others.\n  \n- Crucial business systems might benefit from having personalized accounts also \u2013 one per service for each controlled environment. It isn\u2019t always required, but it would save important systems from the impact they could suffer due to any problem faced by other systems.\n```"}, {"role": "assistant", "content": "If an attacker compromises our CI/CD pipeline, we want to limit what the attacker can do. However, the CI/CD role likely needs to deploy IAM roles and Lambda functions. In the event of a compromise, a least privileged CI/CD role is not enough to stop the attacker. Because the attackers can use the CI/CD role to create confused deputies to act on their behalf. For example, attackers can use the CI/CD role to create IAM roles for them to assume. And they can also create Lambda functions to carry out malicious activities. We can mitigate these risks to some extent with permission boundaries. But that requires lots more work and it\u2019s hard to get right. And again, you have to do it for every CI/CD role.\n## Prefer a permissive role that is hard to abuse\n\nIt\u2019s counterproductive to blindly pursue least privileged CI/CD roles. Beyond a certain point, you get diminished return-on-investments. That is, you have to work much harder to address security risks that are unlikely to happen. With that in mind, here\u2019s my preferred approach to securing a CI/CD pipeline.\n\nUse a separate AWS account for each environment\n\nThis is something everyone should do. It insulates environments from each other. If one environment is compromised, it\u2019s contained by the account boundary. If an attacker compromises your dev environment, they won\u2019t be able to access your production data.\nFor large organizations, you should go a step further. I recommend having at least one account per team per environment. That way, teams are insulated from other teams\u2019 mistakes.\nFor business critical systems, they should have their own set of accounts too. That is one account per service per environment. It\u2019s not always necessary. But it\u2019s a good idea to protect these business critical systems from other systems."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Environments should be divided from each other for security purposes. In the event that one environment is compromised, it would be restricted from impacting other environments notably your production data, due to the account boundary. In bigger organisations, there should be at least one account for each team per environment serving as a protective measure. For essential business systems, a separate set of accounts is advisable.\n \nUtilise Service Control Policies (SCPs) to prevent access to unused resources. Monitor resources that aren't in use and deny all activities related to them which decreases possible targets for attacks.\n\nAttribute-Based Access Control (ABAC)'s effectiveness continues to progress and several services now support aws:ResourceTag/${TagKey} and aws:PrincipalTag/${TagKey} decisions. They can be used to block attackers from accessing and generating resources. They can be used in the IAM permissions of the CI/CD role or implemented through a permission boundary. \n\nFor added security, the capability of the CI/CD role to self-identify should be denied. This makes it difficult for the attacker to identify the required tags to generate new resources. \n\nA high level of vigilance should be maintained. Specific services that may become subjects to malicious code execution or privilege escalation by the attackers include Lambda, EC2, ECS, IAM, CodePipeline and CloudFormation. Most of these services support the aws:ResourceTag/${TagKey} condition which prevents the attacker from generating new resources.\n```"}, {"role": "assistant", "content": "It insulates environments from each other. If one environment is compromised, it\u2019s contained by the account boundary. If an attacker compromises your dev environment, they won\u2019t be able to access your production data.\nFor large organizations, you should go a step further. I recommend having at least one account per team per environment. That way, teams are insulated from other teams\u2019 mistakes.\nFor business critical systems, they should have their own set of accounts too. That is one account per service per environment. It\u2019s not always necessary. But it\u2019s a good idea to protect these business critical systems from other systems.\n## Service Control Policies (SCPs)\n\nUse SCPs to deny access to unused resources. These broad strokes include access to unused AWS regions and unused services. This way, you can eliminate large parts of AWS as possible attack surfaces.\nFor example, if you\u2019re not running any EC2 instances, then deny all EC2 activities. That\u2019s one less target for crypto-jacking attacks. But hey, ECS is the new EC2 for crypto-jacking. If you can\u2019t disable ECS as a whole, then at least deny access to the regions where you don\u2019t have any containers.\n## Attribute-Based Access Control (ABAC)\n\nSupport for ABAC is improving all the time, although it\u2019s still lacklustre in its current form. But many services already support the `aws:ResourceTag/${TagKey}` and `aws:PrincipalTag/${TagKey}` conditions. These conditions can be used to stop attackers from accessing and creating resources. They can be added to the IAM permissions of the CI/CD role. Or they can be enforced through a permission boundary.\nTo make them more effective, the CI/CD role should be denied the ability to find out about itself. That way, the attacker can\u2019t find out what tags the CI/CD role has and what tags they need to use to create new resources.\nDoing so helps us stop the attacker from accessing our resources through the CI/CD role. But they might target the following services to execute malicious code or escalate their privilege:\n- Lambda\n- EC2\n- ECS\n- IAM\n- CodePipeline\n- CloudFormation\nLuckily, all these services (and many others) support the `aws:ResourceTag/${TagKey}` condition. We can use this condition to stop the attacker from creating new resources."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The enforcement of my idea occurs via a permission boundary. This could be further improved by making sure that the CI/CD role can't obtain information about itself in order to better protect it from attackers. This prevents them from understanding what tags are attached to the CI/CD role, thereby restricting their ability to create new resources through this role. This greatly reduces the chances of them being able to access our resources. \n\nYet the above only prevents one mode of attack. The attacker still has the ability to execute malicious codes or escalate their privileges through several different services such as Lambda, EC2, ECS, IAM, CodePipeline and CloudFormation. Thankfully though, all these services and quite a number of others support the `aws:ResourceTag/${TagKey}` feature, another line of defense that could help protect against attackers creating fresh resources in their bid to access our systems. \n\nHaving done all this to protect our resources and operations, we actually get quite a good few benefits: \n\n- We cut down on the potential areas from which we could be attacked by eliminating unused regions and services. \n- A possible attack from any direct exploitation becomes limited to specific teams and services. \n- The CI/CD role is far more difficult to misuse by attackers due to the requirement for resources to be correctly tagged. Plus, the CI/CD role's inability to describe itself further obstructs an attacker's ability to easily figure this out. \n\nNow we can permit ourselves to have a more permissive CI/CD role that's less of a hassle to create and even maintain. This is because even with a more open CI/CD role, it becomes less attractive for misuse, as it's more difficult to totally exploit it due to the safeguards in place.\n\nWhen I say 'permissive', I'm not suggesting going fully with 'AdministratorAccess.' Yet, you are permitted to largely allocate read and write access to virtually all services you use, rather than spending a lot of time refining the specifics down to the teeth.\n```"}, {"role": "assistant", "content": "Or they can be enforced through a permission boundary.\nTo make them more effective, the CI/CD role should be denied the ability to find out about itself. That way, the attacker can\u2019t find out what tags the CI/CD role has and what tags they need to use to create new resources.\nDoing so helps us stop the attacker from accessing our resources through the CI/CD role. But they might target the following services to execute malicious code or escalate their privilege:\n- Lambda\n- EC2\n- ECS\n- IAM\n- CodePipeline\n- CloudFormation\nLuckily, all these services (and many others) support the `aws:ResourceTag/${TagKey}` condition. We can use this condition to stop the attacker from creating new resources.\n## A permissive CI/CD role\n\nAfter doing the above:\n- We have removed the attack surface associated with unused regions and services.\n- We have limited the blast radius of a compromise to individual teams and services.\n- We have made it difficult for attackers to abuse the CI/CD role by requiring correct tags on resources. Which the attacker is not able to figure out easily because the CI/CD role is not allowed to describe itself.\nNow we can allow ourselves to have a permissive CI/CD role that is not a pain in the ass to create and maintain! Because the role will be difficult to abuse, and there is a ceiling to what it can actually do.\nBy \u201cpermissive\u201d, I don\u2019t necessarily mean \u201cAdministratorAccess\u201d. But you can give broad read & write permissions to all the services you use. Instead of picking out just the actions you need with a fine comb!"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```AWS Step Functions and Lambda functions serve different purposes, although they can be used in conjunction. Step Functions is a service for orchestrating and coordinating multiple AWS services, including Lambda functions, to build serverless applications. Lambda functions, on the other hand, are used for running code in response to events or requests.\n\nUsing Step Functions for orchestration can provide benefits such as:\n\n1. Separating the workflow logic from the business logic, making it easier to maintain and update.\n2. Visualizing and monitoring the workflow execution, which can aid in debugging and troubleshooting.\n3. Handling retries, error handling, and state management, which can be complex to implement in code.\n4. Integrating with various AWS services, allowing for complex workflows spanning multiple services.\n\nHowever, Step Functions may introduce additional complexity and overhead, making it less suitable for simple use cases. In such cases, implementing the entire orchestration logic within a Lambda function might be more straightforward.\n\nThe decision to use Step Functions or handle orchestration within a Lambda function depends on factors like the complexity of the workflow, the need for visibility and monitoring, the number of services involved, and the team's familiarity with the services. Simple workflows with few steps and services involved may be easier to implement and maintain within a Lambda function, while complex workflows with multiple decision points, retries, and service integrations could benefit from using Step Functions.\n```"}, {"role": "assistant", "content": "# When to use Step Functions vs. doing it all in a Lambda function\n\nI\u2019m a big fan of AWS Step Functions. I use it to orchestrate all sorts of workflows, from payment processing to map-reduce jobs. Why it\u2019s yet another AWS service you need to learn and pay for. And it introduces additional complexities, such as:\n- It\u2019s hard to test.\n- Your business logic is split between configuration and code.\n- New decision points. Such as whether to use Express Workflows or Standard Workflows.\n\nSo it\u2019s fair to ask \u201cWhy should we even bother with Step Functions?\u201d when you can do all the orchestration in code, inside a Lambda function. Let\u2019s break it down."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```# Comparing Step Functions and Lambda Function \n\nAWS Step Functions offer workflow orchestration solutions for various tasks like payment processing and map-reduce jobs. This service requires additional learning and costs and adds complexity to your systems due to:\n- Difficult testing procedures.\n- Partition of business logic between configuration and code.\n- A necessity to choose between Express Workflows or Standard Workflows.\n\nAn alternative to using Step Functions is incorporating all the orchestration within the Lambda function. Here's an evaluation of the pros of this approach.\n\n## Pros of Using Lambda\n\n1. Performing all orchestration processes in code can be simpler. Familiarity is guaranteed since every aspect one can achieve with Step Functions can be replicated with a few code lines. An example is as follows:\n\n```javascript\nmodule.exports.handler = async (event) => {\n    // error handling\n    try {\n        await doX()\n    } catch (err) {\n        // handle errors\n    }\n    // branching logic\n    if (condition === true) {\n        await doSomething()\n    } else {\n        await doSomethingElse()\n    }\n    // parallelism\n    const promises = event.input.map(x => doY())\n    const results = await Promise.all(promises)\n    return results\n}\n```\n\n2. This may be more economical. Step Functions would typically use Lambda for Task states. Hence, you will end up paying for both Lambda invocations and Step Function state transitions. State transitions cost $25 per million, making it one of the more expensive AWS services. Paying for two services could be costlier than a single one.\n\n3. It may offer better scalability. Using both Step Functions and Lambda functions exposes you to the throughput limits of both services. Step Functions have certain limitations on the cumulation of state transitions and execution starts per second. The Lambda function only limits you by the concurrent executions limit, regular throughput limitations on the no. of concurrent executions. These cost and scalability points, however, depend on the specific architecture. Planning can make them moot. Recent scaling changes on Lambda reinforce this.\n\nThe use of Step Functions versus orchestrating everything in a Lambda function warrants consideration and holistic analysis.\n```"}, {"role": "assistant", "content": "# When to use Step Functions vs. doing it all in a Lambda function\n\nI\u2019m a big fan of AWS Step Functions. I use it to orchestrate all sorts of workflows, from payment processing to map-reduce jobs. Why it\u2019s yet another AWS service you need to learn and pay for. And it introduces additional complexities, such as:\n- It\u2019s hard to test.\n- Your business logic is split between configuration and code.\n- New decision points. Such as whether to use Express Workflows or Standard Workflows.\n\nSo it\u2019s fair to ask \u201cWhy should we even bother with Step Functions?\u201d when you can do all the orchestration in code, inside a Lambda function. Let\u2019s break it down.\n## Lambda pros\n\n1. Doing all the orchestration in code is simpler. It\u2019s more familiar. Everything you can do in Step Functions, you can do with just a few lines of code. Case in point:\n\n```javascript\nmodule.exports.handler = async (event) => {\n    // error handling\n    try {\n        await doX()\n    } catch (err) {\n        // handle errors\n    }\n    // branching logic\n    if (condition === true) {\n        await doSomething()\n    } else {\n        await doSomethingElse()\n    }\n    // parallelism\n    const promises = event.input.map(x => doY())\n    const results = await Promise.all(promises)\n    return results\n}\n```\n\n2. It\u2019s likely cheaper. A Step Functions state machine would likely use Lambda for its Task states. In which case, you\u2019d end up paying for both:\n- The Lambda invocations. There are multiple invocations instead of just one. However, the total billable execution time should be similar.\n- Step Functions state transitions. At $25 per million state transitions, it\u2019s one of the more expensive services in AWS. Paying for two services is likely more expensive than paying for just one.\n3. It\u2019s likely more scalable. When you use both Step Functions and Lambda functions (for the Task states) you are constrained by the throughput limits of both services. Step Functions Standard Workflows have modest limits on the no. of state transitions and the no. of executions you can start per second. Both of these limits can be raised. So with proper planning, they wouldn\u2019t be an issue. Without Step Functions, you are limited only by the concurrent executions limit on Lambda. Similarly, Lambda has default throughput limits on the no. of concurrent executions. Again, with proper planning, and given the recent scaling changes for Lambda, you will be OK. Both the cost and scalability arguments are situational and depend on several architectural choices."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The constraints of using both Step Functions and Lambda functions lie in the throughput limits of both services. The limits on the number of state transitions and executions initiated per second are modest in Step Functions Standard Workflows. However, these limits can be increased. Proper planning negates these issues. Without Step Functions, the constraining factor is the limit on concurrent executions on Lambda. By default, Lambda has limits on the number of concurrent executions, but these can also be adjusted through planning. Recent scaling adjustments for Lambda helps to achieve desired scalability. The considerations for cost and scalability are situational and influenced by various architectural choices.\n\nDisadvantages of Lambda:\n\n1. Existence of cold starts.\n2. Maximum duration is restricted to 15 minutes.\n3. Paying for execution time by the millisecond makes it a less than optimal solution when the function is waiting for something to finish, as this time and money is wasted.\n\nAdvantages of Step Functions:\n\n1. The ability to visualize workflows simplifies troubleshooting, making it accessible even to non-technical team members such as product owners or customer support teams.\n2. The built-in audit history includes comprehensive data such as dates and times when a state started, inputs, outputs, error messages, and stack traces.\n3. Direct service integration with almost all AWS services makes it possible to implement a full workflow without any Lambda functions.\n4. Not using Lambda avoids cold starts, leading to more consistent performance.\n5. Execution time is extended, and a Standard Workflow can run up to a year.\n6. Callback patterns are beneficial for supporting human interventions in a workflow.\n7. Standard Workflows are a cost-efficient way to wait as you\u2019re charged only for the state transitions and not the duration.\n8. More robust error handling can be implemented, essential for operations critical to the business.\n```"}, {"role": "assistant", "content": "When you use both Step Functions and Lambda functions (for the Task states) you are constrained by the throughput limits of both services. Step Functions Standard Workflows have modest limits on the no. of state transitions and the no. of executions you can start per second. Both of these limits can be raised. So with proper planning, they wouldn\u2019t be an issue. Without Step Functions, you are limited only by the concurrent executions limit on Lambda. Similarly, Lambda has default throughput limits on the no. of concurrent executions. Again, with proper planning, and given the recent scaling changes for Lambda, you will be OK. Both the cost and scalability arguments are situational and depend on several architectural choices.\n## Lambda cons\n\n1. Cold starts.\n2. 15 mins max duration.\n3. Not good for waiting. Because you pay for execution time by the milliseconds. It\u2019s a poor solution when it comes to waiting for something to happen because all that execution time (and money) is wasted.\n## Step Functions pros\n\n1. The visual workflows make it very easy to debug problems. This is true even for non-technical team members, such as product owners and customer support teams.\n2. Step Functions has a built-in audit history of everything that happened, including:\n- When did a state start?\n- What were the input and output?\n- Error message and stack trace.\n3. Step Functions have direct service integration with almost every AWS service. So it\u2019s possible to implement an entire workflow without needing any Lambda functions.\n4. No Lambda, no cold starts. No cold starts = more predictable performance.\n5. Long execution time. A Standard Workflow can run for up to a year.\n6. Callback patterns are a great way to support human decisions (e.g. approve a deployment request) in a workflow.\n7. Standard Workflows are arguably the most cost-efficient way to wait. Because you don\u2019t pay for the duration, only the state transition.\n8. You can implement more robust error handling. This is important for business-critical workflows."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```A common error people often make when using Kinesis with Lambda\n```"}, {"role": "assistant", "content": "# The one mistake everyone makes when using Kinesis with Lambda"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Title: Common Mistake in the Use of Kinesis Combined with Lambda\n\nMany developers harness AWS Kinesis and Lambda in tandem to handle considerable data quantities in real time processing. However, a typical error frequently occurs when fusing these two services.\n\nThere exist recommended practices for configuring the EventSourceMapping for Kinesis in Lambda, which include:\n- Setting up an OnFailure destination for unsuccessful records.\n- Turning on the BisectBatchOnFunctionError.\n- Surpassing the MaximumRetryAttempts. Deciding on a value which clerly permits failed messages several retries before transmitting them to the OnFailure destination.\n\nImplementing these suggestions is a forthright solution for creating a robust data processing system and mitigating the impact of unsavoury messages. Nonetheless, an integral piece of information has been overlooked: the exerted payload in the OnFailure location does not encompass the event payload.\n\nThe OnFailure location is anticipated to capture the following type of data:\n\n```json\n{\n  \"requestContext\": {\n    \"requestId\": \"cf6fa2a6-48e2-49a6-bc23-7ed2b154b176\",\n    \"functionArn\": \"arn:aws:lambda:us-east-1:xxx:function:kinesis-processor\",\n    \"condition\": \"RetryAttemptsExhausted\",\n    \"approximateInvokeCount\": 4\n  },\n  \"responseContext\": {\n    \"statusCode\": 200,\n    \"executedVersion\": \"$LATEST\",\n    \"functionError\": \"Unhandled\"\n  },\n  \"version\": \"1.0\",\n  \"timestamp\": \"2023-12-10T22:06:23.446Z\",\n  \"KinesisBatchInfo\": {\n    \"shardId\": \"shardId-000000000000\",\n    \"startSequenceNumber\": \"4964723877401875844914379084747495239034204468389137\",\n    \"endSequenceNumber\": \"496472387740187584491437909411123017485931487851477729\",\n    \"approximateArrivalOfFirstRecord\": \"2023-12-10T22:05:59.280Z\",\n    \"approximateArrivalOfLastRecord\": \"2023-12-10T22:06:19.196Z\",\n    \"batchSize\": 3,\n    \"streamArn\": \"arn:aws:kinesis:us-east-1:xxx:stream/MyKinesisStream\"\n  }\n}\n```\n\nThe KinesisBatchInfo pertains to the shard ID, sequence numbers, and batch size but, it doesn't keep hold of the actual data payload, the very crux of the event needed for data processing and analysis.\n\nSuch an oversight can give rise to several significant problems:\n1. Inability to troubleshoot problems easily: Leaving the actual event data out makes identifying failure modes a challenging task.\n2. Possible data loss: In case the record cannot be retrieved before it gets erased from the stream, the data is lost irrevocably. Kinesis stream's standard retention is set at 24 hours. If a failure arises on a non-working day, the likelihood of developer awareness could take until their return to work, leading to the lacking accessibility to the failure in the stream as it's no longer available.\n3. Extreme complexity levels: Developers have to formulate alternative methodologies to capture and reserve the actual event data.\n\nMany, lacking this knowledge, are taken by surprise when they meet an unsuccessful message attempt for the first time.\n```"}, {"role": "assistant", "content": "# The one mistake everyone makes when using Kinesis with Lambda\n\n## AWS Kinesis and Lambda are a great combo for processing large amounts of data in real-time. However, there\u2019s a common oversight that many developers make when integrating these two services.\nThere are established best practices for configuring Lambda\u2019s EventSourceMapping for Kinesis:\n- Configure an OnFailure destination for failed records.\n- Enable BisectBatchOnFunctionError.\n- Override MaximumRetryAttempts. Choose a value that gives failed messages a reasonable number of retries before sending them to the OnFailure destination.\n\nThese are great advice for building a resilient data processing pipeline and guarding against poison messages. But one crucial detail is missing: the payload captured in the OnFailure destination does not contain the event payload.\n\nThis is what will be captured in the OnFailure destination.\n\n```json\n{\n  \"requestContext\": {\n    \"requestId\": \"cf6fa2a6-48e2-49a6-bc23-7ed2b154b176\",\n    \"functionArn\": \"arn:aws:lambda:us-east-1:xxx:function:kinesis-processor\",\n    \"condition\": \"RetryAttemptsExhausted\",\n    \"approximateInvokeCount\": 4\n  },\n  \"responseContext\": {\n    \"statusCode\": 200,\n    \"executedVersion\": \"$LATEST\",\n    \"functionError\": \"Unhandled\"\n  },\n  \"version\": \"1.0\",\n  \"timestamp\": \"2023-12-10T22:06:23.446Z\",\n  \"KinesisBatchInfo\": {\n    \"shardId\": \"shardId-000000000000\",\n    \"startSequenceNumber\": \"4964723877401875844914379084747495239034204468389137\",\n    \"endSequenceNumber\": \"496472387740187584491437909411123017485931487851477729\",\n    \"approximateArrivalOfFirstRecord\": \"2023-12-10T22:05:59.280Z\",\n    \"approximateArrivalOfLastRecord\": \"2023-12-10T22:06:19.196Z\",\n    \"batchSize\": 3,\n    \"streamArn\": \"arn:aws:kinesis:us-east-1:xxx:stream/MyKinesisStream\"\n  }\n}\n```\n\nThe KinesisBatchInfo contains the shard ID, sequence numbers and batch size. The actual data payload \u2014 the heart of the event that you need for processing and analysis \u2014 is not included.\n\nThis omission can lead to significant issues:\n1. Difficulty in debugging: Without the actual event data, pinpointing the cause of the failure becomes a complex task.\n2. Data loss: If you are not able to fetch the record before it expires from the stream, then the data is lost forever. Default retention for a Kinesis stream is 24 hours. If the error occurs on a Saturday, likely, your developers won\u2019t see it until they are back in the office on Monday. By then, the record is no longer available in the stream.\n3. Increased complexity: Developers need to implement additional mechanisms to retrieve and store the original event data.\n\nMost people don\u2019t realise this and many have been caught off-guard when they encounter a failed message for the first time!"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```In the year 2024, if you intend to develop a REST API employing serverless technologies, you need to decide between using Lambda Function URLs or API Gateway. Understanding the following benefits and drawbacks can assist you in your decision.\n\nFunction URL has several advantages:\n1. It is naturally compatible with Lambdaliths.\n2. It doesn't have the added latency of API Gateway.\n3. Costs related to API Gateway aren't an issue.\n4. It supports response streaming, which is beneficial when sending large HTTP response data packets.\n5. It has less configuration to consider.\n6. The API can operate for a maximum duration of 15 minutes.\n\nFunction URL also has several disadvantages: \n1. It necessitates the utilization of Lambdaliths and the navigating of associated issues.\n2. There are no per endpoint metrics, although this can be compensated for with custom-made metrics utilizing the embedded metric format (EMF).\n3. It doesn't integrate with WAF directly, though this can be achieved through CloudFront, the original function URL would still be devoid of protection.\n4. It only accommodates AWS_IAM authentication.\n5. Different endpoint authorizations can't be modified.\n\nThe use of Function URL would be a valid approach if you long to construct a Lambdalith voluntarily, and lack a need for the supplementary benefits that API Gateway provides. In this case, Function URLs are more affordable, faster, and entail fewer components.\nApart from these scenarios, if there is a need to transmit large data payloads (greater than 10MB) or operate for durations that exceed 29 seconds, Function URL is also a fitting choice. However, this hinges on the inability to modify the interactions between the client and the server.\nConsidering the limited functionality for authentication and authorization, Function URL wouldn't provide optimal usability for user-end APIs, which often necessitate the use of a Cognito authorizer or a customized Lambda authorizer. Therefore, Function URLs are best suited for public APIs or in-house APIs focusing on a microservices architecture. An \"in-house API\" refers to an API that is employed by other services, but is not used directly by the frontend application. These types of APIs usually require AWS_IAM authentication, as they are frequently sourced by another AWS resource - for example, a Lambda function, an EC2 instance, or an ECS task.\n```"}, {"role": "assistant", "content": "# When to use API Gateway vs. Lambda Function URLs\n\n\"In 2024, if you want to build a REST API using serverless technologies. Should you use Lambda Function URLs or API Gateway?\"\nHere are some trade-offs you should consider when making this decision.\n## Function URL pros\n1. It works naturally with Lambdaliths.\n2. No latency overhead from API Gateway.\n3. No API Gateway-related costs.\n4. It supports response streaming. This is useful for returning large payloads in the HTTP response.\n5. There are fewer things to configure.\n6. Your API can run for up to 15 mins.\n## Function URL cons\n1. You have to use Lambdaliths and deal with all the shortcomings that come with that.\n2. No per-endpoint metrics. It\u2019s possible to make up for this by implementing custom metrics with embedded metric format (EMF).\n3. No direct integration with WAF. Although this is possible through CloudFront, the original function URL would still be unprotected.\n4. It only supports AWS_IAM auth.\n5. You cannot configure different auth methods per endpoint.\n### Where Function URL makes sense\nIf you want to (i.e. not forced into it by the choice to use Function URL) build a Lambdalith and you don\u2019t need any of the additional features that API Gateway offers. Then Function URLs make sense - it\u2019s cheaper, faster and has fewer moving parts.\nSimilarly, if you need to return a large payload (> 10MB) or to run for more than 29s, then Function URL can also make sense. That is if you can\u2019t refactor the client-server interaction.\nGiven the limited support for authentication & authorization, it\u2019s not suitable for user-facing APIs. These APIs often require a Cognito authorizer or a custom Lambda authorizer.\nThis leaves function URLs as best suited for public APIs or internal APIs inside a microservices architecture.\nBy \u201cinternal API\u201d, I refer to APIs that are used by other services but not by the frontend application directly. These APIs usually require AWS_IAM auth because the caller is another AWS resource \u2014 a Lambda function, an EC2 instance, an ECS task, etc."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```If you are unable to refactor the interaction between client and server, this might pose a problem. Because of the minimal support for authentication and authorization, this method is not ideal for APIs that are visible to users. Such APIs will need something like a Cognito authorizer or a custom authorizer made using Lambda.\n\nThe best utilization of the function URLs is for public APIs or intra-microservices architecture APIs. When we talk about \"internal APIs,\" it refers to those APIs that are employed by other services and not directly by the frontend application. Such APIs typically need AWS_IAM authorization as the caller is another resource from AWS such as a Lambda function, ECS task or an EC2 instance.\n\n### Advantages of API Gateway\n1. It is more adaptable and works with both a multitude of Lambda functions and a single function per endpoint approach.\n2. Direct integration with the majority of AWS services is possible. This makes a Lambda function unnecessary in many situations.\n3. Proxying to HTTP APIs is feasible. This could be beneficial when integrating with APIs from third parties.\n4. Numerous other features are also offered, like:\n   - Cognito authorizer\n   - Usage plans which suits software as a service applications that have tiered pricing.\n   - Inherent validation of requests with request models.\n   - Detailed metrics for every endpoint.\n   - Creation of mock endpoints which is convenient for endpoints that return static data.\n   - Transformation of request and response which is again beneficial for integration with third part APIs.\n   - Supports WebSockets.\n   - And many more.\n```"}, {"role": "assistant", "content": "That is if you can\u2019t refactor the client-server interaction.\nGiven the limited support for authentication & authorization, it\u2019s not suitable for user-facing APIs. These APIs often require a Cognito authorizer or a custom Lambda authorizer.\nThis leaves function URLs as best suited for public APIs or internal APIs inside a microservices architecture.\nBy \u201cinternal API\u201d, I refer to APIs that are used by other services but not by the frontend application directly. These APIs usually require AWS_IAM auth because the caller is another AWS resource \u2014 a Lambda function, an EC2 instance, an ECS task, etc.\n## API Gateway pros\n1. It\u2019s more flexible. It works with both Lambdaliths and the one function per endpoint approach.\n2. It has direct integration with most AWS services. So in many cases, you don\u2019t even need a Lambda function.\n3. It can proxy requests to any HTTP APIs. This is very useful for integrating with 3rd party APIs.\n4. It offers lots more features, including (but limited to) the following:\n   - cognito authorizer\n   - usage plans (great for SAAS applications that offer tiered pricing)\n   - built-in request validation with request models\n   - detailed per-endpoint metrics\n   - mock endpoints (useful for endpoints that return static data)\n   - request and response transformation (also useful for integrating with 3rd party APIs)\n   - WebSockets support\n   - and lots more\u2026"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The system is functional with both Lambdaliths and the singular function per endpoint methodology. This technology offers a direct connection with a significant majority of AWS services, eliminating the need for a Lambda function in many circumstances. It maintains the ability to proxy requests to diverse HTTP APIs, a feature which enhances integration with third-party APIs. The system also delivers numerous other capabilities, though the following are key highlights:\n- Cognito authorizer\n- Usage plans pertinent for Software as a Service applications featuring tiered pricing\n- In-built request assessment through request models\n- Detailed metrics for each endpoint\n- Mock endpoints beneficial for endpoints presenting static information\n- Transformations of requests and responses, additionally advantageous for integration with third-party APIs\n- Support for WebSockets\n\nDespite these benefits, some disadvantages include added latency overhead and increased costs. Other limitations are the inability to stream responses, integration restrictions to 29 seconds and constraint to a 10MB response limit.\n\nIn contexts where the plethora of features provided by the API Gateway is crucial, this system may be appropriate, assuming any additional costs are not restrictive. Its limitations, such as 29-second and 10MB response confines, can pose difficulties depending on the situation, although there are strategies available to address this like Decoupled Invocations and S3 presigned URLs. It should be noted that these solutions often necessitate a revision of client-server communication and won't always be feasible.\n```"}, {"role": "assistant", "content": "It works with both Lambdaliths and the one function per endpoint approach.\n2. It has direct integration with most AWS services. So in many cases, you don\u2019t even need a Lambda function.\n3. It can proxy requests to any HTTP APIs. This is very useful for integrating with 3rd party APIs.\n4. It offers lots more features, including (but limited to) the following:\n   - cognito authorizer\n   - usage plans (great for SAAS applications that offer tiered pricing)\n   - built-in request validation with request models\n   - detailed per-endpoint metrics\n   - mock endpoints (useful for endpoints that return static data)\n   - request and response transformation (also useful for integrating with 3rd party APIs)\n   - WebSockets support\n   - and lots more\u2026\n## API Gateway cons\n1. Additional latency overhead.\n2. Additional cost.\n3. No response streaming.\n4. 29s integration limit.\n5. 10MB response limit.\n## When API Gateway makes sense\nGiven the vast array of features that API Gateway offers, it makes sense in most cases if you\u2019re OK with the additional cost that comes with the convenience. The 29s and 10MB response limits can be problematic in some cases. But they can be mitigated with patterns such as Decoupled Invocations and S3 presigned URLs. However, these workarounds require you to refactor the client-server interaction, so they are not always viable."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Big businesses are now using more Large Language Models (LLMs) but this is creating issues. Running several LLMs at the same time can be a challenge. AWS is well-placed to make it easier for businesses to use Generative AI. They have the computer resources, strong security and the latest serverless designs. \n\nAWS has launched Bedrock to deal with this growing trend. Bedrock's design, which doesn't need servers and can work with any model, is perfect for this new wave of GenAI. Last week, Bedrock became available to all AWS customers, letting them see how it will can help with their operations.\n\nRunning LLMs on a large scale, especially if you're not an expert in ML/AI, can be hard. You need teams to manage the computers, balance the loads and run the APIs. Not many businesses want to put that much money in up front. \n\nAmazon Bedrock takes these problems away. When you use a serverless service, such as Bedrock, the business only pays for what the LLM uses and creates. For example, your customer service chatbot could have 100 times as many users on Black Friday than on a Monday in March. You don't need to organise extra servers as Bedrock can deal with this increase in demand and then reduce resources afterwards.\n```"}, {"role": "assistant", "content": "# Empowering Enterprises with Serverless Generative AI: Amazon Bedrock\nMatt Carey, aleios\n\nAs the number of Large Language Models (LLMs) continues to grow and enterprises seek to leverage their advantages, the practical difficulties of running multiple LLMs in production is becoming evident. Established hyper-scale cloud providers, such as AWS, are in a favourable position to facilitate the adoption of Generative AI, due to their existing computing infrastructure, established security measures, and modern cloud-native patterns like Serverless.\n\nAWS\u2019s introduction of Bedrock stands out as a poignant reaction to these trends and is well positioned through its platform centric, model agnostic and serverless operating model to be the facilitator for this next era of GenAI. Last week, Bedrock became Generally Available (GA), giving AWS customers their first look at the tools which allow them to integrate the GenAI into all aspects of their operations.\n## Serverless Advantage\nInfrastructure management of LLMs at high scale, especially for those not entrenched in the ML/AI domain, can be daunting. Managing compute, load balancers and exposed APIs requires platform teams and few businesses are willing to make that up front investment. Amazon Bedrock alleviates these concerns. As a serverless service, businesses only pay for the tokens consumed and generated by the LLM. These scaling challenges become things of the past. For instance, you are building a customer support chatbot: It has 100x the users on Black Friday compared to a Monday in March. You do not have to provision extra servers. Bedrock handles the scale out and back in to meet demand."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```- LLM infrastructure management at large scale is a challenge especially for businesses without machine learning or artificial intelligence expertise. This includes managing compute, load balancers and APIs. \n- Amazon Bedrock provides a solution to these concerns as a serverless service that charges based on the tokens consumed and generated by the LLM.\n- Scaling problems are eliminated with Amazon Bedrock. An example situation would be a customer support bot that serves significantly more users on Black Friday compared to a regular weekday; in such a case, there would be no need to prepare extra servers.\n- Data governance and audit trails are crucial in present times and Amazon Bedrock ensures this with top-notch safety measures.\n- Data is encrypted both during stationary periods and while being moved. Moreover, customers can opt to use their own keys.\n- Amazon Bedrock respects privacy legislations such as HIPAA and GDPR. Importantly, provided data does not get employed for base model improvements nor does it get shared with third-party model providers.\n- AWS PrivateLink is compatible with Amazon Bedrock, allowing users to make private connections between LLMs and their Virtual Private Clouds (VPCs), protecting sensitive traffic from public internet exposure.\n- Companies achieve the security they require to create tools using LLMs, which allows them to use their own classified data archives as context.\n```"}, {"role": "assistant", "content": "## Serverless Advantage\nInfrastructure management of LLMs at high scale, especially for those not entrenched in the ML/AI domain, can be daunting. Managing compute, load balancers and exposed APIs requires platform teams and few businesses are willing to make that up front investment. Amazon Bedrock alleviates these concerns. As a serverless service, businesses only pay for the tokens consumed and generated by the LLM. These scaling challenges become things of the past. For instance, you are building a customer support chatbot: It has 100x the users on Black Friday compared to a Monday in March. You do not have to provision extra servers. Bedrock handles the scale out and back in to meet demand.\n## Data Security\nWith an increasing emphasis on ensuring good data governance and audit trails, Bedrock provides peace of mind for enterprises seeking to adopt GenAI. All data provided to the Bedrock LLMs is encrypted at both rest and in transit and customers are free to use their own keys. Amazon Bedrock has achieved HIPAA eligibility and GDPR compliance and provided data is never used to improve the base models or shared with third-party model providers. Enterprises can even use AWS PrivateLink with Amazon Bedrock to establish private connectivity between LLMs and their VPC\u2019s to avoid exposing traffic to the public internet. This gives businesses the security to create tools using LLMs that can use their own sensitive data archives as context."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Bedrock LLMs ensure encryption of all provided information during rest as well as transit, giving customers the option to utilize their own keys. It is also compliant with HIPAA eligibility and GDPR, and it never uses provided information for improving base models or sharing with third-party model providers. Amazon Bedrock also allows businesses to maintain privacy via AWS PrivateLink, enabling them to connect their LLMs and VPC\u2019s privately without exposing traffic to the public internet. This allows entities to set up tools employing their own confidential data archives for context with LLMs.\n\nOne primary aspect of Amazon Bedrock is it's developed as a comprehensive suite of tools for builders. Ease in model selection is one strong point in this offering. It supports a wide array of proprietary as well as open-source models, including Amazon\u2019s Titan models. It can meet various use-cases, such as long form text generation, quick summarisation, or conversational discourse. Bedrock further more has in place a playground to test and fine-tune different models. It will also introduce Meta\u2019s Llama 2 model through a Serverless API, which is an attractive feature of Bedrock. In line with AWS's recent partnership with Hugging Face, and its $235 million Series D funding round contribution, it looks promising for Bedrock to include more open-source models in the foreseeable future.\n```"}, {"role": "assistant", "content": "All data provided to the Bedrock LLMs is encrypted at both rest and in transit and customers are free to use their own keys. Amazon Bedrock has achieved HIPAA eligibility and GDPR compliance and provided data is never used to improve the base models or shared with third-party model providers. Enterprises can even use AWS PrivateLink with Amazon Bedrock to establish private connectivity between LLMs and their VPC\u2019s to avoid exposing traffic to the public internet. This gives businesses the security to create tools using LLMs that can use their own sensitive data archives as context.\n## Features\nAmazon Bedrock\u2019s offering designed to be a full suite of tools to empower builders:\n### Easy Model Selection\nBedrock supports a selection of both proprietary and open-source models including Amazon\u2019s new Titan models. Depending on the task at hand, whether it's long form text generation, quick summarisation, or back and forth conversation, you will be able to find a model which meets your use-case. Bedrock also offers a playground, allowing teams to try out various models and work out the best prompts for their chosen model. The confirmed addition of Meta\u2019s Llama 2 model through a Serverless API is definitely a unique selling point of Bedrock. AWS recently partnered with Hugging Face and made a significant investment into their $235 million Series D funding round so it\u2019s a safe bet to expect more open-source models to be included with Bedrock in the coming months."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```You can choose a model based on the task you need to accomplish, such as generating long articles, summarising texts, or engaging in conversation. Bedrock provides a platform that lets teams experiment with different models and find the best prompts for the model they choose. The fact that it integrates Meta's Llama 2 model with a Serverless API makes Bedrock distinct. After AWS invested a lot of money in Hugging Face and their recent funding round, we anticipate that more open-source models will be integrated into Bedrock in the near future. \n\nSelf-sufficient agents for Bedrock are being demoed. Agents are skilled in automating tasks typically performed by humans, like making pitch decks, responding to emails, and coding. Companies such as Canva and Adobe have already incorporated GenAI to modify image sizes, eliminate backgrounds and items. It\u2019s predicted that external style guides will eventually become parameters for these creations. With a handful of notes, these tools can generate slides, flyers, and other documents. Generating codes is also getting simpler with increasingly accurate outcomes for more complicated use-cases. In recent times, there has been remarkable progress in areas including AI code assistants, code generation via test-driven development, and autonomous code reviews. \n\nWhile the results produced by the independent agents might not be flawless, a 70% accuracy rate is still considerably helpful. The custom of paying hefty amounts for analysts to do simple tasks like adjusting the colours on slide decks may soon be a thing of the past.\n```"}, {"role": "assistant", "content": "Depending on the task at hand, whether it's long form text generation, quick summarisation, or back and forth conversation, you will be able to find a model which meets your use-case. Bedrock also offers a playground, allowing teams to try out various models and work out the best prompts for their chosen model. The confirmed addition of Meta\u2019s Llama 2 model through a Serverless API is definitely a unique selling point of Bedrock. AWS recently partnered with Hugging Face and made a significant investment into their $235 million Series D funding round so it\u2019s a safe bet to expect more open-source models to be included with Bedrock in the coming months.\n### Agents\nAutonomous agents for Bedrock are now in preview. Agents are capable of automating tasks normally done by humans; such as pitch deck creation, replying to emails or coding tasks. Companies like Canva and Adobe have already integrated GenAI to resize images, remove backgrounds and objects, and it won't be long before we can also incorporate external style guides as context for these creations. With just a selection of notes, these tools will be able to create slides, flyers and other materials. Code generation is also becoming easier with single shot accuracy possible for increasingly more complex use-cases. Progress in this area has been rocketing recently with AI code assistants, code generation through test driven development and autonomous code reviews becoming more common.\n\n\nAlthough the output of agents may not be perfect, even at around 70% accuracy, it is a significant time saver for the human operator. The days of paying analysts substantial sums for mundane tasks like colour adjustments on slide decks may soon be seen as nostalgic."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Utilizing a specific set of instructions, these instruments are capable of producing items like presentation slides, flyers and diverse types of materials. The procedure of generating code is progressively becoming more straightforward as there is an increasing emergence of competent single shot accuracy scenarios for more intricate applications. Advances in this particular field have accelerated rapidly because of the prevalence of tools such as AI code facilitators, code generation impelled by test-driven development, and autonomous code assessments that are getting much recognition.\n\nDespite the conceivable imperfections in the results delivered by these systems, an approximate accuracy rate of 70% still effectively reduces the time consumed by the human workforce. With these developing technologies, delegating repetitive tasks such as color adjustments on presentation slides to analysts for large compensations may soon be viewed as unnecessary.\n\nLanguage Lab Model (LLMs) operates at their optimal basics when they receive training based on the generic context of the defined task and have the ability to rephrase accordingly. If they lack the in-depth comprehension of the specific idea\u2014for example, certain exclusive practices peculiar to your establishment\u2014in the ideal scenario, the model would need systematic fine-tuning to cater to your unique needs. Users can conveniently make any necessary modifications to any of the Bedrock LLMs directly in the console using data acquired from S3.\n\nCertain justifiably rational arguments advocate the utilization of Bedrock over OpenAI. Both platforms exhibit a myriad of models and showcase a serverless cost per token pricing model. They equally support the feature of fine-tuning directly from the console. Nevertheless, Bedrock has an added advantage of supporting models from an extensive array of providers coupled with a much more comprehensible data governance policy. Drawing benefits from the massive AWS ecosystem, Bedrock allows seamless integration with ancillary services like Lambda for computation, OpenSearch for vectors, and S3 for object storage. On the aspect of pricing, Bedrock evidently wins over OpenAI. For instance, 1k tokens utilizing Claude 2 through Bedrock would approximately cost $0.01102 for inputs and $0.03268 for outputs, in contrast with OpenAI (GPT4 32k context) that charges $0.06 for inputs and $0.12 for outputs. \n\nIn some rigid situations where a user has a complex requirement, they might opt for OpenAI\u2019s GPT models, particularly if they have significant value for their prompts or use function calling where the LLM explicitly returns JSON. In such special circumstances, considering OpenAI could be a suitable option. Alternately, the transition to Bedrock is seemingly effortless, especially if your application relies on a library function such as LangChain, which inherently possesses drop-in replacement compiler for Bedrock.\n```"}, {"role": "assistant", "content": "With just a selection of notes, these tools will be able to create slides, flyers and other materials. Code generation is also becoming easier with single shot accuracy possible for increasingly more complex use-cases. Progress in this area has been rocketing recently with AI code assistants, code generation through test driven development and autonomous code reviews becoming more common.\n\n\nAlthough the output of agents may not be perfect, even at around 70% accuracy, it is a significant time saver for the human operator. The days of paying analysts substantial sums for mundane tasks like colour adjustments on slide decks may soon be seen as nostalgic.\n## Fine-Tuning\nLLMs work well when they have been trained on the general context of the task and are able to rephrase it. If it has no knowledge of the underlying concept, for instance a particular part or process which only happens in your company, you may get better results from fine-tuning your own custom model. Users can fine-tune any of the Bedrock LLMs directly in the console, using data stored in S3.\n## Bedrock over OpenAI?\nMany of the arguments for using Bedrock are also applicable to OpenAI\u2019s platform. Both have a choice of models, a Serverless cost per token pricing model and support fine-tuning in the console. However Bedrock supports models from a large variety of providers and has much clearer data governance policies. Bedrock also benefits from the huge AWS ecosystem, allowing for closer integrations with other services such as Lambda for compute, OpenSearch for vectors and S3 for object storage. Pricing is also in favour of Bedrock: 1k tokens using Claude 2 through Bedrock will cost $0.01102 for input and $0.03268 for output whereas the closest offering from OpenAI (GPT4 32k context) will cost $0.06 for input and $0.12 for output. There is a situation where an individual may opt for using OpenAI\u2019s GPT models. If they are particularly invested in their prompts or are making use of function calling where the LLM returns strictly JSON then sticking with OpenAI could be a good option. Otherwise switching to Bedrock is straightforward, especially if your application uses a library like LangChain which has a drop-in replacement for Bedrock."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```This post is about Lift, an open-source tool for deploying serverless applications more easily. It's a Serverless Framework plugin that aids this deployment process. During operation, an application may need to interact with third party SaaS providers. To facilitate this interaction, a webhook HTTP endpoint can be included in the application. This allows for notification from the network of external applications which the application can take actions upon. In the coming weeks, we plan to release the first beta of Lift, which will include a deep dive on Lift's 4th component, webhooks. This will include details on deploying production-grade HTTP webhook endpoints and how Lift is going to aid in accomplishing this complicated process.\n## The early concepts\n### Elementary webhook endpoint structure\n\nAPI Gateway facilitates serving of HTTP endpoints in the serverless application. It is an Amazon-managed service which can handle tasks such as authorization and input validation, with native support for Lambda integration. Consequently, distinctive HTTP requests can trigger your code via a specific application path. The use of the Serverless Framework to announce a webhook endpoint is straightforward, involving just a minimum lines of code configuration requirement:\n\n```yaml\n```"}, {"role": "assistant", "content": "# Serverless webhooks \u2014 Designing Lift\n\nThis article is part of a series on Lift, an open-source project that simplifies deploying serverless applications. As introduced in previous articles, Lift is a Serverless Framework plugin that simplifies deploying serverless applications. At some point, every application needs to interact with an ecosystem of 3rd party SaaS providers. Implementing a webhook HTTP endpoint in your application allows this ecosystem of external applications to notify you. Your application can then react to those notifications and perform tasks accordingly. As we approach the first beta planned in the next weeks, let\u2019s deep dive about Lift\u2019s 4th component, webhooks: How to deploy production-ready HTTP webhook endpoints. How we plan to simplify that with Lift.\n## The naive approach\n### Simple webhook endpoint architecture\n\nAPI Gateway is the go-to solution to easily expose HTTP endpoints in your Serverless application. API Gateway is an AWS managed service handling authorization, routing and input validation. You can natively integrate Lambda with API Gateway. Doing so, your code will be triggered every time an HTTP request hits a specific path of your application. To deploy a webhook endpoint with the Serverless Framework, it\u2019s as easy as a few lines of configuration:\n\n```yaml"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```API Gateway is a managed service from AWS that provides functionality for exposing HTTP endpoints in a serverless application. It handles tasks such as authorization, routing, and input validation. API Gateway can be natively integrated with AWS Lambda, allowing your code to be triggered whenever an HTTP request is received at a specific path of your application. To deploy a webhook endpoint using the Serverless Framework, configuration is required in the serverless.yml file, where the function is defined along with the HTTP method and path for the event trigger.\n```"}, {"role": "assistant", "content": "### Simple webhook endpoint architecture\n\nAPI Gateway is the go-to solution to easily expose HTTP endpoints in your Serverless application. API Gateway is an AWS managed service handling authorization, routing and input validation. You can natively integrate Lambda with API Gateway. Doing so, your code will be triggered every time an HTTP request hits a specific path of your application. To deploy a webhook endpoint with the Serverless Framework, it\u2019s as easy as a few lines of configuration:\n\n```yaml# serverless.yml# ...\nfunctions:\n  webhook:\n    handler: webhook.handler\n    events:\n      # Bind our function to /webhook path for POST requests\n      - httpApi: 'POST /webhook'\n```"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The architecture of a simple webhook endpoint is integrative with API Gateway serving numerous purposes such as facilitating HTTP endpoints in a Serverless application. It is an AWS-managed system adept at managing authorization, routing, and input validation aspects. API Gateway can be integrated with Lambda that causes the code to be prompted whenever an HTTP request approaches a particular path of the application. Deploying a webhook endpoint with the Framework of Serverless simply requires a few configurations.\n\nDespite the above approach working sufficiently, several characteristics pose to be missing. These criteria are imperative for maximising the efficiency of third-party apps notifiers that expect the webhook endpoints to be available, swiftly responsive, and acknowledge all notifications. Points to necessarily consider include the separation of Transport/Processing tasks which means authenticating, validating, parsing acknowledgments, and processing notifications should be assessed as distinct responsibilities. As long as the message is recognized, a 200 response should inevitably return from the API. In addition, an asynchronous decoupling needs to be ensured such that the application is not overly burdened with the influx of notifications. Implementations to optimise costs service to hold restraint on infrastructure invocation for all arriving webhooks. Thus, reducing the risk of Denials of Wallet, efficiently encouraging the application of filtering to process desired notification types. The management of errors ensures that processing errors aren't reached by the notifier on attribution to Transport/Processing separation. Still, mechanisms must be devised for keeping unsolved message from getting forever lost. Lastly, prompt notice of an unable-to-authorize notification needs to be assured. It indeed could be arising from a malicious endpoint attack or from a wrongful secret configuration causing check notification signature discrepancies.\n```"}, {"role": "assistant", "content": "# ...\nfunctions:\n  webhook:\n    handler: webhook.handler\n    events:\n      # Bind our function to /webhook path for POST requests\n      - httpApi: 'POST /webhook'\n```\n# serverless.yml### Simple webhook endpoint architecture\n\nAPI Gateway is the go-to solution to easily expose HTTP endpoints in your Serverless application. API Gateway is an AWS managed service handling authorization, routing and input validation. You can natively integrate Lambda with API Gateway. Doing so, your code will be triggered every time an HTTP request hits a specific path of your application. To deploy a webhook endpoint with the Serverless Framework, it\u2019s as easy as a few lines of configuration:\n\n```yaml## The missing characteristics of a good webhook design\n\nThe naive approach, while working, lacks some important criterion to maximize its efficiency. 3rd party apps notifiers expect your webhook endpoints to be available, respond fast and acknowledge all notifications. In order to do so, you should keep in mind: Transport / Processing separation of concerns: Webhooks are an inter-system notification medium based on HTTP transport. Acknowledging messages (authenticating, validating and parsing) and processing them are two different tasks: they should be done separately. As long as the message is acknowledged, our API should always return a 200 response. The response code should not depend on the ability of the application to use the notification content to perform some actions. Asynchronous decoupling: Webhooks are by nature an asynchronous notification protocol. A good webhook design should not put excessive stress on your application when the quantity of notification increases. A decoupling service (like a queue or a bus) should be implemented between the acknowledging and the processing workloads. Cost optimized: Webhook endpoints absorbs a various range of notification types. Your application might not want to actually do something for all of those types. It is therefore a good practice to ensure all of your infrastructure is not invoked for all incoming webhooks. Doing so, you\u2019ll reduce risk of Denial of Wallet and be able to implement easily application filtering to process only the notification types you want. Error management: Processing error should not bubble up back to the notifier thanks to Transport/Processing separation of concern. However, some mechanism should be in place to ensure unprocessable messages are not lost forever. Monitoring: You should be notified of excessive amount of notification you cannot authorize. This could either be a malicious attack on your endpoint or a wrong configuration of the secret used to check notification signature."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```It is advisable to not engage all your infrastructure for every incoming webhook. This action will decrease potential wallet denials and allow easier application filtering to process preferred notification types. Error control: Any processing errors should not reach back to the notifier due to the boundary of transportation and processing. Nevertheless, there must be a method present to ensure not all unprocessable messages are irretrievably lost. Observability: You must be alerted if there is a surplus of notifications that you cannot authorize. This could be an erroneous attack on your endpoint or incorrect secret configuration checking the notification signature.\n## An approach suitable for production\n### Serverless webhook endpoint structure ready for production\n\nHere is an intro to a stripped-down `serverless.yml` configuration incorporating these good practices:\n\n```yaml# serverless.yml# ...\nprovider:\n  eventBridge:\n    useCloudFormation: true# You can use native Serverless framework functionalities to define your processors\n```"}, {"role": "assistant", "content": "It is therefore a good practice to ensure all of your infrastructure is not invoked for all incoming webhooks. Doing so, you\u2019ll reduce risk of Denial of Wallet and be able to implement easily application filtering to process only the notification types you want. Error management: Processing error should not bubble up back to the notifier thanks to Transport/Processing separation of concern. However, some mechanism should be in place to ensure unprocessable messages are not lost forever. Monitoring: You should be notified of excessive amount of notification you cannot authorize. This could either be a malicious attack on your endpoint or a wrong configuration of the secret used to check notification signature.\n## A production-ready approach\n### Production-ready serverless webhook endpoint architecture\n\nHere is a preview of a minimal `serverless.yml` configuration that includes those best practices:\n\n```yaml# serverless.yml# ...\nprovider:\n  eventBridge:\n    useCloudFormation: true# You can use native Serverless framework functionalities to define your processors"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```- The team had to build a highly scalable chat app in just 4 weeks due to a surge in demand for their client's video conferencing service during the COVID-19 lockdown.\n- The chat app needed to handle 250,000 participants per video event.\n- It had to be released quickly to handle the increased traffic.\n- The existing development team needed to maintain and iterate the chat system.\n- Real-time communication was required, so websockets were used instead of long-polling for better performance and cost-efficiency at scale.\n- AWS AppSync was chosen as it's a serverless GraphQL interface that can connect to different data sources.\n- It offered speed of delivery, scalability, and ease of use.\n- API Gateway websockets weren't viable due to the need for mass message broadcasting.\n- AppSync's GraphQL interface enabled rapid frontend development.\n```"}, {"role": "assistant", "content": "# Building a Massively Scalable Serverless Chat Application with AWS AppSync\n## Introduction\nWhen demand strikes, developers are tasked with building features quickly. My team knew this all too well when we were set to build a highly scalable chat app in just 4 weeks, when demand for our client\u2019s video conferencing service went through the roof at the beginning of the first Covid-19 lockdown. Google Trends: Searches for \u2018Video Conferencing\u2019 in the UK. There was a significant increase in demand in March 2020.\n\nWhen building the chat application discussed above we had to take into account the following requirements:\n- Release to production quickly \u2014 their systems were failing to cope with the increased traffic levels\n- Scale to 250,000 chat participants per video event\n- Build the capability in their existing development team to maintain and further iterate the chat system\n\nThe chat application needed to have functionality where users could talk to each other in real-time. We used websockets rather than long-polling due to their cost efficiency and performance at scale (read more about the pros and cons of long-polling vs websockets).\n\nThe answer to all of the above was to use AWS AppSync, a fully-managed Serverless GraphQL interface to a number of different data sources \u2014 it offers speed of delivery, scale and ease of use. API Gateway websockets were also considered, but due to our requirements for mass broadcast of messages, it was not a viable option (there is no way to broadcast messages to all connected clients with one API call \u2014 you need an API call per connection). In addition, the GraphQL interface of AppSync allows for rapid frontend development."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Websockets were chosen over long-polling due to their advantages in cost efficiency and performance when scaled up. AWS AppSync, a fully-managed Serverless GraphQL interface to a range of different data sources, was used for its speed of delivery, scalability and user-friendliness. API Gateway websockets were also assessed but were not practical due our mass message broadcasting needs. The drawback is that there is no capability to broadcast messages to all connected clients with a single API call- each connection requires an individual API call. Beside this, the GraphQL interface of AppSync facilitates speedy frontend development.\n\nIn relation to the architecture, its simplicity strengthened the delivery speed of our chat application and provided a definitive set of technologies for the existing team to get acquainted with. AppSync is central to the architecture and it integrates with Cognito user pools for authentication purposes. DynamoDB is also utilized as our data source. \n\nThis all connects to our React frontend that employs the Apollo Client to dispatch our requests to the GraphQL server from the frontend. The Serverless Framework was utilized for managing the Infrastructure as Code (IaC), whereas amplify provided the SDK for frontend interactions with backend deployed services.\n```"}, {"role": "assistant", "content": "We used websockets rather than long-polling due to their cost efficiency and performance at scale (read more about the pros and cons of long-polling vs websockets).\n\nThe answer to all of the above was to use AWS AppSync, a fully-managed Serverless GraphQL interface to a number of different data sources \u2014 it offers speed of delivery, scale and ease of use. API Gateway websockets were also considered, but due to our requirements for mass broadcast of messages, it was not a viable option (there is no way to broadcast messages to all connected clients with one API call \u2014 you need an API call per connection). In addition, the GraphQL interface of AppSync allows for rapid frontend development.\n## Architecture\nThis simple architecture enabled us to have a high speed of delivery for our chat application while also have a clear set of technologies that would need to be taught to the existing team.\n\nThe architecture is based around AppSync, which integrates with Cognito user pools for our authentication. DynamoDB is also leveraged as our data source (read about DynamoDB in the next section).\n\nAll of this hooks up to our React frontend which uses the Apollo Client to fire our requests to the GraphQL server from the frontend. Note, the Serverless Framework was used to manage the IaC (infrastructure as code), while amplify simply provided the SDK for frontend interaction with backend deployed services."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```## Architecture\nWe chose a simple architecture for our chat application, which allowed for a fast delivery rate, given the technology requirements that needed to be handled by our existing team. \n\nOur architecture uses AppSync, integrated with Cognito user pools for authentication needs. DynamoDB serves as our data source.\n\nThe architecture connects to the React frontend via Apollo Client which sends requests to the GraphQL server. Serverless Framework was utilized to manage the infrastructure as code (IaC), and amplify provided the SDK needed for frontend-backend communication.\n## DynamoDB\nSince DynamoDB is a NoSQL Serverless Database, it satisfied our needs for scalability without server management. It's used by large corporations like Airbnb and Samsung due to its capability to handle high velocity demands, which lends credibility. DynamoDB streams were also useful for analytics integration. \n\nAppSync's integration with DynamoDB is an advantage, as it reduces set-up time and thus speeds up the delivery process. \n\n```\nDynamoDB Schema for basic Serverless chat application\n```\n```"}, {"role": "assistant", "content": "## Architecture\nThis simple architecture enabled us to have a high speed of delivery for our chat application while also have a clear set of technologies that would need to be taught to the existing team.\n\nThe architecture is based around AppSync, which integrates with Cognito user pools for our authentication. DynamoDB is also leveraged as our data source (read about DynamoDB in the next section).\n\nAll of this hooks up to our React frontend which uses the Apollo Client to fire our requests to the GraphQL server from the frontend. Note, the Serverless Framework was used to manage the IaC (infrastructure as code), while amplify simply provided the SDK for frontend interaction with backend deployed services.\n## DynamoDB\nBeing a NoSQL Serverless Database, DynamoDB provides a fully scalable solution to our needs without the need to manage servers. It was built for enormous, high velocity use cases and big companies such as AirBnb and Samsung use the service, so we\u2019re in good company! In addition DynamoDB streams came in use later on for further analytics integrations.\n\nThe fact that AppSync integrates with DynamoDB is ideal as it means we need to do minimal set-up and therefore increase our speed of delivery. \n\n```\nDynamoDB Schema for basic Serverless chat application\n```"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```## DynamoDB\nDynamoDB, a NoSQL Serverless Database, offers a scalable solution without the requirement of managing servers. Big corporations such as AirBnb and Samsung utilize this service, showcasing its potential for large-scale operations. Furthermore, DynamoDB streams have been used for additional analytics integrations.\n\nThe integration of AppSync and DynamoDB is advantageous as it minimizes set-up time, boosting our rate of project completion. \n\n```\nDynamoDB Schema for basic Serverless chat application\n```\n## Understanding GraphQL\nThe usage of REST requires calls to different endpoints to receive the needed information. However, with GraphQL, one simply calls the same GraphQL endpoint.\n\nIn simpler terms, GraphQL presents a different potential way of connecting your client applications to the backend. It originated from Facebook, aiming to improve bandwidth efficiency due to the inconsistent internet connections of mobile devices. Let's compare GraphQL to REST, which most of us are familiar with, for better comprehension.\n\nWith GraphQL, we avoid calling multiple endpoints as we do with REST.\n## Queries\nQueries within GraphQL provide similar function to REST's GET requests. Queries offer a method of data retrieval (in our context, we refer to collecting data from DynamoDB, our database).\n\n```\nThe above demonstration shows a query to receive messages. A roomId is involved as an argument that we implement in our query. The purpose is to attain messages specific to that certain chat room. We then declare the return type to be an array of messages, where 'Message' is a characterizing type.\n```\n```"}, {"role": "assistant", "content": "## DynamoDB\nBeing a NoSQL Serverless Database, DynamoDB provides a fully scalable solution to our needs without the need to manage servers. It was built for enormous, high velocity use cases and big companies such as AirBnb and Samsung use the service, so we\u2019re in good company! In addition DynamoDB streams came in use later on for further analytics integrations.\n\nThe fact that AppSync integrates with DynamoDB is ideal as it means we need to do minimal set-up and therefore increase our speed of delivery. \n\n```\nDynamoDB Schema for basic Serverless chat application\n```\n## GraphQL Explained\nWith REST we need to call separate endpoints to get the information you require. With GraphQL we need only call the same GraphQL endpoint.\n\nIn a nutshell, GraphQL is an alternative way to connect your client applications to your backend. It was developed at Facebook, to improve bandwidth efficiency since mobile devices don\u2019t always have a good internet connection. Most of us are more familiar with REST, so let\u2019s compare it to GraphQL to get a better understanding.\n\nWhen using GraphQL we simply call one endpoint rather than having separate defined endpoints as in REST.\n## Queries\nQueries in GraphQL are analogous to GET requests in REST. Queries are a way of fetching data (in our case we are fetching from DynamoDB as our database so we\u2019ll use this as our example going forward).\n\n```\nSee how we define a query above to get messages. We pass in a roomId as an argument which we can use in our query to only get messages for that specific chat room. We then specify that the return type is an array of messages, where \u2018Message\u2019 is a type.\n```"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The following text defines that the return type is an array comprised of 'Messages', a specific type.\n```\n## Explanation of GraphQL\nUsing REST requires to contact multiple endpoints in order to receive the necessary information. However, with the use of GraphQL, there is only need to call one consistent GraphQL endpoint. \n\nTo summarize, GraphQL is simply another method of connecting client applications to the backend system. It was formulated at Facebook to enhance bandwidth efficiency, especially due to the unstable internet on mobile devices. Many people are better acquainted with REST, hence a comparison with GraphQL provides a more comprehensible understanding of the concept.\n\nIn the practice of GraphQL, rather than contacting distinct endpoints as it occurs with REST, there is only need to call upon a single endpoint.\n## About Mutations\nThe concept of mutations within GraphQL has considerable similarity to the REST POST requests. They essentially mutate data as their name suggests.\n\n```\nIn the above example, there is a demonstration of executing a sendMessage mutation, where the roomId and message constituents are fed in as arguments, so they can be added to a DynamoDB table with those specified details. The return then is a 'Message'.\n```\n## Concept of Subscriptions\nAppSync subscriptions are intriguing as they facilitate real-time updates. As for the chat application in question, the need is to \u2018subscribe\u2019 to the \u2018sendMessage\u2019 mutation event so as to ensure that the frontend application updates every time a message is sent, transferring the new message across all users. \n\n```\nExamining here, it can be interpreted that the roomId gets passed into it. This signifies that only users belonging to that specific chat room will monitor messages that are sent.\n```\n```"}, {"role": "assistant", "content": "We then specify that the return type is an array of messages, where \u2018Message\u2019 is a type.\n```\n## GraphQL Explained\nWith REST we need to call separate endpoints to get the information you require. With GraphQL we need only call the same GraphQL endpoint.\n\nIn a nutshell, GraphQL is an alternative way to connect your client applications to your backend. It was developed at Facebook, to improve bandwidth efficiency since mobile devices don\u2019t always have a good internet connection. Most of us are more familiar with REST, so let\u2019s compare it to GraphQL to get a better understanding.\n\nWhen using GraphQL we simply call one endpoint rather than having separate defined endpoints as in REST.\n## Mutations\nMutations in GraphQL are similar to POST requests in REST. Mutations do what they say, they are way of mutating the data. \n\n```\nSee above how we set up a sendMessage mutation \u2014 we pass in the roomId and message as arguments so that we can add it with that information as an item to our DynamoDB table. The return type is a Message.\n```\n## Subscriptions\nAppSync subscriptions, which allow for real-time updates, are where the fun begins. For our chat application, we want to \u2018subscribe\u2019 to the mutation event \u2018sendMessage\u2019 so that the frontend application updates when a message is sent with the new message to all users. \n\n```\nYou can see here that we pass in the roomId which means that only users in that particular chat room will be listening for messages sent.\n```"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The aim is to 'subscribe' to the mutation event 'sendMessage' in our chat application. This event enables the app to refresh when a user sends a message. The new message is universally disseminated to all active users.\n```\nTo refine it further, participants of a particular chat room get the updates. If you inspect the code, you'll spot that we used roomId variable to achieve this specification.\n```\n## Mutations\nIn GraphQL, mutations mirror REST's POST requests. As their name implies, mutations alter the data.\n```\nIn the demonstration above, we crafted a 'sendMessage' mutation. We input roomId and message as arguments and then append it to our DynamoDB table. Here, our return type is a Message.\n```\n## Resolvers\nWe apply Apache VTL when dealing with our business logic. Communication with the data source comes through resolvers. They bridge between GraphQL and the data source. They operate in Apache Velocity Template Language (VTL). Here, the resolver ingests the request and spews out a JSON document.\n## Pipeline Resolvers\nIn scenarios where numerous operations need to be conducted to decipher the GraphQL field, the pipeline resolvers feature of AppSync comes handy. It comprises 'before' mapping template, continuous functions, and concludes with an 'after' mapping template.\n```\nLike when launching a thread in response to a query, for instance, we apply pipeline resolver. We have designed MESSAGE and THREAD rows in single- table set-up. When someone responds in a thread, we constitute a THREAD row and adjust the MESSAGE \nrow to incorporate a 'repliedAt' timestamp. Each operation needs a separate request.\n```\n```"}, {"role": "assistant", "content": "For our chat application, we want to \u2018subscribe\u2019 to the mutation event \u2018sendMessage\u2019 so that the frontend application updates when a message is sent with the new message to all users. \n\n```\nYou can see here that we pass in the roomId which means that only users in that particular chat room will be listening for messages sent.\n```\n## Mutations\nMutations in GraphQL are similar to POST requests in REST. Mutations do what they say, they are way of mutating the data. \n\n```\nSee above how we set up a sendMessage mutation \u2014 we pass in the roomId and message as arguments so that we can add it with that information as an item to our DynamoDB table. The return type is a Message.\n```\n## Resolvers\nWe use Apache VTL for our business logic. In order to communicate with our data source we need to use resolvers \u2014 this is the connection between GraphQL and our data source. The resolvers are written in Apache Velocity Template Language (VTL) which takes the request as an input and outputs a JSON document.\n## Pipeline Resolvers\nSometimes we may need to perform multiple operations to resolve the GraphQL field \u2014 in this case we use a great feature of AppSync called pipeline resolvers. Pipeline resolvers consist of a \u2018before\u2019 mapping template, followed by a number of functions, finishing with an \u2018after\u2019 mapping template.\n\n```\nAs an example we use a pipeline resolver for starting a thread on a question. To make use of single table design we have \u2018MESSAGE\u2019 rows and \u2018THREAD\u2019 rows. When a person replies in a thread, we need to add a THREAD row and mutate the MESSAGE row to add a \u2018repliedAt\u2019 timestamp. This requires 2 separate requests. \n```"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Pipeline Resolvers are utilized when multiple operations are required to resolve the GraphQL field. This feature of AppSync, the pipeline resolvers, encompass a \"before\" mapping template, numerous functions, and culminate with an \"after\" mapping template. \n\nA typical application of the pipeline resolver involves initiating a thread regarding a certain question. Single table design makes use of \"MESSAGE\", and \"THREAD\" rows. When a reply occurs in a thread, a THREAD row must be added and the MESSAGE row should be adjusted to incorporate a \"repliedAt\" timestamp. This entails two distinct requests.\n\nOn the user interface end, AWS Amplify SDK with Cognito is employed to enable user authentication. Pre-designed user interface elements for sign-in and sign-out are provided by Amplify. 'Auth' from aws-amplify along with our Cognito user pool data (deployed via the Serverless Framework) assist in configuring the authentication process. The Apollo Client is used for requests on the frontend, primarily due to its compatibility with TypeScript, creating a positive user experience, and its ability to manage state on the frontend using a cache mechanism.\n```"}, {"role": "assistant", "content": "## Pipeline Resolvers\nSometimes we may need to perform multiple operations to resolve the GraphQL field \u2014 in this case we use a great feature of AppSync called pipeline resolvers. Pipeline resolvers consist of a \u2018before\u2019 mapping template, followed by a number of functions, finishing with an \u2018after\u2019 mapping template.\n\n```\nAs an example we use a pipeline resolver for starting a thread on a question. To make use of single table design we have \u2018MESSAGE\u2019 rows and \u2018THREAD\u2019 rows. When a person replies in a thread, we need to add a THREAD row and mutate the MESSAGE row to add a \u2018repliedAt\u2019 timestamp. This requires 2 separate requests. \n```\n## Frontend \u2014 Amplify & Apollo Client\nTo authenticate our users we use the AWS Amplify SDK with Cognito. Amplify provides pre-built UI components to cater for sign-in and sign-out. We configure the authentication by using \u2018Auth\u2019 from aws-amplify and using our user pool information (from the Cognito user pool deployed by the Serverless Framework). To make the requests from the frontend we use the Apollo Client- it is our favourite tool for interacting with our backend as it is compatible with TypeScript, providing a great developer experience, and it manages state in the frontend using a caching mechanism."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The testing of automated software is crucial for ensuring its quality in a continuous integration setting. Other types of testing such as checking backend logic and web interfaces, although complicated and lengthy when ensuring its precision, is nonetheless thoroughly documented and there are existing guidelines to follow. Features like PDF generation, which are often used in products designed for consumers (like order confirmation, data analysis or personalized planning documents), tend to be overlooked when it comes to testing. However, these features are often of critical importance. \n\nWe tried a new approach to test PDFs by combining Puppeteer (a headless Chrome manipulator), Jest (a testing framework for JavaScript), and a visual regression Jest plugin from American Express. The main concept of visual regression testing is comparing acceptedount screen captures of an interface to those produced following alterations to the underlying code. If deviations of pixels are detected that surpass a set limit, the builds are seen as unsuccessful. \n\nIn order to do this, we require: \n\n- Something to convert a PDF page to an image that can be evaluated using an image comparison tool. The arrangement of PDFs is unsatisfactory for comparison and it does not function well with current image diff instruments. Puppeteer possesses the ability to create a PNG for each PDF page.\n- A framework to conduct testing (Jest).\n- A technic to assess the produced screen captures to the new ones (jest-image-snapshot).\n```"}, {"role": "assistant", "content": "# PDF Visual Regression Testing \u2014 'The PuppetMaster' Approach\n\nTest automation is the only way to ensure quality in a CI (continuous integration) environment. Testing backend logic and web frontends, although time consuming and difficult to ensure accuracy, is well documented and best practices exist. PDF generation a feature often involved in consumer facing products (e.g. booking confirmation, data analysis, personalised planning documents). Yet, although often a mission critical feature, testing the generated PDFs is generally overlooked. We experimented with a new approach to PDF testing by combining Puppeteer (a way to manipulate headless Chrome), the jest JavaScript testing framework and a visual regression jest extension from American Express. The basic premise of visual regression testing is comparing known good snapshots of an interface with ones generated after a change to the underlying code. With builds failing if a difference of pixels is detected above some threshold.\n\nTo do this we need:\n1. A way to convert a PDF page to an image snapshots that can be used by an image comparison tool. The PDF format is not good for comparison and does not work with existing image diff tools. Puppeteer will be able to generate a PNG of each page of the PDF.\n2. A test running framework (jest).\n3. A tool to compare the generated snapshots to new snapshots. (jest-image-snapshot)."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Visual regression testing fundamentally involves comparing old versions of a user interface with the new ones created after any changes are made to the underlying code. A build may fail if there's a noticeable pixel difference beyond a certain anticipated range.\n\nTo perform this task, some requirements are:\n\n1. A system for creating image snapshots from PDF pages because the PDF format isn't as effective for comparisons and doesn't work well with available image difference tools. Puppeteer can be used to create a PNG of each PDF page. \n2. A framework for running tests, such as Jest.\n3. A tool to correlate the created snapshots with new ones (like Jest-image-snapshot).\n\n### Creating snapshot pictures from PDF\n\nAs a headless instance of Chrome, Puppeteer can generate visual snapshots of websites for image comparisons using its `screenshot` function.\n\n```javascript\n// Sample usage of Puppeteer's screenshot function\n```\n\nPuppeteer can launch a PDF file using Chrome's built-in PDF viewer and then create screenshots for that page. From the previous example, one could change line 6 to enable the opening of a local PDF file using file protocol.\n\nThe only problem with this approach is when there's a scrollbar on the viewer's right side. It's best to exclude this from the screenshot as Chrome's scrollbars can potentially introduce unexpected issues and delays to the tests. Fortunately, it's possible to crop the screenshot using Puppeteer's `clip` function. \n\n```javascript\n// An example how to use Puppeteer's clip parameter\n```\n```"}, {"role": "assistant", "content": "The basic premise of visual regression testing is comparing known good snapshots of an interface with ones generated after a change to the underlying code. With builds failing if a difference of pixels is detected above some threshold.\n\nTo do this we need:\n1. A way to convert a PDF page to an image snapshots that can be used by an image comparison tool. The PDF format is not good for comparison and does not work with existing image diff tools. Puppeteer will be able to generate a PNG of each page of the PDF.\n2. A test running framework (jest).\n3. A tool to compare the generated snapshots to new snapshots. (jest-image-snapshot).\n## Generating snapshot images from a PDF\n\nPuppeteer is a headless instance of chrome. It can be used to generate visual snapshots of websites to use for image comparison via a simple `screenshot` function.\n\n```javascript\n// Example of using Puppeteer's screenshot function\n```\n\nChrome has a built in PDF viewer, and Puppeteer can manipulate chrome to open a url or a file. So Puppeteer can open a PDF file and then be used to generate a screenshot of the page. From the example above we can change line 6 to use the file protocol to open a local PDF file.\n\nOne issue with this approach is the scrollbar on the right hand side of the viewer. Due to the way chrome loads its scrollbar it is good to remove this from the snapshot as it can add flakiness and timing issues to the tests. Luckily Puppeteer can take a `clip` parameter to its screenshot function to crop the screenshot. For instance:\n\n```javascript\n// Example of using Puppeteer's clip parameter\n```"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Puppeteer has the ability to open a PDF file and create a screenshot of the page. It can be done by modifying line 6 to use a local PDF file with the file protocol based on the given example.\n\nOne drawback with this method is the scrollbar appearing on the viewer's right side. Removing the scrollbar from the snapshot can be fewer chances of inconsistencies and timing problems in the tests due to the way Chrome loads its scrollbar. Fortunately, Puppeteer has a `clip` parameter for its screenshot feature, thus allowing for screenshot cropping.\n\n```javascript\n// Demonstration of Puppeteer's clip parameter\n```\n## Regarding Jest\n\nJest is an efficient JavaScript testing framework characterized by its simplicity. It's relatively straightforward to add jest to an existing JS project; you just need to follow these commands:\n\n```bash\n// Instructions for integrating jest\n```\n\nA sample test template could be constructed as below:\n\n```javascript\ndescribe('pdf suite', () => {\n   it('works', async () => {\n      expect(1).toBe(1);\n   });\n});\n```\n\nFurthermore, there's a helpful extension provided by American Express for the Jest platform. This extension facilitates image comparison for visual regression tests. It can be added to your project by executing these commands:\n\n```bash\nyarn add -D jest-image-snapshot\nnpm i --save-dev jest-image-snapshot\n```\n\nOnce installed, Jest's built-in 'expect' object can be expanded thus:\n\n```javascript\nconst { toMatchImageSnapshot } = require('jest-image-snapshot');\nexpect.extend({ toMatchImageSnapshot });\n```\n```"}, {"role": "assistant", "content": "So Puppeteer can open a PDF file and then be used to generate a screenshot of the page. From the example above we can change line 6 to use the file protocol to open a local PDF file.\n\nOne issue with this approach is the scrollbar on the right hand side of the viewer. Due to the way chrome loads its scrollbar it is good to remove this from the snapshot as it can add flakiness and timing issues to the tests. Luckily Puppeteer can take a `clip` parameter to its screenshot function to crop the screenshot. For instance:\n\n```javascript\n// Example of using Puppeteer's clip parameter\n```\n## In jest\n\nJest is a delightful JavaScript Testing Framework with a focus on simplicity. Adding jest to an existing JS project is as simple as:\n\n```bash\n// Commands to add jest\n```\n\nOur test template will then be as follows:\n\n```javascript\ndescribe('pdf suite', () => {\n   it('works', async () => {\n      expect(1).toBe(1);\n   });\n});\n```\n\nAmerican express have a great extension to jest providing image comparison for visual regression testing. It can be added as follows:\n\n```bash\nyarn add -D jest-image-snapshot\nnpm i --save-dev jest-image-snapshot\n```\n\nJest\u2019s inbuilt expect object can then be expanded as follows:\n\n```javascript\nconst { toMatchImageSnapshot } = require('jest-image-snapshot');\nexpect.extend({ toMatchImageSnapshot });\n```"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Serverless technology, among others, has been monumental in accelerating project timelines, reducing the management cost, and generally enhancing the autonomy of developers in the systems they construct. Despite these advantages, its implementation is not devoid of challenges \u2014 particularly regarding corporate management essential for fully leveraging the benefits of Cloud technology. As we increasingly orient towards Cloud-Native technology, including Serverless, certain key components demand reevaluation, such as: management-driven constraints hindering developers, and cognitive load that could be overwhelming.\n\nAddressing these issues extends beyond mere technical refinements \u2014 a blend of CDK can possibly drive the necessary socio-technical transformations.\n\nSecurity Frequently Calls for Modifications\nLarge establishments venturing into Serverless frequently encounter two major obstacles: viewing security as an upper-management hurdle that impedes budding initiative and expanded awareness about steadily rising security incidents over years, requiring a shift in tandem efforts rather than tasks traditionally passed onto the Information Security team. \n\nPrimarily Due to Security Seen As Hierarchical Hinderance\nAs businesses increasingly align towards the paradigms of Cloud and DevOps, developers are mandated to have closer proximity with security measures, though collaboration sometimes encounters resistance. The great symbiosis between the application code and the cloud in Serverless signifies that developers further venture into security-related territories, often causing territorial contradictions during the early stages of Serverless adoption, yet offers a chance to rethink the age-old approach to managing security.\n\nSecurity is best approached as an active facilitator, providing tools and framework, along with multiplying secure by default standards obliging structural and technical modifications in the engineering teams. The integration of approaches like \"Team Topologies\" and CDK could hold the key to drive the requisite amendments. The aim is to shift the perception of security from a hindrance to an enabler enriching development, rather than stifling deployment and eternal inventiveness. To accomplish the same, both security and system engineering sectors require significant changes to support a bottom approach to the secure by design pattern.\n```"}, {"role": "assistant", "content": "# Enabling the Optimal Serverless Platform Team \u2014 CDK and Team Topologies\n\nServerless, and related technologies, have enabled teams to move faster, reduce total cost of ownership and overall empowered developers to have greater ownership of the systems they build. However, Serverless is not a silver bullet \u2014 there is an organisational side that\u2019s key to unlock the full benefits of Cloud.\nWhen it comes to team structure as we adopt more Cloud-Native technologies, like Serverless, there are 2 key areas that need to be rethought:\n- The top-down approach to security bottlenecking developers\n- Overwhelming cognitive load\n\nAs discussed above, the solution is not purely technical \u2014 but CDK can enable the socio-technical change needed.\n## Rethinking Security\n\nIn many large organisations that move to adopt Serverless, security is typically seen as a top-down bottleneck that slows development and innovation. At the same time security incidents have been rising for years, and a strong cyber security posture requires organisations to work together, not just throwing things over the wall to InfoSec.\n### Security as \u201cTop Down\u201d Blocker\n\nAs companies have started to adopt Cloud and DevOps developers have been brought closer to security, yet a wall still exists in this collaboration. With Serverless the application code and the cloud are inextricably linked, meaning developers have moved further into the realm of the security. This typically creates some turf wars in the initial stages of a transformation to adopt Serverless, but also provides the opportunity for us to rethink the traditional security approach.\n\nIdeally Security is seen as an enabling function, with tools, approaches and teams enabling a secure by default paradigm. This requires us to turn our view of security on its head and have development teams be enabled by security rather than held hostage from deploying or innovating.\n\nThis requires a change in engineering team structure, as well as a technical solution to enable a bottom-up secure by design paradigm. The combination of \u201cTeam Topologies\u201d and CDK can be leveraged to achieve this change."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```During the early phases of transitioning to a Serverless framework, there are often conflicts regarding territorial ownership. However, these situations offer a chance for reconsidering the conventional methods of ensuring security. \n\nSecurity, ideally, should not be an inhibiting factor but should be an enabler endowing with essential tools and strategies, and empowering teams to create a default secure environment. This demands a paradigm shift in how we perceive security - as an inclusive function facilitating security rather than a restrictive one hindering deployment or innovation.\n\nImplementing this shift necessitates a modification in the structure of engineering teams, and the introduction of technical solutions to vest a secure by design rule from the bottom up. Harnessing the synergies of \"Team Topologies\" and CDK can thrust this transformation.\n\nMoreover, Cognitive load operates as an aspect of friction on teams. The greater the cognitive load, the more ponderous the team's work-rate (often, consequently affecting their gratification negatively). A primary strategy to alleviate cognitive load would be by tailoring our architecture and systems considering the underlying domain and creating services rooted in exclusive subsections of said domain, to allow teams and services to function independently. Such an arrangement permits teams to limit their scope to a certain sector of the total domain complexity.\n```"}, {"role": "assistant", "content": "This typically creates some turf wars in the initial stages of a transformation to adopt Serverless, but also provides the opportunity for us to rethink the traditional security approach.\n\nIdeally Security is seen as an enabling function, with tools, approaches and teams enabling a secure by default paradigm. This requires us to turn our view of security on its head and have development teams be enabled by security rather than held hostage from deploying or innovating.\n\nThis requires a change in engineering team structure, as well as a technical solution to enable a bottom-up secure by design paradigm. The combination of \u201cTeam Topologies\u201d and CDK can be leveraged to achieve this change.\n## Cognitive Load\n\nCognitive load is a form of friction on teams \u2014 the greater the cognitive load the slower their velocity (and often the lower their satisfaction). One key way to reduce cognitive load is to design our architecture and systems on the underlying domain and building services based on independent subsections of the domain to enable independence of teams and services. This allows teams to restrict their scope to a subset of the overall domain complexity."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```This necessitates a restructuring of the engineering team and implementation of a technical solution to facilitate a bottom-up approach to security by design. Both \"Team Topologies\" and CDK can be utilized to achieve this structural change. \n\nThe concept of cognitive load refers to the amount of working memory being used. As the convergence of cloud and application code increases, so too does the reliance on smaller cross-functional teams. Despite the numerous advantages, such as increased autonomy and velocity, improper design of organisational structures and architectures might impose a substantial cognitive load on the teams involved. This can impede productivity and complicate the onboarding of new developers.\n\nWithin team topologies four different type of teams exist:\n1. The Stream-Aligned Team, which ideally segregates the Business Domain into sections (e.g., Bounded Contexts) and delivers a flow of work.\n2. The Enabling Team, which assists Stream-Aligned Teams, removing obstacles and detecting capability gaps.\n3. The Complicated Subsystem Team, which is primarily concerned with complex domain-specific expertise, such as algorithmic and machine learning processes.\n4. The Platform Team, which serves to facilitate the work of the Stream-Aligned Teams via a product-centric approach whilst preserving their independence.\n\nAlong with Domain-Driven Design techniques, Stream-Aligned Teams can focus on separate areas of the domain with services designed accordingly. \n\nPlatform Teams hold the potential to alleviate the cognitive burden imposed on Stream-Aligned Teams by offering foundational services that the latter are no longer tasked with creating. It is beneficial for Platform Teams to visualize themselves as service providers, viewing the Stream-Aligned Teams as their clients.\n\nKey to the success of the Platform Team is its ability to facilitate:\n- Abstraction\n- Encapsulation\n- Composition \n\nThese elements empower the Platform Team to obfuscate complexity through abstraction, assure optimal security via encapsulation, and enable Stream-Aligned Teams to individually combine components into their respective sub-domain cases.\n```"}, {"role": "assistant", "content": "This requires us to turn our view of security on its head and have development teams be enabled by security rather than held hostage from deploying or innovating.\n\nThis requires a change in engineering team structure, as well as a technical solution to enable a bottom-up secure by design paradigm. The combination of \u201cTeam Topologies\u201d and CDK can be leveraged to achieve this change.\n### Cognitive Load: The amount of working memory being used\n\nAs cloud and application code move closer together we\u2019re leveraging smaller cross-functional teams to a greater and greater degree. This has many advantages, with smaller teams building at a higher velocity and with complete autonomy. At the same time, if we don\u2019t design our architectures and organisational structures correctly it can create such a large cognitive load on the teams involved that they struggle to deliver and it becomes extremely difficult to onboard new developers.\n\nInside team topologies there are 4 types of team:\n1. Stream-Aligned Team: Ideally split into segments of the Business Domain (e.g. Bounded Contexts) and delivering a flow of work.\n2. Enabling Team: Assist Stream-Aligned teams, remove obstacles and detect lacking capabilities.\n3. Complicated Subsystem team: Domain specific complex expertise (e.g. algorithmic, machine learning)\n4. Platform Team: Accelerate the Stream-Aligned teams via a product based approach while maintaining their autonomy.\n\nIn combination with Domain-Driven Design techniques Stream-Aligned Teams can work on independent areas of the domain with services architected accordingly (see EventBridge Storming).\n\nPlatform Teams can greatly reduce the cognitive load placed on Stream-Aligned teams by providing foundational services that no longer need to be created by the Stream-Aligned teams. These Platform teams should see themselves as service providers, with the Stream-Aligned teams as their \u201ccustomers\u201d.\n## CDK \u2014 The Enabling Technology\n\nKey to enable the platform team is the need for:\n- Abstraction\n- Encapsulation\n- Composition\n\nThese components would enable the Platform team to hide complexity via abstraction, provide protected security defaults via encapsulation and allow Stream-Aligned Teams to self-compose components into their sub-domain specific use cases."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Complex Subsystem division: Distinct intricate skills for specific areas (like algorithmic, machine learning)\n4. Platform Group: Enhance the flow-oriented teams with a product-related approach keeping their independence intact.\n\nWhen paired with Domain-driven Design methods, Stream-Aligned Teams can manage independent sections of the domain with services structured to match (reference EventBridge Storming).\n\nPlatform Groups could play a key role in decreasing the cognitive burden on Stream-Aligned teams by facilitating fundamental services which don't need to be developed by the Stream-Aligned teams anymore. These Platform groups should consider themselves as service suppliers, treating Stream-Aligned teams as their \"clientele\".\n### What's CDK?\n\nThe AWS Cloud Development Kit, short as AWS CDK, is open-source software framework aimed for defining application resources for cloud using familiar programming languages.\n\nAWS CDK enables programmers to use the programming language they prefer like TypeScript to establish their architecture. It enables programmers to utilize the dynamic expressive attributes of contemporary programming languages, encapsulating principles such as Abstraction, Encapsulation & Composition.\n\nBehind the scenes, CDK formulates recognizable CloudFormation, offering consistent deployment and compatibility with pre-existing deployment tools and practices.\n```"}, {"role": "assistant", "content": "Complicated Subsystem team: Domain specific complex expertise (e.g. algorithmic, machine learning)\n4. Platform Team: Accelerate the Stream-Aligned teams via a product based approach while maintaining their autonomy.\n\nIn combination with Domain-Driven Design techniques Stream-Aligned Teams can work on independent areas of the domain with services architected accordingly (see EventBridge Storming).\n\nPlatform Teams can greatly reduce the cognitive load placed on Stream-Aligned teams by providing foundational services that no longer need to be created by the Stream-Aligned teams. These Platform teams should see themselves as service providers, with the Stream-Aligned teams as their \u201ccustomers\u201d.\n### What is CDK?\n\nThe AWS Cloud Development Kit (AWS CDK) is an open-source software development framework to define your cloud application resources using familiar programming languages.\n\nAWS CDK allows developers to use their programming language of choice, e.g. TypeScript, to define their architecture. It allows developers to use the powerful expressive characteristics of modern programming languages, including concepts like Abstraction, Encapsulation & Composition.\n\nUnder the hood CDK synthesises familiar CloudFormation, providing a repeatable deployment and compatibility with existing deployment tooling and practices."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The AWS Cloud Development Kit (AWS CDK) is an open-source framework for software development. It is used to define the resources of cloud applications via standard programming languages.\n\nThe AWS CDK allows the use of a preferred programming language, for instance TypeScript, for the definition of the toolkit architecture. Accordingly, it allows the utilisation of modern programming languages characteristics such as Abstraction, Encapsulation and Composition.\n\nAt its core, CDK converts its original structure into cloud formation facilitating repeatable deployments and compatibility with existing tooling and common practices.\n\n\nA Construct in AWS CDK represents the primary elements of AWS architectures. It has all the components needed to generate the CloudFormation essential for the deployment of a specific architecture component.\n\nThere are three levels of constructs and each one builds on top of the other:\n- Level 1: AWS CloudFormation-Only\n- Level 2: Curated\n- Level 3: Patterns\n\n```plaintext\nL1 constructs are directly generated from CloudFormation, which reflects a complete feature interface to generate resources at the lowest level of abstraction.\nconst bucket = new s3.CfnBucket(this, \"MyBucket\", {\n  bucketName: \"MyBucket\"\n});\n\nL2 constructs are based on L1, with added default values, boilerplate and glue.\nimport * as s3 from 'aws-cdk-lib/aws-s3';\n// \"this\" is HelloCdkStack\nnew s3.Bucket(this, 'MyFirstBucket', {\n  versioned: true\n});\n```\n\nLastly, L3 constructs act as patterns for standard use cases, such as a REST API. This often involves multiple resource types composed into an end-to-end framework.\n```"}, {"role": "assistant", "content": "### What is CDK?\n\nThe AWS Cloud Development Kit (AWS CDK) is an open-source software development framework to define your cloud application resources using familiar programming languages.\n\nAWS CDK allows developers to use their programming language of choice, e.g. TypeScript, to define their architecture. It allows developers to use the powerful expressive characteristics of modern programming languages, including concepts like Abstraction, Encapsulation & Composition.\n\nUnder the hood CDK synthesises familiar CloudFormation, providing a repeatable deployment and compatibility with existing deployment tooling and practices.\n#### The Construct Hierarchy\n\nCDK Constructs represent the basic building blocks of AWS architectures. They contain everything needed to generate the CloudFormation needed to deploy a part of the architecture.\n\nThere are 3 levels of construct, each building on each other:\n- Level 1: AWS CloudFormation-Only\n- Level 2: Curated\n- Level 3: Patterns\n\n```plaintext\nL1 constructs are generated directly from CloudFormation, and therefore represent a feature complete interface to generate resources at the lowest level of abstraction.\nconst bucket = new s3.CfnBucket(this, \"MyBucket\", {\n  bucketName: \"MyBucket\"\n});\n\nL2 constructs build on L1, adding some default values, boilerplate and glue.\nimport * as s3 from 'aws-cdk-lib/aws-s3';\n// \"this\" is HelloCdkStack\nnew s3.Bucket(this, 'MyFirstBucket', {\n  versioned: true\n});\n```\n\nAnd finally L3 constructs are patterns for common use cases, e.g. a REST API, often composing multiple types of resources into an end-to-end pattern."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```EventBridge Pipes, developed by Amazon Web Services (AWS), offers an efficient method for transferring data between sources and destinations. In the process, the data can be filtered, enriched, and transformed. Formerly known as the EventBridge Bus, it has recently been extended from being a standalone product to a comprehensive product suite for event-driven application development, inclusive of EventBuses, Pipes, and Schedulers.\n\nWhile constructing high-scale messaging systems that manage vast amounts of parallel messages without losing any due to systems failures can be quite complex, Pipes simplifies this task. It presents a solution that ensures broad horizontal scaling and redundancy with dead letter queues, alongside inbuilt transformations, filtering, and enrichment functionalities.\n\nBy leveraging the benefits of the Cloud Development Kit (CDK), we are able to construct and implement our messaging service right from our favored development environment without needing to navigate the AWS console or compose YAML, as it provides support for Typescript as well.\n```"}, {"role": "assistant", "content": "# Building a Robust Serverless Messaging Service with Amazon EventBridge Pipes and CDK\n\nEventBridge Pipes is a powerful new tool from Amazon Web Services (AWS) that makes it easy to move data between sources and targets while filtering, enriching and even transforming the data en route. EventBridge used to be a singular product, the EventBridge Bus. However, in the last few months, AWS has expanded it into a complete product suite for building event-driven applications, including; EventBuses, Pipes and Schedulers.\n\nHigh-scale messaging systems are traditionally complex to build. They must be able to handle a high volume of messages concurrently whilst ensuring that messages are not lost due to failures. In addition, they often require complex features such as routing, filtering, and transformation of messages, which can be challenging to implement and maintain. Pipes solves these problems by providing industry-leading horizontal scaling, redundancy with dead letter queues and inbuilt transformations, filtering and enrichment capabilities.\n\nUsing the Cloud Development Kit (CDK), we can build and deploy our messaging service without leaving our chosen development environment (in Typescript, too! Bye-bye AWS console and bye-bye writing YAML)."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The text provided describes the need for a messaging service that can handle high volumes of messages concurrently without losing them due to failures. It mentions that such services often require complex features like routing, filtering, and transforming messages, which can be challenging to implement and maintain. The text then introduces Pipes as a solution that provides industry-leading horizontal scaling, redundancy with dead letter queues, and built-in transformations, filtering, and enrichment capabilities.\n\nThe text also mentions using the Cloud Development Kit (CDK) to build and deploy the messaging service without leaving the chosen development environment (Typescript) and avoiding the AWS console and writing YAML.\n\nAn example application is then described, which is a web app with a fully serverless backend. It requires a microservice that sends an email to the user whenever they change something security-related on their account, such as an address. The account data is stored in a DynamoDB table.\n\nThe messaging service should construct and send an email to the user with identifying information about them and the change that has occurred. The email should include the user's email address, subject line, and body with placeholders for the modified attribute, first name, and last name.\n\nHowever, the DynamoDB table does not store names or email addresses, only a Cognito sub (user id). The Cognito User Pool is used as a single source of truth for security reasons and GDPR compliance. Therefore, the rest of the information, such as name and email address, must be queried from Cognito directly.\n```"}, {"role": "assistant", "content": "They must be able to handle a high volume of messages concurrently whilst ensuring that messages are not lost due to failures. In addition, they often require complex features such as routing, filtering, and transformation of messages, which can be challenging to implement and maintain. Pipes solves these problems by providing industry-leading horizontal scaling, redundancy with dead letter queues and inbuilt transformations, filtering and enrichment capabilities.\n\nUsing the Cloud Development Kit (CDK), we can build and deploy our messaging service without leaving our chosen development environment (in Typescript, too! Bye-bye AWS console and bye-bye writing YAML).\n## Example Application\n\nWe are building a web app with a fully serverless backend. We need a microservice that sends an email to the user whenever the user changes something security-related on their account, such as an address. This account data is stored in a DynamoDB table. The messaging service should construct and send an email to the user with identifying information about them and the change that has occurred.\n\n```\nToAddresses: [userEmail]\nSubject: `Notice: ${modifiedAttribute} Change Successful`\nBody: `Dear ${firstName} ${lastName},\n\nThis is confirmation that your ${modifiedAttribute} for your account associated \nIf this is a change you made, we're good to go!\n \nIf you did not personally request this change, please reset your password or con\n```\n\nThe catch is that the DynamoDB table the user has modified does not store names or email addresses, only a Cognito sub (user id). The Cognito User Pool is used as a single source of truth for security reasons and GDPR compliance. Therefore, we must query the rest of the information, such as name and email address, from Cognito directly."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The Cognito User Pool serves as a singular source for both security and GDPR compliance objectives. Consequently, details such as the user's name and email address will need to be elicited from Cognito.\n\nPipes, introduced on December 1, 2022, by AWS, streamlines the process of assembling integrations between event producers and consumers for developers, thus diminishing the requirement for composing integration code. A Pipe's function is to route events automatically from the source to its destination which simplifies the task of creating event-driven applications.\n\nIn the case of EventBridge buses on AWS, these act as a central exchange point for transferring messages or \"events\" between various sources. Envision it as an intermediate dispatcher applying routing rules to direct messages to the desired services. \n\nIn contrast, EventBridge Pipes is purposed to channel events without any interruption from one AWS facility to another. This allows the bridging of event sources aggregated with a specified service without the necessity of any additional connecting code. Moreover, you have the potential to connect a Lambda function or any other equivalent AWS service to refine the events prior to their receiving point.\n\nThe adaptability that Pipes provides makes it an ideal option for use in our messaging service. We have an emerging situation where an event source requires to transfer an event to another AWS service. The particular event demands further information acquisition and incorporation along the course of its flow. With the utilization of Pipes, we hold the capability to assemble every piece of infrastructure with the insertion of minimal additional code.\n```"}, {"role": "assistant", "content": "The Cognito User Pool is used as a single source of truth for security reasons and GDPR compliance. Therefore, we must query the rest of the information, such as name and email address, from Cognito directly.\n## What are Pipes?\n\nAWS released Pipes on 1st December 2022. It makes life easier for developers to \u201ccreate point-to-point integrations between event producers and consumers\u201d while reducing the need for writing integration code. Essentially a Pipe automatically routes events from source to target, reducing the amount of extra code to write when building event-driven applications.\n## How are EventBridge Pipes different from EventBridge Buses?\n\nEvent buses are like post offices for AWS services. Different resources can send and receive messages or \u201cevents\u201d to each other through this central service. Think of it as a middleman using forwarding rules to direct messages to the target services. \n\nEventBridge Pipes, on the other hand, passes events from one AWS service directly to another. They allow you to connect an event source to a target service with no extra glue code needed. In addition, you can connect a Lambda function or other AWS service to manipulate or \u201cenrich\u201d the events before they reach the target.\n## Why use Pipes?\n\nThe flexibility of Pipes makes it perfect for our messaging service. We have an event source needing to send an event to another AWS service. The event needs extra information fetching and adding somewhere along the flow. With Pipes, we can plug together each piece of infrastructure with minimal extra code."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Pipes have the ability to connect each piece of infrastructure with minimal additional code. \n## What is the distinction between EventBridge Pipes and EventBridge Buses?\n\nThink of event buses as post offices for AWS services. Various resources can deliver and obtain messages or \"events\" to each other through this centralised mechanism. Consider it as a third party who uses forwarding plus rules to direct messages to the desired services. \n\nConversely, EventBridge Pipes delivers events from one AWS service directly to another. This feature lets you connect an event origin to a target service without the need for any additional glue code. Further, you have the option to connect a Lambda function or yet another AWS service to modify or \"enrich\" the events prior to their arrival at the target.\n## Architecture Construction\n\nIn order to construct the Pipe, we first need to identify a source that generates the event. In this scenario, the DynamoDB stream of the table serves this purpose.\n### DynamoDB Streams\n\nDynamoDB Streams is an aspect of Amazon DynamoDB that captures alterations to the data in a DynamoDB table almost instantaneously. \n## Infrastructure as Code (IaC)\n\nCDK is our chosen method for IaC for this project. CDK allows us to write IaC in directive programming languages like TypeScript or Python rather than indicative languages like YAML or JSON.\n```"}, {"role": "assistant", "content": "With Pipes, we can plug together each piece of infrastructure with minimal extra code. \n## How are EventBridge Pipes different from EventBridge Buses?\n\nEvent buses are like post offices for AWS services. Different resources can send and receive messages or \u201cevents\u201d to each other through this central service. Think of it as a middleman using forwarding rules to direct messages to the target services. \n\nEventBridge Pipes, on the other hand, passes events from one AWS service directly to another. They allow you to connect an event source to a target service with no extra glue code needed. In addition, you can connect a Lambda function or other AWS service to manipulate or \u201cenrich\u201d the events before they reach the target.\n## Building the Architecture\n\nTo build the Pipe, we must first choose a source producing the event. In our case, this is the DynamoDB stream of the table.\n### DynamoDB Streams\n\nDynamoDB Streams is a feature of Amazon DynamoDB that allows you to capture changes to the data in a DynamoDB table in near real-time. \n## Infrastructure as Code (IaC)\n\nFor this project, we are using CDK for IaC. CDK allows us to write IaC in imperative programming languages such as TypeScript or Python rather than declarative ones such as YAML or JSON."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```We're using the AWS Cloud Development Kit (CDK) as an Infrastructure as Code (IaC) solution for this project. This lets us code in languages like Python and TypeScript instead of YAML or JSON.\n\nOne feature we're using from Amazon is DynamoDB Streams. This captures changes in data from a DynamoDB table in almost real time.\n\nHere's the construction process for our architecture:\n\n- First, decide on the event source. We're using the table's DynamoDB stream.\n\nHere's the sample TypeScript code for creating the table:\n\n```typescript\nconst sourceTable = new Table(this, 'example-table-id', {\n  tableName: 'example-table',\n  partitionKey: { name: 'PK', type: AttributeType.STRING },\n  sortKey: { name: 'SK', type: AttributeType.STRING },\n  stream: StreamViewType.NEW_AND_OLD_IMAGES,\n  billingMode: BillingMode.PAY_PER_REQUEST,\n  removalPolicy: RemovalPolicy.DESTROY,\n  pointInTimeRecovery: true,\n});\n```\nThe Lambda function code for our enrichment handler is below:\n\n```typescript\nexport const handler = async (event: DynamoDBStreamEvent): Promise<string> => {\n  const record: DynamoDBRecord = event.Records[0];\n  if (record.dynamodb?.NewImage == null && record.dynamodb?.OldImage == null) {\n    throw new Error('No NewImage or OldImage found');\n  }\n  const modifiedAttributes: string[] = [];\n  for (const key in record.dynamodb.NewImage) {\n    if (record.dynamodb.NewImage[key].S !== record.dynamodb.OldImage?.[key].S) {\n      modifiedAttributes.push(key);\n    }\n  }\n  if (modifiedAttributes.length === 0) {\n    throw new Error('No changed parameters found');\n  }\n  const userId = record.dynamodb.NewImage?.userId.S;\n  if (userId == null) {\n    throw new Error('No userId found');\n  }\n  const user = await getUser(userId);\n  return JSON.stringify({\n    ...user,\n    modifiedAttributes,\n  } as MessageBody);\n};\n```\n```"}, {"role": "assistant", "content": "## Infrastructure as Code (IaC)\n\nFor this project, we are using CDK for IaC. CDK allows us to write IaC in imperative programming languages such as TypeScript or Python rather than declarative ones such as YAML or JSON. \n### DynamoDB Streams\n\nDynamoDB Streams is a feature of Amazon DynamoDB that allows you to capture changes to the data in a DynamoDB table in near real-time. \n## Building the Architecture\n\nTo build the Pipe, we must first choose a source producing the event. In our case, this is the DynamoDB stream of the table.\n### DynamoDB\n\nThe table is created as shown below. \n\n```typescript\nconst sourceTable = new Table(this, 'example-table-id', {\n  tableName: 'example-table',\n  partitionKey: { name: 'PK', type: AttributeType.STRING },\n  sortKey: { name: 'SK', type: AttributeType.STRING },\n  stream: StreamViewType.NEW_AND_OLD_IMAGES,\n  billingMode: BillingMode.PAY_PER_REQUEST,\n  removalPolicy: RemovalPolicy.DESTROY,\n  pointInTimeRecovery: true,\n});\n```\n### Lambdas\n\nThe code for the enrichment Lambda handler function is found below.\n\n```typescript\nexport const handler = async (event: DynamoDBStreamEvent): Promise<string> => {\n  const record: DynamoDBRecord = event.Records[0];\n  if (record.dynamodb?.NewImage == null && record.dynamodb?.OldImage == null) {\n    throw new Error('No NewImage or OldImage found');\n  }\n  const modifiedAttributes: string[] = [];\n  for (const key in record.dynamodb.NewImage) {\n    if (record.dynamodb.NewImage[key].S !== record.dynamodb.OldImage?.[key].S) {\n      modifiedAttributes.push(key);\n    }\n  }\n  if (modifiedAttributes.length === 0) {\n    throw new Error('No changed parameters found');\n  }\n  const userId = record.dynamodb.NewImage?.userId.S;\n  if (userId == null) {\n    throw new Error('No userId found');\n  }\n  const user = await getUser(userId);\n  return JSON.stringify({\n    ...user,\n    modifiedAttributes,\n  } as MessageBody);\n};\n```"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The management of risks associated with generative artificial intelligence (GenAI) systems is a critical consideration, particularly in regard to the supply chain involved in their development and deployment. The supply chain encompasses various stages, including data acquisition, model training, deployment, and ongoing monitoring and maintenance. Each stage presents unique challenges that necessitate careful attention and mitigation strategies.\n\nDuring the data acquisition phase, it is crucial to ensure the quality, relevance, and diversity of the data used for training GenAI models. Biased or low-quality data can lead to flawed models that perpetuate harmful biases or produce inaccurate outputs. Rigorous data curation processes, including techniques like data cleaning, debiasing, and augmentation, are essential to mitigate these risks.\n\nThe model training stage involves the use of complex algorithms and computational resources to develop GenAI systems. Risks in this stage include unintended model behavior, such as amplifying biases present in the training data or generating outputs that violate ethical or legal norms. Robust model evaluation, testing, and validation procedures, as well as the incorporation of ethical principles and guidelines, are necessary to address these concerns.\n\nDuring deployment, GenAI systems may interact with various stakeholders, including end-users, third-party applications, and other AI systems. Risks at this stage include potential misuse, security vulnerabilities, and unintended consequences arising from the interaction between GenAI systems and their environment. Implementing robust security measures, monitoring mechanisms, and clear guidelines for responsible use are crucial to mitigate these risks.\n\nOngoing monitoring and maintenance are essential to ensure the continued safe and effective operation of GenAI systems. As these systems interact with dynamic environments and new data, their behavior may drift or become outdated, leading to potential risks. Regular model updates, continuous monitoring, and incident response plans are necessary to address these challenges.\n\nEffective risk management in the GenAI supply chain requires a collaborative effort involving developers, researchers, policymakers, and end-users. Establishing clear governance frameworks, fostering transparency and accountability, and promoting responsible innovation are essential steps towards mitigating the risks associated with GenAI systems while harnessing their potential benefits.\n```"}, {"role": "assistant", "content": "# Managing GenAI Risk: The Supply Chain"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```# The Relationship of GenAI Risk and Supply Chain\n\n## Setting the Scene\n\nIn a short time frame, Generative AI (GenAI) has grabbed attention worldwide through high-profile applications such as ChatGPT, Midjourney, and DALL-E. People are applying these pioneering applications after their first encounter and have begun to integrate them into several business areas. One of the tools, known as Copilot, developed by GitHub and built on a GPT model, is now widely used throughout my engineering team. On top of previous databases, we have developed several productivity tools and received inquiries from clients looking to get on board with this technology trend. Current applications are predominantly for customer service and data recovery fields, but there are plenty who are eyeing applications in the creative and knowledge sectors as well.\n\nPublic attention is not only on the advantages of integrating these technologies, but security and regulation issues are also being scrutinised. ChatGPT's creator, open-source AI development company OpenAI, has been reviewed by global regulatory departments. Two major investment banks, JP Morgan and Goldman Sachs, have prohibited the use of ChatGPT due to concerns about data security and commercial rights issues. In response to wrongful assumptions presented by ChatGPT and alleged copyright violations, The Federal Trade Commission (FTC) of the U.S. is now calling for a declaration of how each underlying model was trained to use data by ChatGPT.\n\nThe fundamental technology, Large Language Models (LLMs) from which GenAIs are formed, isn't inherently dangerous. However, due to their need for a large number of data and uninterrupted learning, problems can arise if they are not managed properly. While it's plausible to consider that commercial interests might be gleaned from petabytes of openly accessible material given during public LLM training exercises, it also brings up questions of generated data owner rights. To top it off, any new interactions received during training further improves the model, which proposes any data given by a user can be utilized. For instance, if syncing a free-to-use GPT application, air marketing secrets firsthand to the application itself concerning an undisclosed launch campaign by a multinational soft drinks brand, a senior executive of another competitive brand could also be fishing for the same information to new product launches using the same application itself. Conceivable concerns also arise when individuals begin using GenAI applications to transcribe every business meeting.\n\nA large body of my work within GenAI is helping clients comprehend and scale down associated risks. The straightforward interpretation offers risks that can emerge anywhere throughout the GenAI applications supply chain, from model training, basic model selection right through to perfecting setting questions, settings, hosting, fine tuning, and integrating. Analyzing, declining, and demonstrating these risks will equate to businesses reaping GenAI benefits while bypassing law-suit chances, societal impacts, and brand identity threats.\n```"}, {"role": "assistant", "content": "# Managing GenAI Risk: The Supply Chain\n\n## Introduction\n\nGenerative AI (GenAI) has captured the imagination of millions in just a few months since the release of high-profile applications like ChatGPT, DALL-E and Midjourney. After being stunned by the seemingly magical interaction with these early applications, many of us started to see the real-world uses in our work. My entire engineering team leverages Copilot (the AI pair programmer from GitHub powered by a GPT model), and we've built several productivity tools on top of our existing knowledge banks. Our clients have also been quick to react and ask for help in leveraging this technology. Right now, many use cases are for obvious customer support and information retrieval, but creative/knowledge work is already in the sight of many.\n\nAlmost as heavily publicised as the benefits of such technologies are the regulatory and privacy concerns. Even OpenAI, the company behind ChatGPT, has received attention from regulators across the globe. Other companies, including Goldman Sachs and JP Morgan, have also been quick to ban ChatGPT due to data security and intellectual property concerns. The US Federal Trade Commission (FTC) is now demanding disclosure on the data used to train ChatGPT\u2019s underlying models in reaction to false accusations made by ChatGPT and alleged copyright infringement.\n\nThe core technology behind GenAI, Large Language Models (LLMs), are not inherently risky but require such a large volume of data and learning that can lead to issues when not handled well. When LLMs are trained on petabytes of public data, it's not inconceivable for intellectual property to be picked up, leading to complex questions about the ownership of generated materials. Moreover, these models are further trained on their interactions, which means that the content of \"prompts\" (questions asked by the user) can be \"learned\" by the model, potentially leading to leaks. For example, if I were working at a marketing firm and asked a free public GPT application for advice on how to handle the secret launch of a new soda by a large beverage provider, it's possible that an executive at a direct competitor could be asking the same application for new product ideas. As people begin to use GenAI applications to automatically take notes during all their business meetings, the situation becomes concerning.\n\nMost of the work I\u2019ve been involved in with GenAI has been in helping clients understand and mitigate risks. Simply put, risks exist throughout the supply chain of the GenAI application, from model training and foundational model selection through to prompt engineering, context, hosting, fine tuning, and integration. By understanding, mitigating, and documenting these risks, companies can leverage the power of GenAI while avoiding legal, societal, and reputational risks."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The increasing usage of GenAI application automations for note harvesting during business meetings presents a salient issue.\n\nIn most tasks associated with GenAI, the main objective is to assist clients in both comprehending and reducing potential threats. Potentially threatening situations may occur at various stages of the GenAI software procession, ranging from model instruction and choosing an elementary model, to fashioning prompts, arranging contexts, hosting, delicately fine tuning parameters, and interlocking segments. Recognising, reducing, and chronicling these possible issues will enable corporations to harness the capabilities of GenAI, while simultaneously sidestepping potential legal, social, and status-related hazards.\n\nFundamental Model Instruction/Selection Contemplations\n\nLong-Form Models (LLMs) are honed using profound studying methodologies on immense data repositories, outscaling the standard volumes used previously in profound learning modelling. The initial data source and the corresponding licences and assurances pertaining to it are pivotal factors or 'control points' in averting oblivious violations of Intellectual Property (IP) rights. For the majority of corporations, the application of GenAI means engaging with third-party foundational models (FMs) that were developed externally. Owing to the overwhelming magnitudes and convolument of third party FMs, tracing individual elements back to their original sources becomes a near impossible task, which leads companies into a sphere of obscurity in defending themselves against IP claims. Employing the classic phrase 'input of poor-quality data results in the production of equally lacklustre output', one can extend this notion to IPs as 'Ingress of IP results in a resultant IP threat exponentiation'. Threat reduction can be attained most effectively through conducting rigorous analytical tests for assurances on the third-party foundational models being utilised, or alternatively, initiating a stringent data validation method should there be a need to construct a FM with an element of component specificity.\n\nIn addition to the IP-related potential risks, when one undertakes the development of their own FM, there arises the emergence of data-related threats. For instance, a vast financial entity making use of an internal library of data could decide to train a GPT PM and utilise it to enhance their employees' work efficiency. Data combinations sourced from the CRM or proprietary knowledge banks could be easily roped into the zone of influence of the used datasets for educational purposes. Consequently, an examiner might compare customer request data sets with previously-entered requests of the same nature, thereby potentially gaining unauthorised access to sensitive customer-related data imperceptibly. Crucially, openly festivities nature there's a control bypassing occurring that circumvents standard Client Access Protocols that are instituted into everyday data handling systems. Knowing the pure unaltered progression of the data and comprehending the reasons for its very creation is fundamental to the prerequisite controls needed to guard against erroneous leaks, even in the internal-only information world.\n```"}, {"role": "assistant", "content": "As people begin to use GenAI applications to automatically take notes during all their business meetings, the situation becomes concerning.\n\nMost of the work I\u2019ve been involved in with GenAI has been in helping clients understand and mitigate risks. Simply put, risks exist throughout the supply chain of the GenAI application, from model training and foundational model selection through to prompt engineering, context, hosting, fine tuning, and integration. By understanding, mitigating, and documenting these risks, companies can leverage the power of GenAI while avoiding legal, societal, and reputational risks.\n## Foundational Model Training / Selection\n\nLLMs are trained using deep learning techniques on extremely large datasets, much larger than those typically used to train previous deep learning models. The source of the data, as well as the licensing and guarantees on this data, are crucial in avoiding unintentional infringement of IP protections. For most companies, the use of GenAI will involve the use of a third-party Foundational Models (FMs). Due to the size and scale of the parameters involved in these large FMs, it is almost impossible to reference back to sources, leading to a grey area when it comes to defending against IP claims. To reuse the simple \"garbage in \u21d2 garbage out\" analogy from the ML world, \"IP in \u21d2 IP risk out\". The best way to mitigate this risk is to conduct due diligence on third-party FMs in use, or indeed, to establish a rigorous process for data validation if you are building a specialized FMs.\n\nApart from the IP risks, when training your own FM there is also a data security risk. For example, if you are a large financial institution that leverages an internal knowledge bank, you may decide to train a GPT-powered assistant to make your staff more efficient. The large corpus of data from your knowledge bank and CRM could be used as part of the training data. Then, an analyst may ask a query from a client and get access to data from or about another client, with potentially sensitive information. This would bypass the traditional \"role-based access controls\" that your other systems and tools have in place. Knowing the data's provenance, purpose, and permissions required to access it in the current organization is crucial to ensuring that a leaky model, even if only used internally, is not created."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Large financial institutions may utilize internal knowledge banks and customer relationship management (CRM) data to train language models for assisting staff. However, this process raises concerns about data privacy and unauthorized access. Analysts querying the model could potentially access sensitive client information they should not have access to, bypassing traditional role-based access controls. To mitigate risks, it is crucial to thoroughly vet the provenance, purpose, and required permissions for data used in model training. This ensures that no sensitive or unauthorized information is inadvertently incorporated, preventing the creation of a \"leaky\" model that could expose sensitive data, even if only used internally.\n\nThe process of fine-tuning language models for specific use cases involves further training a pre-trained model on a smaller, domain-specific dataset. While the data volume is typically lower than initial training, making auditing more manageable, it is still essential to implement robust processes for vetting the provenance, purpose, and permission level of the fine-tuning data. Teams should be educated on these processes, and all training activities should be clearly documented for audit and accountability purposes.\n```"}, {"role": "assistant", "content": "For example, if you are a large financial institution that leverages an internal knowledge bank, you may decide to train a GPT-powered assistant to make your staff more efficient. The large corpus of data from your knowledge bank and CRM could be used as part of the training data. Then, an analyst may ask a query from a client and get access to data from or about another client, with potentially sensitive information. This would bypass the traditional \"role-based access controls\" that your other systems and tools have in place. Knowing the data's provenance, purpose, and permissions required to access it in the current organization is crucial to ensuring that a leaky model, even if only used internally, is not created.\n## Fine Tuning\n\nLLMs can be customized for a particular use case through a process called \"Fine Tuning\". In this approach, a copy of a trained model with known weights is taken and then trained again with a use-case-specific data set, starting from the initial weights. This process can be made more efficient by training a subset of weights in an approach called \"Parameter Efficient Tuning\". However, this process presents an opportunity for the model to be contaminated with IP-infringing data sets or sensitive information. Fortunately, the volume of data used for fine tuning is typically orders of magnitude lower than that used to train the FM. This means that auditing is simpler, and mistakes are less likely. Nonetheless, it is essential to have strong processes in place regarding the provenance, purpose, and permission level of data used for fine tuning, as well as to educate teams and clearly document any training for audit and accountability purposes."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The technique known as \"Parameter Efficient Tuning\" can potentially enhance efficiency by training a subset of weights. The downside to the process, however, is creating an avenue for the model to become tainted with intellectual property-violating data sets or confidential material. The bright side is that the quantity of data employed for refining is usually drastically lesser than that utilized in foundational model training, rendering the auditing process simpler and less prone to errors. Nevertheless, it is a necessity to establish robust procedures concerning the origin, intent, and access level of data employed for refining, along with educating teams and streamlining any training documentation for conceivable auditing and accountability purposes.\n\nRegarding hosting matters, the place where model perches for training and inference sessions is significantly important, particularly when businesses adapt base models by means of high-value intellectual property. In absence of secure hosting, the intellectual property and sensitive details utilized for model training may be jeopardized. Also, there is the potential for prompts and outputs to contain confidential information, which necessitates that the round trip from the application layer to the model, and conversely, should be secure and in accordance with data privateness regulations. It's additionally essential to exercise control over where training and hosting of models occur, to assure compliance with data sovereignty laws. Charting out an application's full route of prompts to the outputs, coupled with the place and security duties of any contracting components, is essential to getting a thorough perspective on the hosting facet. Cloud service providers have responded swiftly, making regional segregation and security controls attainable hosting options.\n```"}, {"role": "assistant", "content": "This process can be made more efficient by training a subset of weights in an approach called \"Parameter Efficient Tuning\". However, this process presents an opportunity for the model to be contaminated with IP-infringing data sets or sensitive information. Fortunately, the volume of data used for fine tuning is typically orders of magnitude lower than that used to train the FM. This means that auditing is simpler, and mistakes are less likely. Nonetheless, it is essential to have strong processes in place regarding the provenance, purpose, and permission level of data used for fine tuning, as well as to educate teams and clearly document any training for audit and accountability purposes.\n## Hosting\n\nThe location where a model resides during training and inference is crucial, especially when companies customize foundational models using high-value intellectual property. Without secure hosting, the intellectual property and sensitive information used to train the model could be at risk. Additionally, the prompts and outputs may contain sensitive information, and the roundtrip from the application layer to the model and back needs to be secure and compliant with data privacy laws. It is also important to have control over where models are trained and hosted to ensure compliance with data sovereignty laws. Mapping out the end-to-end flow of application prompts to outputs, the locations and security responsibilities of any outsourced components is key to understand your holistic hosting picture. Cloud providers have been quick to make hosting options available with regional segregation and security controls."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Furthermore, the suggestions and results might encompass confidential data, necessitating secure passage from the application phase to the model phase and rewind, satisfying data confidentiality laws. Moreover, managing where models are developed and housed to assure adherence to data sovereignty laws is vital. Drafting a map for the complete journey of application suggestions to outcomes, the places, and safety duties of any third-party elements, is crucial for a comprehensive apprehension of your hosting panorama. Cloud resources have proactively delivered accommodation choices with regional seclusion and security constraints.\n## Implementation\n\nIt's vital while granting users admittance to the LLM's outcomes, to mitigate the threat of illusion. An LLM may convincingly and decisively respond with either a solution, even if it's untrue, resulting in vilification legal trouble if individuals are dishonestly implicated in unlawful activities using fake connections to unreal news stories. This produces a risk of slandering and misinformation affecting crucial business operations. Despite the continuous research on developing new FM that are less predisposed to illusions, it is important to enlighten final users and blend user interface facets that hint users about the potential for incorrect information. Additionally, safeguarding measures must be installed to shield against harmful utilization of public-proxy applications, like producing false news or inappropriate content (profanity, violence, hate speech, etc.). There are also FM under development with inherent safeguard like Amazon's Titan FM's. \n## Surroundings Embeddings\n\nEmbeddings vectors grant LLM's the ability to research for equal data. Vector embedding is spawned from application data and stored as vectors inside a database. Consequently, a user's inquiry can then be switched to a vector, allowing for a similarity quest providing the LLM with more details and framework to the prompt.\n```"}, {"role": "assistant", "content": "Additionally, the prompts and outputs may contain sensitive information, and the roundtrip from the application layer to the model and back needs to be secure and compliant with data privacy laws. It is also important to have control over where models are trained and hosted to ensure compliance with data sovereignty laws. Mapping out the end-to-end flow of application prompts to outputs, the locations and security responsibilities of any outsourced components is key to understand your holistic hosting picture. Cloud providers have been quick to make hosting options available with regional segregation and security controls.\n## Application\n\nWhen providing users with access to the outputs of LLMs, it is crucial to manage the risk of hallucination. An LLM may confidently and plausibly respond with an answer, even if it is false, leading to defamation lawsuits if individuals are falsely accused of crimes with fake links to non-existent news articles. This creates a risk of defamation and of false information impacting critical business processes. While researchers are working on creating new FM that are less susceptible to hallucinations, it is important to educate end users and incorporate UI features that remind users of the potential for false information. In addition, safeguards should be implemented to protect against malicious use of public-facing applications, such as the creation of fake news or inappropriate content (profanity, violence, hate speech, etc.). New FM with built-in protection, such as Amazon's Titan FMs, are also emerging.\n## Context Embeddings\n\nVector embeddings enable LLMs to search for similar data. Vector embeddings are generated from application data and stored as vectors inside a database. A user's query can then be converted to an embedding, and a similarity search can be performed, providing a prompt to the LLM with formatting and further information."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The AWS Cloud Development Kit (CDK) is a framework that allows developers to define cloud application resources using programming languages like TypeScript or Python. This approach enables developers to leverage their existing knowledge and build cloud infrastructure rapidly. By utilizing provided high-level components designed with best practices, the CDK abstracts complexity away from developers, and by encapsulating resources into constructs, it facilitates scalability and management of complex resources.\n\nInfrastructure as Code (IaC) is a machine-readable representation of all the resources required for a cloud application. It offers benefits such as version control, a single source of truth, reusability across multiple environments, typing for suggestions and autocomplete, and the ability to test the IaC.\n\nThe CDK distinguishes itself from other IaC options like Serverless Framework, Terraform, CloudFormation, and AWS SAM by allowing developers to write IaC in imperative programming languages like TypeScript and Python, rather than declarative formats like YAML or JSON. These languages offer superior development experiences, with robust code editors, formatters, syntax checkers, and linters, making the code more readable and comprehensible to developers.\n\nThe CDK code is transpiled into CloudFormation templates, AWS's YAML or JSON IaC format. During this 'synth' process, additional errors in the IaC can be detected, similar to compile-time errors in compiled languages like C or Rust. This early error detection saves developers time by avoiding failed deployments.\n```"}, {"role": "assistant", "content": "# Finally, the end of YAML? AWS CDK for Serverless\n## The AWS Cloud Development Kit (CDK) \nThe AWS Cloud Development Kit (CDK) is a framework that lets you define your Cloud Application resources using familiar programming languages such as TypeScript or Python. This allows developers familiar with these languages to apply their existing knowledge and get to grips with building Cloud infrastructure rapidly. By using provided high-level components, built with best practices as default, we can abstract much of the complexity away from the developer, and by encapsulating resources into constructs we can continue this practice as our resources scale in size and complexity.\n## What is Infrastructure as Code?\nInfrastructure as Code (IaC) is a discrete representation of all the various resources that our cloud application will use, in machine readable definition files. The main benefits this offers are:\n- Version control via git\n- Single source of truth\n- Reusability \u2014 e.g. easy to deploy several production-like environments\n- Typing \u2014 allowing suggestions and autocomplete\n- Testing of the IaC\n## Why CDK?\nIaC is not a new concept, and there are many options to choose from, including Serverless Framework, Terraform, CloudFormation and AWS SAM. One of the unique features of CDK compared to these others is that it allows us to write IaC in imperative programming languages such as TypeScript and Python, rather than declarative ones such as YAML or JSON. The expressive power of these languages and their ubiquitous support by code editors, formatters, syntax checkers and linters is leagues ahead of support for any YAML-based IaC. This makes the development experience more approachable and rapid, as many errors can be caught by static checks performed by the editor. The code is also more easily readable and comprehensible to developers written in these familiar languages than in YAML.\n\nThis CDK code is transpiled into CloudFormation templates, AWS\u2019s YAML or JSON IaC format. During this \u2018synth\u2019 process, further errors in the IaC can be caught \u2014 I like to think of these like compile-time errors in compiled languages like C or Rust. This means we don\u2019t have to wait for a failed deployment to catch such errors, saving the developers time."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The development experience is streamlined and efficient, as many errors can be detected through static checks performed by the editor. The code is also more easily readable and comprehensible to developers written in these familiar languages than in YAML.\n\nThe CDK code is transpiled into CloudFormation templates, AWS's YAML or JSON IaC format. During this 'synth' process, further errors in the IaC can be identified \u2014 these can be likened to compile-time errors in compiled languages like C or Rust. This means developers do not have to wait for a failed deployment to catch such errors, saving time.\n\nThis approach differs from Terraform's API-based method, where infrastructure is provisioned in a cloud provider agnostic language, and plugins are used to interact with Cloud and SaaS providers. The benefit of this approach is that migration between cloud providers does not require a complete rewrite of the IaC. However, with serverless architectures, cloud-provider-specific constructs are likely already being utilized. Terraform has also released 'CDK for Terraform', which is an AWS CDK equivalent that transpiles to Terraform rather than CloudFormation. However, this is currently less mature than AWS CDK.\n\nCDK additionally provides libraries to write assertion and validation tests with familiar testing tools such as jest, further moving the point at which errors are caught to earlier in the development cycle. CDK also adopts the paradigm of encapsulation with its 'constructs'. This allows specific resource provisioning to be wrapped into a simple, reusable package that can be used elsewhere.\n```"}, {"role": "assistant", "content": "This makes the development experience more approachable and rapid, as many errors can be caught by static checks performed by the editor. The code is also more easily readable and comprehensible to developers written in these familiar languages than in YAML.\n\nThis CDK code is transpiled into CloudFormation templates, AWS\u2019s YAML or JSON IaC format. During this \u2018synth\u2019 process, further errors in the IaC can be caught \u2014 I like to think of these like compile-time errors in compiled languages like C or Rust. This means we don\u2019t have to wait for a failed deployment to catch such errors, saving the developers time.\n## CDK deploy process\nThis is in contrast to Terraform\u2019s API-based approach, where infrastructure is provisioned in a cloud provider agnostic language, and then plugins are used to interact with Cloud and SaaS providers. The benefit of this approach is migration between cloud providers does not require a complete rewrite of the IaC. However, with serverless architectures we\u2019re likely already buying into cloud-provider-specific constructs. Terraform have also released \u2018CDK for Terraform\u2019, which is an AWS CDK equivalent that transpiles to Terraform rather than CloudFormation. However, this is currently less mature than AWS CDK.\n\nCDK additionally provides libraries to write assertion and validation tests with familiar testing tools such as jest, further moving the point at which errors are caught to earlier in the development cycle. CDK also adopts the paradigm of encapsulation with its \u2018constructs\u2019. This allows you to wrap specific resource provisioning into a simple, reusable package that can be used again elsewhere."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Serverless architectures often involve adopting cloud-provider-specific constructs. Terraform has introduced 'CDK for Terraform', which is an equivalent to AWS CDK that generates Terraform configurations instead of CloudFormation templates. However, 'CDK for Terraform' is currently less mature compared to AWS CDK.\n\nAWS CDK offers libraries for writing assertion and validation tests using familiar testing tools like Jest, allowing errors to be caught earlier in the development cycle. CDK also follows the encapsulation paradigm with its 'constructs', enabling developers to package specific resource provisioning into reusable components for use elsewhere.\n\nTo install AWS CDK, the AWS CLI must be installed and configured with an appropriate account. After meeting this prerequisite, CDK can be installed globally using 'npm i -g cdk'. A new CDK project can be created from scratch using CDK's TypeScript template by executing 'cdk init app --language typescript' in a new directory.\n\nThe main files in a CDK project include 'lib/cdk-demo-stack.ts', which defines a CDK stack corresponding to a CloudFormation template for provisioning resources, and 'bin/cdk-demo.ts', which defines the CDK app. An app can contain multiple stacks, allowing for modular deployment of services across separate stacks.\n\nThe code snippet 'env: { region: 'eu-west-1' }' is likely part of the CDK configuration, specifying the AWS region for resource deployment.\n```"}, {"role": "assistant", "content": "However, with serverless architectures we\u2019re likely already buying into cloud-provider-specific constructs. Terraform have also released \u2018CDK for Terraform\u2019, which is an AWS CDK equivalent that transpiles to Terraform rather than CloudFormation. However, this is currently less mature than AWS CDK.\n\nCDK additionally provides libraries to write assertion and validation tests with familiar testing tools such as jest, further moving the point at which errors are caught to earlier in the development cycle. CDK also adopts the paradigm of encapsulation with its \u2018constructs\u2019. This allows you to wrap specific resource provisioning into a simple, reusable package that can be used again elsewhere.\n## Installation\nA pre-requisite is to have the AWS CLI installed and configured with an appropriate account. Once you\u2019ve done this, you can install CDK using `npm i -g cdk` - this will install it globally on your machine. We\u2019re going to start a new project from scratch using CDK\u2019s TypeScript template. Create a new `cdk-demo` directory and execute `cdk init app --language typescript` inside it.\n## The basics\nThere are a couple main files to go over here. Firstly, `lib/cdk-demo-stack.ts` creates our first CDK stack. A stack corresponds to a CloudFormation template, which provisions the resources needed for our applications and services.\n\nNext, we have `bin/cdk-demo.ts`, which defines our CDK app. An app can contain multiple stacks, and modularising our services into stacks decouples their deployments. This means if we make changes to only one stack we only need to worry about redeploying this stack.\n\n```typescript\nenv: { region: 'eu-west-1' }\n```"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```There are two primary files to discuss. First, `lib/cdk-demo-stack.ts` creates the initial CDK stack. A stack corresponds to a CloudFormation template, which provisions the required resources for applications and services.\n\nNext, `bin/cdk-demo.ts` defines the CDK app. An app can contain multiple stacks, and separating services into stacks decouples their deployments. This means if changes are made to only one stack, only that stack needs to be redeployed.\n\nThe code snippet `env: { region: 'eu-west-1' }` is provided.\n\nTo create a Lambda, a `lambdas` directory is made to store the code. Inside this, `getLunchSpots.ts` is created containing a handler function that returns a 200 response with a serialized lunchSpots object.\n\nA Lambda resource is then created in the stack file directly from the TypeScript handler function. The code snippet `// This creates a Node.js Lambda function with the name 'getLunchSpots', using the handler function found in lambdas/getLunchSpots.ts` is provided.\n\nTo deploy the changes, `cdk deploy` is run. A prompt may ask to confirm the deployment. The terminal output will include an 'Outputs' section with an entry 'CdkDemoStack.sohoLunchSpotsEndpoint...', which is the endpoint of the API Gateway API. Making a GET request to this URL + /lunch-spots will return the JSON object specified in the Lambda function.\n```"}, {"role": "assistant", "content": "## The basics\nThere are a couple main files to go over here. Firstly, `lib/cdk-demo-stack.ts` creates our first CDK stack. A stack corresponds to a CloudFormation template, which provisions the resources needed for our applications and services.\n\nNext, we have `bin/cdk-demo.ts`, which defines our CDK app. An app can contain multiple stacks, and modularising our services into stacks decouples their deployments. This means if we make changes to only one stack we only need to worry about redeploying this stack.\n\n```typescript\nenv: { region: 'eu-west-1' }\n```\n## Creating a Lambda\nLet's first create a `lambdas` directory to store our code. Inside this, we'll create `getLunchSpots.ts` containing the following:\n\n```typescript\n// This creates a simple handler that returns a 200 response with the serialised lunchSpots object\n```\n## Adding to the Stack\nNow we\u2019ll create a Lambda resource in our stack directly from our TypeScript handler function. Add the following code to the stack file:\n\n```typescript\n// This creates a Node.js Lambda function with the name \u2018getLunchSpots\u2019, using the handler function found in lambdas/getLunchSpots.ts\n```\n## Deployment\nTo deploy our changes, we can simply run `cdk deploy`. You may get a prompt asking to enter 'y' to confirm deployment of the changes. Near the end of the deployment script's terminal output you should see an 'Outputs' section with an entry 'CdkDemoStack.sohoLunchSpotsEndpoint...'. This is the endpoint of our API Gateway API. If we make a GET request to this url + /lunch-spots, we'll see the JSON object we specified in our Lambda function returned."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```LocalStack: A Brief Overview and Its Benefits for Developers \n\nLocalStack emulates cloud services, particularly those from AWS, to run exclusively on a personal laptop without the need for connection to any remote cloud sources.\n\nThe use of LocalStack may be beneficial for various categories of users:\n\n1. Beginning users in need of practice functionality for AWS utilities but lack the required credit card for AWS registration.\n2. Educational users who wish to gain practical knowledge of AWS services without exposure to financial costs.\n3. Practising professionals who want to examine or trail infrastructure configurations offline or on personal equipment, without setting up a parallel cloud environment for testing and smoothly transferring to the main production environment of AWS once finalisation and optimisation are achieved.\n\nThis last scenario especially presents a clear benefit of LocalStack, as one could unknowingly face considerable financial costs from improve their skills by setting up a separate AWS environment without understanding cost plans or setting up budget alerts.\n\nLocalStack's Emulation of AWS Services \n\nLocalStack at present, replicates over 60 AWS cloud services, most of which are free of charge. An illustrative example would be creating an S3 bucket and simulating the deployment of a rudimentary webpage to demonstrate the (italics)functional similarity(end italics) between an action performed on the AWS S3 service and its emulation within LocalStack.\n```"}, {"role": "assistant", "content": "# Run AWS on Your Laptop: Introduction to LocalStack\n## What is LocalStack\n\nLocalStack is a cloud service emulator that runs AWS services solely on your laptop without connecting to a remote cloud provider.\n### How can it help us as a developer?\n\nWhether you are,\n\n1. A beginner who is looking to practice your AWS skills but don\u2019t have access to a credit card which is required upon AWS registration.\n2. A student, who wants to gain hands-on experience with AWS services without spending any money.\n3. A professional who wants to troubleshoot or test your infrastructure configurations offline on your machine, without setting up a separate cloud environment for testing, and then seamlessly transition to your main AWS production environment once everything is ready and optimized.\n\nLocalStack has got you covered. Because the last thing you want to do is set up an AWS environment to improve your skills, only to accidentally shoot yourself in the foot with a ridiculous amount of money because you didn\u2019t know about cost plans or didn\u2019t setup budget alerts.\n## AWS services offered by LocalStack\n\nThere are currently more than 60 emulated AWS cloud services (and most of them free) provided by LocalStack. As an introduction we will create an S3 bucket and emulate the deployment of a static website and witness that it works just as if it were deployed on AWS S3."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```LocalStack is a tool that allows users to emulate AWS services locally on their machines. It provides a convenient way for students to gain hands-on experience with AWS services without incurring any costs. Additionally, professionals can utilize LocalStack to troubleshoot or test their infrastructure configurations offline before seamlessly transitioning to the main AWS production environment once everything is optimized.\n\nLocalStack can be installed on a personal computer through two methods: using PIP or running it with Docker. For this article, the first option of installing LocalStack using PIP was chosen. The AWS Command Line Interface (CLI) was also installed to interact with the emulated services.\n\nAfter installing the AWS CLI, it was configured with test credentials and the default region set to us-east-1. LocalStack was then started on the localhost port 4566 using the command 'localstack start -d'.\n\nTo interact with LocalStack and emulate AWS services, the AWS CLI or SDK needs to be configured to point to the LocalStack endpoint URL. Alternatively, a tool called 'awslocal' can be installed, which automatically configures the CLI to use the LocalStack endpoint URL, eliminating the need to manually specify the '--endpoint-url' option for every command.\n```"}, {"role": "assistant", "content": "A student, who wants to gain hands-on experience with AWS services without spending any money.\n3. A professional who wants to troubleshoot or test your infrastructure configurations offline on your machine, without setting up a separate cloud environment for testing, and then seamlessly transition to your main AWS production environment once everything is ready and optimized.\n\nLocalStack has got you covered. Because the last thing you want to do is set up an AWS environment to improve your skills, only to accidentally shoot yourself in the foot with a ridiculous amount of money because you didn\u2019t know about cost plans or didn\u2019t setup budget alerts.\n### Installation\n\nThere are two ways to run LocalStack on your pc:\n\n1. **Using PIP**\n\n```shell\npython -m virtualenv venv\nvenv/Scripts/activate\npip install localstack\n```\n\n2. **Starting LocalStack with Docker**\n\n```shell\ndocker run --rm -it -p 4566:4566 -p 4510-4559:4510-4559 localstack/localstack\n```\n\nFor this article, I will use the first choice and install localstack using pip. We also need to install AWS Command Line Interface so that we can interact with the underlying emulated services.\n\nTo download the AWS Command Line Interface go to AWS CLI Docs and find AWS CLI MSI installer for Windows, then run the setup. After the installation is complete, then let's proceed and configure the CLI using the commands below.\n\n```shell\naws configure \nAWS Access Key ID: test\nAWS Secret Access Key: test\nDefault region name: us-east-1\nDefault output format: [None]\n```\n\nThe next step is to run LocalStack. So open a terminal and run the following:\n\n```shell\nlocalstack start -d\n```\n\nThis will start LocalStack on localhost port 4566.\n\nWhen interacting with LocalStack to emulate AWS services it\u2019s important to configure your AWS CLI or SDK to point to the LocalStack endpoint URL. This allows you to interact with LocalStack easily without having to specify the `--endpoint-url` option every time you run a command.\n\nAnother option is installing a tool called \u201cawslocal\u201d which is a wrapper around the AWS CLI for LocalStack. It automatically configures the CLI to use the LocalStack endpoint URL, saving you from the manual step of specifying the `--endpoint-url` option."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Please initiate LocalStack via a terminal. The command below will accomplish this on localhost port 4566.\n\n```shell\nlocalstack start -d\n```\n\nCoordination with LocalStack will be necessary when simulating AWS services. Accessing LocalStack can be eased by adjusting your AWS CLI or SDK to aim for the LocalStack endpoint URL. This renders the need to define the `--endpoint-url` when issuing commands unnecessary. \n\nAn alternative approach would be to implement a tool named \"awslocal\". This tool serves as a shell for the AWS CLI compatible with LocalStack. Usage of this tool results in the automatic configuration of the CLI to operate with the LocalStack endpoint URL, reducing the need for manual input of the `--endpoint-url`.\n\nHere are some examples of LocalStack usage:\n### Example 1 - Creation of S3 Bucket \n\n```shell\naws --endpoint-url=http://localhost:4566 s3api create-bucket --bucket testbucket\n```\n### Example 2 - Hosting a Static Website \n\nIf it becomes necessary to act as a host for a static website through the S3 bucket which has been previously created, one must develop two html files: `index.html` and `error.html`. Store both these files in a directory labeled `website`. \n\nThe `index.html` will act as the principal access gate to your website, establishing its content and structural organization. The `error.html` will be employed to generate personalized error pages that emerge during HTTP errors such as 404 (Not found) or 403 (Forbidden). \n\nIt is possible to assign a bucket policy and grant unrestricted access to its entirety. Thus, establish a file bearing the name `bucket_policy.json` and insert the code provided below.\n\n```json\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"PublicReadGetObject\",\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": \"s3:GetObject\",\n            \"Resource\": \"arn:aws:s3:::testbucket/*\"\n        }\n    ]\n}\n```\n\nSynchronize the website directory with the S3 bucket through the application of the command given below.\n\n```shell\naws --endpoint-url=http://localhost:4566 s3 sync .\\website\\ s3://testbucket\n```\n\nIn conclusion, provisioning of the static website hosting through the bucket has been completed and configurations of the index and error documents made. \n\nThe functional, hosted website is now available on the emulated S3 bucket. This site can be accessed via the following URL: `http://testbucket.s3website.localhost.localstack.cloud:4566/`.\n```"}, {"role": "assistant", "content": "So open a terminal and run the following:\n\n```shell\nlocalstack start -d\n```\n\nThis will start LocalStack on localhost port 4566.\n\nWhen interacting with LocalStack to emulate AWS services it\u2019s important to configure your AWS CLI or SDK to point to the LocalStack endpoint URL. This allows you to interact with LocalStack easily without having to specify the `--endpoint-url` option every time you run a command.\n\nAnother option is installing a tool called \u201cawslocal\u201d which is a wrapper around the AWS CLI for LocalStack. It automatically configures the CLI to use the LocalStack endpoint URL, saving you from the manual step of specifying the `--endpoint-url` option.\n### Example 1 - Creating S3 bucket\n\n```shell\naws --endpoint-url=http://localhost:4566 s3api create-bucket --bucket testbucket\n```\n### Example 2 - Host a static website\n\nIn order to host a static website inside the S3 bucket we just created above, we need to prepare two html files: `index.html` and `error.html` and put them in a folder called `website`.\n\nThe `index.html` file serves as the main entry point for your website, representing the content and structure of your homepage. The `error.html` file is used to create custom error pages that get displayed during HTTP errors like 404 (Not Found) or 403 (Forbidden).\n\nWe also can attach a bucket policy and allow public access to all of its contents. So create a file named `bucket_policy.json` in the root directory and add the following code:\n\n```json\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"PublicReadGetObject\",\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": \"s3:GetObject\",\n            \"Resource\": \"arn:aws:s3:::testbucket/*\"\n        }\n    ]\n}\n```\n\nSync the website directory to the S3 bucket using the command below:\n\n```shell\naws --endpoint-url=http://localhost:4566 s3 sync .\\website\\ s3://testbucket\n```\n\nFinally, we enable static website hosting on the bucket and configure the index and error documents.\n\nOur website is now hosted on the emulated S3 bucket. So let's browse to `http://testbucket.s3website.localhost.localstack.cloud:4566/` and see what it looks like."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```DynamoDB is an AWS service providing a fully managed NoSQL database renowned for its high scalability and minimal latency. While it is accessible publicly, user data is secured via AWS IAM. The principles of zero-trust networking stress the critical need for caller authentication rather than relying heavily on trust based solely on network perimeters. Although security of data on DynamoDB does not strictly need network security, augmenting IAM authentication and authorization with network security has its advantages, and at times, becomes a requirement for regulatory compliance or to meet chief security officer's demands. This discourse details how to restrict DynamoDB table access exclusively to a VPC.\n### Is it possible to simply use a VPC endpoint?\n\nA VPC endpoint for DynamoDB facilitates a connection between your VPC and DynamoDB, making it possible for traffic to move from your VPC to DynamoDB even without a public IP address. It is generally considered secure since it maintains distance from the public internet. However, as far as the present knowledge suggests, the traffic between a VPC and DynamoDB (via a NAT Gateway) unlikely goes via the public internet infrastructure, even though a public IP address is required so that DynamoDB knows the destination for the response. What is significant is that a VPC endpoint does not prevent access to the DynamoDB table from outside your VPC.\n```"}, {"role": "assistant", "content": "# How to create Private DynamoDB tables accessible only within a VPC\n\nDynamoDB is a fully managed NoSQL database service known for its low latency and high scalability. Like most other AWS services, it\u2019s protected by AWS IAM. So, despite being a publically accessible service, your data is secure. Conversely, zero-trust networking tells us we should always authenticate the caller and shouldn\u2019t trust someone just because they\u2019re in a trusted network perimeter. All this is to say that you don\u2019t need network security to keep your DynamoDB data safe. However, adding network security on top of IAM authentication and authorization is not a bad thing. Sometimes it\u2019s even necessary to meet regulatory requirements or to keep your CSO happy! This has come up numerous times in my consulting work. In this post, let me show you how to make a DynamoDB table accessible only from within a VPC.\n## Can\u2019t you just use a VPC endpoint?\n\nYou can create a VPC endpoint for DynamoDB to create a tunnel between your VPC and DynamoDB. This allows traffic to go from your VPC to DynamoDB without needing a public IP address. This is often perceived to be more secure \u201cbecause it doesn\u2019t go out to the public internet\u201d. But as far as I know, traffic from a VPC to DynamoDB (through a NAT Gateway) would never traverse over public internet infrastructure anyway. It\u2019s just that you need a public IP address so DynamoDB knows where to send the response. More importantly, a VPC endpoint doesn\u2019t stop other people from accessing the DynamoDB table from outside your VPC."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```- You can create a VPC endpoint for DynamoDB to connect your VPC to DynamoDB without needing a public IP.\n- Traffic from a VPC to DynamoDB (through a NAT Gateway) doesn't traverse over public internet infrastructure anyway. \n- A VPC endpoint doesn't stop others from accessing the DynamoDB table from outside your VPC.\n\n- DynamoDB doesn't support resource policies. \n- You can use the aws:SourceVpc or aws:SourceVpce conditions in IAM policies to ensure access is only from a VPC or VPC endpoint.\n- But it's difficult to ensure everyone follows this requirement.\n\n- An easy and reliable way is to use Service Control Policies (SCPs) with AWS Organizations.\n- SCPs can enforce rules across your AWS accounts and resources.\n- You can create an SCP that requires the aws:SourceVpc or aws:SourceVpce condition for DynamoDB access.\n- This way, feature teams can create their own IAM roles, but they must comply with your VPC requirements.\n```"}, {"role": "assistant", "content": "## Can\u2019t you just use a VPC endpoint?\n\nYou can create a VPC endpoint for DynamoDB to create a tunnel between your VPC and DynamoDB. This allows traffic to go from your VPC to DynamoDB without needing a public IP address. This is often perceived to be more secure \u201cbecause it doesn\u2019t go out to the public internet\u201d. But as far as I know, traffic from a VPC to DynamoDB (through a NAT Gateway) would never traverse over public internet infrastructure anyway. It\u2019s just that you need a public IP address so DynamoDB knows where to send the response. More importantly, a VPC endpoint doesn\u2019t stop other people from accessing the DynamoDB table from outside your VPC.\n## No resource policy\n\nUsually, this can be done using resource policies. Unfortunately, DynamoDB doesn\u2019t support resource policies. Instead, you can use the aws:SourceVpc or aws:SourceVpce conditions in your IAM policies. You can use these conditions to ensure the IAM user/role can only access DynamoDB from a VPC, or through a VPC endpoint. But how we can ensure that everyone follows this requirement? As a member of the platform team, I don\u2019t want to be a gatekeeper. I want the feature teams to create their own IAM roles for their Lambda functions or containers. But how can I make sure they don\u2019t violate our compliance requirements? There are a few ways to do this. The easiest and most reliable is to use Service Control Policies (SCPs) with AWS Organizations."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```DynamoDB does not have the capability to support resource policies. However, one can employ either aws:SourceVpc or aws:SourceVpce stipulations in IAM policies as a workaround. These conditions restrict IAM user/role to access DynamoDB from only a Virtual Private Cloud (VPC) or via a Virtual Private Cloud endpoint, thus fulfilling the requirement. Ensuring all stakeholders adhere to the necessary IAM principles can be complex. Ideally, teams should be able to establish their individual IAM roles for their corresponding Lambda features or containers. The challenge is preserving the compliance guidelines. Possible solutions include adopting Service Control Policies (SCPs) through AWS organizations.\n\n## Service Control Policies (SCPs)\nSCPs facilitate the prevention of certain actions in member accounts within an AWS organization, apart from the master account. For instance, specific activities can be prohibited in all sectors except us-east-1 or eu-west-1, as in these locations the application is positioned. Creation of EC2 examples can also be restricted. These controls are widely invoked as protective measures against cryptojacking. An illustration would be the use of SCP to repudiate any requests made towards DynamoDB that are not sourced from within a selected Virtual Private Cloud.\n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"DenyDynamoDBOutsideVPC\",\n      \"Effect\": \"Deny\",\n      \"Action\": \"dynamodb:*\",\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringNotEquals\": {\n          \"aws:SourceVpc\": \"vpc-xxxxxxxxx\"\n        }\n      }\n    }\n  ]\n}\n```\n\nAs every AWS account has dedicated VPC, the usage of an account-specific SCP is suggestive. In spite of this, the SCP may cause a hindrance to the development course since all actions against DynamoDB tables need to be executed from within the chosen VPC leaving the DynamoDB console inaccessible.\n\nA better arrangement may be to disconnect the SCP from the development accounts providing developers ample liberty and ensuring the sustenance of adherence in production.\n```"}, {"role": "assistant", "content": "Unfortunately, DynamoDB doesn\u2019t support resource policies. Instead, you can use the aws:SourceVpc or aws:SourceVpce conditions in your IAM policies. You can use these conditions to ensure the IAM user/role can only access DynamoDB from a VPC, or through a VPC endpoint. But how we can ensure that everyone follows this requirement? As a member of the platform team, I don\u2019t want to be a gatekeeper. I want the feature teams to create their own IAM roles for their Lambda functions or containers. But how can I make sure they don\u2019t violate our compliance requirements? There are a few ways to do this. The easiest and most reliable is to use Service Control Policies (SCPs) with AWS Organizations.\n## Service Control Policies (SCPs)\n\nSCPs let you apply broad strokes to deny certain actions in the member accounts (but NOT the master account) of an AWS organization. For example, you can use SCPs to: Stop any actions in regions other than us-east-1 or eu-west-1, where your application resides. Stop anyone from creating EC2 instances. Both are common SCPs people use to protect against cryptojacking. In our case, we can use a SCP like this to reject any requests to DynamoDB, unless they originate from within the specified VPC.\n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"DenyDynamoDBOutsideVPC\",\n      \"Effect\": \"Deny\",\n      \"Action\": \"dynamodb:*\",\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringNotEquals\": {\n          \"aws:SourceVpc\": \"vpc-xxxxxxxxx\"\n        }\n      }\n    }\n  ]\n}\n```\n\nEvery AWS account would have its own VPC(s), so you will likely need an account-specific SCP. This SCP can also create significant friction to development. Every action against DynamoDB tables, including the initial deployment, has to be performed from within the designated VPC. And you won\u2019t even be able to use the DynamoDB console!\n\nA good compromise is to leave the SCP off the development accounts. That way, developers are not hamstrung by this strict SCP and you can still enforce compliance in production."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Queues and workers are essential components in serverless architectures for handling asynchronous tasks. The combination of Amazon Simple Queue Service (SQS) and AWS Lambda provides a scalable and cost-effective solution for processing jobs in a serverless environment.\n\nThe basic approach involves creating an SQS queue to receive and store messages (jobs) and an AWS Lambda function to process these messages. Lambda integrates seamlessly with SQS, automatically invoking the configured function when a new message is available in the queue. This setup eliminates the need for polling and allows for automatic scaling based on the incoming workload.\n\nWhile this approach is straightforward, it lacks proper error handling mechanisms. By default, SQS retries failed jobs indefinitely, which can lead to resource wastage and unnecessary costs, especially if the code contains bugs or errors. To address this issue, it is essential to set up a dead-letter queue and limit the number of retries.\n\nA dead-letter queue is a separate queue designed to store messages that could not be processed successfully after a specified number of retries. This allows for easier monitoring and troubleshooting of failed jobs. Additionally, several other configurations need to be fine-tuned, such as setting the visibility timeout, configuring Lambda's maximum concurrency, enabling message batching, and setting up alarms to notify the team when failed messages are present in the dead-letter queue.\n\nTo deploy production-ready queues and workers on AWS, a certain level of CloudFormation configuration is required. This includes defining the necessary resources, such as the SQS queues, Lambda functions, and their respective configurations, as well as setting up the appropriate integrations and error handling mechanisms.\n```"}, {"role": "assistant", "content": "# Serverless queues and workers \u2014 Designing Lift\n\nThis article is part of a series on Lift, an open-source project that simplifies deploying serverless applications.\n\nAs introduced in previous articles, Lift is a Serverless Framework plugin that simplifies deploying serverless applications.\n\nAs we approach the first beta planned for May, let\u2019s talk about queues and workers:\n- How to deploy production-ready queues and workers on AWS today.\n- How we plan to simplify that with Lift.\n## The naive approach\n\nWhen going serverless, SQS + AWS Lambda is an iconic duo:\n- SQS is a queue service that is entirely managed by AWS, scales automatically, and bills by the usage.\n- AWS Lambda runs our code and is entirely managed by AWS, scales automatically, and bills by the usage.\n\nThe best part: Lambda integrates with SQS natively. We can write \u201cqueue workers\u201d on Lambda without having to poll messages from SQS: instead, our code (the worker) is automatically invoked when a new message (aka job) is pushed into SQS.\n\nTo deploy queues and workers with the Serverless Framework, we need a bit of CloudFormation:\n\n```yaml# Example configuration here\n```\n### Problems with the naive approach\n\nThe biggest missing piece here is error handling.\n\nBy default, SQS retries failed jobs indefinitely. Imagine your code has a bug (let\u2019s pretend that can happen): the job will be retried over and over for days, possibly costing a lot of money and wasting resources.\n\nTo deal with errors, we need to set up a \u201cdead letter queue\u201d (a queue that stores failed messages) and limit the number of retries.\n\nOn top of that, there are many details that need to be configured, for example:\n- We should configure the dead letter queue to store failed messages for 14 days (the maximum).\n- We should set up an alarm that sends our team an email whenever there are failed messages in the dead letter queue.\n- Many other settings need to be fine-tuned: the SQS \u201cvisibility timeout\u201d, the Lambda max concurrency, message batching, etc."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```- Your code may have a bug that causes a job to be retried repeatedly for days, potentially costing money and wasting resources.\n- To handle errors, you need to set up a \"dead letter queue\" (a queue that stores failed messages) and limit the number of retries.\n- You'll need to configure various details, such as:\n    - Setting the dead letter queue to store failed messages for 14 days (the maximum).\n    - Setting up an alarm to send an email when there are failed messages in the dead letter queue.\n    - Adjusting other settings like the SQS visibility timeout, Lambda max concurrency, and message batching.\n- The text includes a sample serverless.yml configuration with best practices, such as setting the worker function timeout, reserved concurrency, SQS event source, batch size, queue visibility timeout, message retention period, and redrive policy.\n- Adding an alarm for failed messages would require around 30 more lines of YAML configuration.\n```"}, {"role": "assistant", "content": "Imagine your code has a bug (let\u2019s pretend that can happen): the job will be retried over and over for days, possibly costing a lot of money and wasting resources.\n\nTo deal with errors, we need to set up a \u201cdead letter queue\u201d (a queue that stores failed messages) and limit the number of retries.\n\nOn top of that, there are many details that need to be configured, for example:\n- We should configure the dead letter queue to store failed messages for 14 days (the maximum).\n- We should set up an alarm that sends our team an email whenever there are failed messages in the dead letter queue.\n- Many other settings need to be fine-tuned: the SQS \u201cvisibility timeout\u201d, the Lambda max concurrency, message batching, etc.\n## A production-ready approach\n\nHere is a preview of a `serverless.yml` configuration that includes those best practices:\n\n```yaml# serverless.yml\nfunctions:\n  worker:\n    handler: worker.handler\n    timeout: 10 # seconds\n    reservedConcurrency: 10\n    events:\n      - sqs:\n          arn: !GetAtt ReportGenerationQueue.Arn\n          batchSize: 1\nresources:\n  Resources:\n    ReportGenerationQueue:\n      Type: AWS::SQS::Queue\n      Properties:\n        QueueName: !Sub '${AWS::StackName}-myqueue'\n        VisibilityTimeout: 60\n        MessageRetentionPeriod: 172800\n        RedrivePolicy:\n          maxReceiveCount: 5\n          deadLetterTargetArn: !GetAtt DeadLetterQueue.Arn\n    DeadLetterQueue:\n      Type: AWS::SQS::Queue\n      Properties:\n        MessageRetentionPeriod: 1209600\n```\n\nIf we wanted to add an alarm for failed messages, we would need 30 more lines of YAML."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```A Step Function consists of different types of states, including Task, Choice, Fail/Succeed, Pass, Wait, Parallel, and Map states. Each state performs a specific operation within the state machine.\n\nTesting Step Functions is important to ensure proper configuration and prevent future issues. Both unit testing of individual states and integration testing of the overall state machine are recommended, with integration tests simulating the real production environment.\n\nUnit tests focus on testing individual components, while integration tests verify the interaction between various system components. Both types of tests are necessary. Testing individual states is relatively straightforward, while integration tests are more complex but essential.\n```"}, {"role": "assistant", "content": "# Integration Testing Step Functions: Using sls-test-tools\n## The states which make up a Step Function\nA Step Function is a state machine, where states have types, namely:\n- Task state: for performing an operation\n- Choice state: for choosing which branch of the state machine to execute next\n- Fail/Succeed state: for ending the execution of a state machine\n- Pass state: for passing its input to output, or injecting fixed data\n- Wait state: for imposing a delay into the execution of the state machine\n- Parallel state: for creating parallel branches of execution in the state machine\n- Map state: for performing the same state action for each item in a list\n## Should we test Step Functions?\nAs with any AWS infrastructure, we need to be able to test it, so that we know we\u2019ve configured it correctly and so that it can't be broken in the future. It's important to test the logic in the individual states, as well as the overall state machine. Testing on the real infrastructure is recommended to best simulate the real production environment.\n## Unit vs Integration Testing\nUnit tests focus on testing individual pieces of code, whereas integration tests check that various pieces of our system are interacting correctly with each other. It is important to have both! Testing individual states is relatively simple, while integration tests are slightly more complex but crucial."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```## Contrast between Unit and Integration Testing\nThe primary focus of unit tests is on evaluating single units of code in isolation, while integration tests are designed to see if different components of the system are interacting as expected. It's critical to run both kinds of tests. Individual state tests can be relatively straightforward, but integration tests, although slightly complicated, are essential for overall system stability. \n\n## The Importance of Step Function Testing\nTesting is an essential part of any AWS infrastructure management to ensure proper configuration and to maintain its robustness against possible future issues. Alongside testing the logic of individual states, the overall state machine testing also contributes to system stability. Testing these scenarios using real infrastructure offers a practical simulation comparable to an actual production environment.\n\n## Testing Step Functions Assertions with sls-test-tools\nThe subsequent code samples will illustrate testing for a Step Function termed 'TestAddAndDouble', developed with the capacity to handle two task states; one for adding two numbers and another for doubling the result. \n\n### Ensuring Correct Execution Status of State Machine \nInitially, check the test that transitions to a 'Succeeded' status upon receipt of a valid input. This can be achieved by applying the newly introduced `toHaveCompletedExecutionWithStatus()` assertion. It lets you provide the state machine's executed name and the envisaged status for the execution. We then inspect the latest execution status of the state machine to check if it actually matches the expected one.\n```"}, {"role": "assistant", "content": "## Unit vs Integration Testing\nUnit tests focus on testing individual pieces of code, whereas integration tests check that various pieces of our system are interacting correctly with each other. It is important to have both! Testing individual states is relatively simple, while integration tests are slightly more complex but crucial.\n## Should we test Step Functions?\nAs with any AWS infrastructure, we need to be able to test it, so that we know we\u2019ve configured it correctly and so that it can't be broken in the future. It's important to test the logic in the individual states, as well as the overall state machine. Testing on the real infrastructure is recommended to best simulate the real production environment.\n## Introducing Step Functions assertions with sls-test-tools\nThe below code samples will demonstrate how to test a step function called 'TestAddAndDouble' which contains two task states, one which sums two numbers, and another which doubles the number.\n### Assert that the state machine has the correct execution status\nFirst let\u2019s test that we get to a Succeeded status when we pass a valid input. To do this we can use the new `toHaveCompletedExecutionWithStatus()` assertion. This allows us to type in the name of the state machine we\u2019ve executed, as well as the expected status for the execution. It then checks that the most recent execution of the state machine has the expected status."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```### Verify the correct execution status of the state machine\nLet's begin by testing whether a valid input results in a Succeeded status. This can be done using the `toHaveCompletedExecutionWithStatus()` assertion. You would need to indicate the state machine that's been executed and the anticipated status. The function then verifies if the most recent execution of the state machine conforms to the expected status.\n## Exploring step function assertions with sls-test-tools\nIn the following code samples, we will examine how to evaluate a step function named 'TestAddAndDouble'. This function incorporates two task states: one to sum two numbers and another to double the result.\n### Let's ensure the output corresponds to a valid input\nFor instance, if we provide inputs {3, 6}, the anticipated output is 18. Once again, we can use an assertion, the `toMatchStateMachineOutput` assertion this time. For this operation, you need to supply the state machine that's been executed and the projected output. The function then verifies if the most recent execution of the state machine generated the expected output. Below is an instance of using this assertion:\n\n```javascript\n// Helper: Execute the Step Function until its completed\n```\n\nIt should be reiterated here that we depended on having the step function executed. Instead of having to execute a manual step or making a complicated API call, we've also included a helper function making execution simpler and allowing for waiting until the execution finished. Here's how you can make a call to the helper:\n```"}, {"role": "assistant", "content": "### Assert that the state machine has the correct execution status\nFirst let\u2019s test that we get to a Succeeded status when we pass a valid input. To do this we can use the new `toHaveCompletedExecutionWithStatus()` assertion. This allows us to type in the name of the state machine we\u2019ve executed, as well as the expected status for the execution. It then checks that the most recent execution of the state machine has the expected status.\n## Introducing Step Functions assertions with sls-test-tools\nThe below code samples will demonstrate how to test a step function called 'TestAddAndDouble' which contains two task states, one which sums two numbers, and another which doubles the number.\n### Now let\u2019s check we get the right output given a valid input\nFor example, if we enter {3, 6} we expect to get 18 as the output. We can once again use a new assertion, the `toMatchStateMachineOutput` assertion. This allows us to type in the name of the state machine we\u2019ve executed, as well as the expected output for this execution, and checks that the most recent execution of the state machine produced the expected output. Here is an example of the usage of this assertion:\n\n```javascript\n// Helper: Execute the Step Function until its completed\n```\n\nYou\u2019ll have noticed above that we had to rely on the step function having been executed. Rather than require a manual step or complex call to the API we\u2019ve also bundled in a helper function to simplify execution and wait for execution to complete. Here is an example of the call to the helper:"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The text discusses the concept of an Anti-Corruption Layer (ACL) in the context of building large distributed systems that interact with external systems. It presents a scenario where a company needs to modify multiple sections of its software to accommodate a new third-party provider due to incompatible protocols. The ACL is proposed as a solution to address this issue and facilitate easier transitions in the future.\n\nThe ACL implements the principles of Domain Driven Design (DDD), a software development approach that models software based on the real-world business domain it serves. DDD emphasizes collaboration between technical and domain experts to ensure the software accurately reflects and evolves with the domain. It advocates for a ubiquitous language among stakeholders and organizes the system into bounded contexts, also known as separate services in a distributed system context.\n\nThe ACL follows DDD principles by providing a separation of these services, allowing for effective structuring of business domains and protecting the system's design and language from third-party influence. When executed correctly, the ACL can prevent vendor lock-in and provide better visibility into the system's architecture.\n```"}, {"role": "assistant", "content": "# Building a Serverless Anti-Corruption Layer with CDK\n\nWhen building large distributed systems, interacting successfully with external systems can introduce large amounts of complexity into your system design. Consider a company\u2019s software system that interacts with a third-party provider. A plan is made to move to a new provider which is faster and cheaper. An issue is identified that the protocol to interact with the old third-party provider is not compatible with the new one. The company will need to modify multiple sections of its software to interact with this new provider. They want to implement a design principle to account for this and allow for easier transitions in future.\n\nA common solution is the Anti-Corruption Layer (ACL). An ACL implements the principles of Domain Driven Design (DDD). DDD is a software development approach that focuses on modelling software based on the real-world business domain it aims to serve. It emphasizes collaboration between technical and domain experts to ensure the software accurately reflects and evolves with the domain. DDD advocates for a ubiquitous language for all stakeholders and organizes the system into bounded contexts to manage complexity. These bounded contexts are called domains but are often referred to as separate services in the context of a distributed system.\n\nACL follows the principles of DDD by providing a separation of these services, allowing you to structure your business domains most effectively and ensuring that the design and language of your system are protected from 3rd party influence. In addition, when executed correctly an ACL can also prevent vendor lock-in, and provide easier visibility on your system\u2019s architecture."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```- DDD organizes a system into separate bounded contexts (domains) to manage complexity. In distributed systems, these are often called services.\n\n- ACL follows DDD principles by separating services, allowing effective structuring of business domains. It ensures the system's design and language are protected from external influence. \n\n- When executed correctly, ACL can prevent vendor lock-in and provide better visibility into the system's architecture.\n\nAn Anti-Corruption Layer (ACL) is a design pattern that transforms or translates information passing between systems. It's not just for communicating messages, but ensures misinterpreted data cannot corrupt the primary system. \n\n- ACL can be a reusable package or standalone service. \n- Acting as the communication layer between services, it increases decoupling, allowing independent scaling and portability.\n- External systems can be removed without major changes due to low coupling.\n- Decoupled services can be developed independently, increasing flexibility and reducing build time.\n```"}, {"role": "assistant", "content": "DDD advocates for a ubiquitous language for all stakeholders and organizes the system into bounded contexts to manage complexity. These bounded contexts are called domains but are often referred to as separate services in the context of a distributed system.\n\nACL follows the principles of DDD by providing a separation of these services, allowing you to structure your business domains most effectively and ensuring that the design and language of your system are protected from 3rd party influence. In addition, when executed correctly an ACL can also prevent vendor lock-in, and provide easier visibility on your system\u2019s architecture.\n## What is an ACL?\n\nAn Anti-Corruption Layer is a design pattern that provides the functionality to transform or translate information passing from one system to another. It is not purely a mechanism for communicating messages between two systems, instead, it provides guardrails to ensure that misinterpreted data cannot enter into and corrupt your primary system. An ACL can be designed as a package to be reused across other services, or as a self-standing service. By acting as the communication layer between services, an ACL separates and increases decoupling allowing for independent scaling and greater portability of each of the services. Due to its low coupling with the rest of the software system, an external system, like a third-party service above, can be torn out when needed without large changes. This brings the added benefit that decoupled services can now be developed independently of each other increasing developer flexibility and build time."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```An Access Control List (ACL) can be designed as a reusable package or a standalone service. By acting as the communication layer between services, an ACL separates and increases decoupling, allowing for independent scaling and greater portability of each service. Due to its low coupling with the rest of the software system, an external system, such as a third-party service, can be removed when necessary without significant changes. This provides the added benefit that decoupled services can now be developed independently of each other, increasing developer flexibility and build time.\n\nImplementing an ACL as a service offers the following advantages:\n\n- It promotes the abstraction of protocols required for communication with external services. Building a new layer to handle communication between services can be extremely complex. By encapsulating the ACL as a standalone service, an organization can dedicate a team of experts to build and maintain this service, protecting developers from other domains from needing to understand the intricacies of the ACL domain.\n- Logic used to communicate with external services can often result in a large codebase. Having this within a package can quickly become difficult to maintain, document, and use by consumers. A service simplifies this complexity by allowing the creation of standalone features, such as dedicated infrastructure and organization of logic, in a way that best fits the ACL rather than its consuming services.\n- The ACL service itself can leverage internal packages to reuse code across the service, and the ACL team will have context on these internal development tools. Ideally, having the ACL as a service, rather than a module or package within a service, will provide easier construction, understanding, and correct visibility of this tool at the levels required from those within and outside the ACL domain.\n```"}, {"role": "assistant", "content": "An ACL can be designed as a package to be reused across other services, or as a self-standing service. By acting as the communication layer between services, an ACL separates and increases decoupling allowing for independent scaling and greater portability of each of the services. Due to its low coupling with the rest of the software system, an external system, like a third-party service above, can be torn out when needed without large changes. This brings the added benefit that decoupled services can now be developed independently of each other increasing developer flexibility and build time.\n## ACL as a Service:\n\nAn ACL can be created as a package or a fully-fledged service, however, having it as a service will yield the following benefits:\n\n- It encourages the abstraction of protocols that are required to communicate with external services. Building a whole new layer to handle the communication between services can become extremely complex. By encapsulating the ACL as a standalone service, an organisation is likely to have a dedicated team of experts building and maintaining this service. This protects developers from other domains needing knowledge of the intricacies of the ACL domain.\n- Very often, logic used to communicate with external services can result in a large repository of code. Having this within a package can quickly become difficult to maintain, document and use by consumers. A service simplifies this complexity by allowing the creation of standalone features, such as dedicated infrastructure and organisation of logic, in a way that best fits the ACL rather than its consuming services.\n- In addition, the ACL service itself can leverage internal packages to reuse code across the service and the ACL team will have context on these internal development tools. Ideally, having the ACL as a service, not a module or package within a service, will provide easier construction, understanding, and correct visibility of this tool at the levels required from those in and outside the ACL domain."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Having an Anti-Corruption Layer (ACL) within a package can become challenging to maintain, document, and use by consumers. A service simplifies this complexity by allowing the creation of standalone features, such as dedicated infrastructure and organization of logic, in a way that best fits the ACL rather than its consuming services. Additionally, the ACL service itself can leverage internal packages to reuse code across the service, and the ACL team will have context on these internal development tools. Ideally, having the ACL as a service, not a module or package within a service, will provide easier construction, understanding, and correct visibility of this tool at the levels required from those in and outside the ACL domain.\n\nWhen building an Anti-Corruption Layer, its core functionality can be broken down into three mechanisms: Facades, Adapters, and Translators. A Facade provides a simplified interface on top of a complex set of underlying classes. In terms of the ACL, it simplifies access for the client by making the subsystem easier to use. An Adapter allows the client to use a different protocol than the one understood by the system they are trying to interact with. The final element is the Translator. The Translator's job can be seen as a subset of the Adapter's. The translator's job is to translate the actual logic to be recognizable to the client's system.\n```"}, {"role": "assistant", "content": "Having this within a package can quickly become difficult to maintain, document and use by consumers. A service simplifies this complexity by allowing the creation of standalone features, such as dedicated infrastructure and organisation of logic, in a way that best fits the ACL rather than its consuming services.\n- In addition, the ACL service itself can leverage internal packages to reuse code across the service and the ACL team will have context on these internal development tools. Ideally, having the ACL as a service, not a module or package within a service, will provide easier construction, understanding, and correct visibility of this tool at the levels required from those in and outside the ACL domain.\n## Building Blocks of an ACL:\n\nWhen building an Anti-Corruption Layer, we can break down its core functionality into three mechanisms: Facades, Adapters, and Translators.\n\n- A **Facade** provides a simplified interface on top of a complex set of underlying classes. In terms of the ACL, it simplifies access for the client by making the subsystem easier to use.\n- An **Adapter** allows the client to use a different protocol than the one understood by the system they are trying to interact with.\n- The final element is the **Translator**. The Translator\u2019s job can be seen as a subset of the Adapter\u2019s. The translator\u2019s job is to translate the actual logic to be recognisable to the client\u2019s system."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Building event-driven serverless architectures with Domain-Driven Design can typically end in services that are divided by business function and communicate asynchronously with something like Amazon EventBridge. This type of architecture has several pros, including loose coupling, easy deployment, easy testing, and less complexity. However, it is not possible to eliminate infrastructure dependencies like the event channel. \n\nThe degree to which these dependencies are addressed determines whether the service manages to be relatively independent, or if it becomes unnecessarily complex. \n\nThis text will show how using well-defined service ports can solve those last few infrastructure dependencies. In doing so, services gain loose coupling, enhanced deployment/testing, and a quickened development velocity.\n```"}, {"role": "assistant", "content": "# Service Ports: Finding a Loosely Coupled Utopia with Event-Driven Serverless\n\nWhen we build Event-Driven Serverless architectures using Domain-Driven Design, we end up with a set of services clearly split by business function and communicating asynchronously over an event channel (e.g. Amazon EventBridge). These architectures bring many advantages: loose coupling, independent deployability, testability, and reduced complexity, to name a few. Yet, no matter how elegant our modelling of the domain, there are always some cross-cutting infrastructure dependencies (e.g. the event channel itself) that can\u2019t be eliminated.\n\nHow we deal with them determines if we get a loosely-coupled utopia, or a sprawling tangle of semi-independent services. When you\u2019ve put in 90% of the effort, don\u2019t let this be the final stumbling block.\n\nIn this article we\u2019ll see how clearly defined Service ports resolve the final 10% of infrastructure dependencies and bring loose coupling, independent deployment/testing and increase development velocity."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The provided text discusses the challenges of cross-cutting infrastructure dependencies in a loosely coupled, event-driven architecture. It highlights the importance of clearly defined service boundaries and managing shared resources to avoid issues such as deployment order dependencies, independent development difficulties, and complex integration testing.\n\nThe text proposes a solution using the concept of \"Service ports\" to resolve these infrastructure dependencies and achieve true loose coupling, independent deployment/testing, and increased development velocity.\n\nThe core message is conveyed in a straightforward manner, focusing on the technical aspects and potential pitfalls of cross-cutting dependencies in a microservices architecture. The tone is informative and objective, akin to a technical article or a university essay.\n```"}, {"role": "assistant", "content": ". Amazon EventBridge). These architectures bring many advantages: loose coupling, independent deployability, testability, and reduced complexity, to name a few. Yet, no matter how elegant our modelling of the domain, there are always some cross-cutting infrastructure dependencies (e.g. the event channel itself) that can\u2019t be eliminated.\n\nHow we deal with them determines if we get a loosely-coupled utopia, or a sprawling tangle of semi-independent services. When you\u2019ve put in 90% of the effort, don\u2019t let this be the final stumbling block.\n\nIn this article we\u2019ll see how clearly defined Service ports resolve the final 10% of infrastructure dependencies and bring loose coupling, independent deployment/testing and increase development velocity.\n## How the problem arises\n\nWe\u2019ve discussed previously how Amazon EventBridge can be combined with strong Domain-Driven Design to create such architectures (see EventBridge Storming). When we build a collection of Services (or \u201cmicroservices\u201d) in such a way there are still some cross-cutting global dependencies. These dependencies get in the way of developing services independently, make integration testing difficult and can create complex code in our infrastructure definition. In addition, when such dependencies are not made explicit, the deployment order of Services can become a mystery at best, and a set of race conditions at worst.\n\nTo make this more tangible, let\u2019s imagine we conducted an EventBridge Storming session to understand an example problem domain of an e-commerce website by gathering all the business events of the system into Bounded Contexts. From this session we would understand the business domain, the events that belong to it, and how they can be grouped together. From such a session we could derive the following services, grouping together the domain events and creating service boundaries with loose coupling.\n\nA high level view of the architecture would look as follows:\n\nNotice two classic shared infrastructure dependencies are the EventBus itself, and the Cognito User Pool. Reference to these in the infrastructure is simple, it\u2019s an ARN. But requiring these dependencies to be deployed (with everything else in the stack they belong to) makes independent development and testing complex. In addition, these references can also be hidden in the weeds of the infrastructure as code files.\n\nSuch cross-cutting infrastructure dependencies need to be intentional, and to be designed for and made explicit to the developers. If not, this will lead to several problems:\n\n\u201cThe Source of Existence Problem\u201d. We could keep this definition of the dependencies in the Finance Service and reference it in the HR Service. When it\u2019s 2 Services it\u2019s fine, but what about 10 Services all needing this resource.\n\nIn addition, we\u2019ve created some knock-on issues. The Finance Service now needs to be deployed before the HR service. In production this seems manageable, it will happen the first time; but what about Dev, Staging and UAT? Also, with Serverless, one amazing benefit is ephemeral stacks (see Serverless Flow) \u2014 how will this be achieved?\n\n\u201cThe New Joiner Problem\u201d: Independent development of the HR service is now no longer possible. Any new developer to the company will have had to have deployed their own version of the Finance Service before they can develop on the HR service \u2014 or they depend on the staging version and"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The example provided highlights the challenges that arise when multiple services depend on a shared resource, such as the Finance Service. As the number of dependent services grows, the complexity of managing deployments and ensuring consistency across environments (dev, staging, UAT, production) increases. Additionally, this tight coupling hinders independent development and onboarding of new developers.\n\nTo address these concerns, the concept of Hexagonal Architecture, also known as Ports and Adapters, is introduced. This architectural approach promotes the interchangeability of dependencies, simplifying development and testing processes. By defining clear specifications for ports (interfaces), dependencies can be explicitly defined and implemented through adapters.\n\nThe example illustrates the use of Hexagonal Architecture by defining ports for an EventBus and a UserPool. These ports represent abstract dependencies that can be implemented by various adapters, such as an EventBridge EventBus or a Cognito UserPool. This approach allows for flexibility in swapping out implementations based on the environment or specific requirements.\n\nThe provided YAML file snippet demonstrates how these ports can be defined in a Serverless Framework Infrastructure as Code (IaC) configuration. The environment-dependent configuration allows for easy switching between different implementations, supporting scenarios like temporary CI stacks or local development environments.\n\nBy adopting the Hexagonal Architecture principles, the architecture becomes more modular and loosely coupled. This promotes independent development, as services can be developed and tested independently by mocking the required dependencies. Additionally, the explicit definition of ports and their protocols facilitates better documentation and understanding of the system's dependencies.\n\nOverall, the Hexagonal Architecture approach aims to address the challenges of managing dependencies, promoting flexibility, and enabling independent development and testing within a distributed system architecture.\n```"}, {"role": "assistant", "content": "When it\u2019s 2 Services it\u2019s fine, but what about 10 Services all needing this resource.\n\nIn addition, we\u2019ve created some knock-on issues. The Finance Service now needs to be deployed before the HR service. In production this seems manageable, it will happen the first time; but what about Dev, Staging and UAT? Also, with Serverless, one amazing benefit is ephemeral stacks (see Serverless Flow) \u2014 how will this be achieved?\n\n\u201cThe New Joiner Problem\u201d: Independent development of the HR service is now no longer possible. Any new developer to the company will have had to have deployed their own version of the Finance Service before they can develop on the HR service \u2014 or they depend on the staging version and\n## Hexagonal Architecture To The Rescue\n\nPorts and Adapters, or hexagonal architecture, is an approach that tries to make dependencies interchangeable. The interchangeable aspect allows development and testing to be simplified. In addition, clear specification of ports ensures that dependencies are explicitly defined. Components list their connections as ports, with an API specification of the protocol they expect. One or more adapters can then implement this protocol.\n\nA classic example is a UI port implemented as either a GUI for users or a CLI for development ease. These are typically at the application code level, but we can bring the concept up to higher level architecture dependencies.\n\nIf we look at our example above, the clear dependencies are an EventBus and a UserPool. We could take things further and make these more abstract dependencies, but for now let\u2019s just expect an EventBridge EventBus (any EventBridge EventBus) and a Cognito UserPool (any Cognito UserPool).\n\nA simple representation of these ports can be shown in a Serverless Framework IaC yaml file:\n\n```yaml\nNotice here that it\u2019s environment dependent, and there is a clear switch case on the environment. This can be easily overwritten to support a temporary CI stack as per Serverless flow, and switched out manually during local development as needed.\n```\n\nCreating this explicit ports section makes the interfaces clearer and should be further explained in a consistent README section for search service of the architecture."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The dependencies could be made more abstract, but for now, it is expected to have an EventBridge EventBus and a Cognito UserPool. A simple representation of these ports can be shown in a Serverless Framework Infrastructure as Code (IaC) YAML file. The file demonstrates that it is environment-dependent, with a clear switch case on the environment. This can be easily overwritten to support a temporary CI stack as per Serverless flow, and switched out manually during local development as needed.\n\nCreating an explicit ports section makes the interfaces clearer and should be further explained in a consistent README section for the search service of the architecture.\n\nRegarding the location of cross-cutting infrastructure dependencies, if they live with a Service, such as the Finance Service, they will be deployed with it. However, this may not be appropriate, as a change to the Finance Service does not necessarily mean there should be a deployment of the EventBridge or Cognito User Pool.\n\nThe Ports have mitigated some of the interdependence that would be created, but that does not mean a host Service should be arbitrarily chosen. Instead, these should be considered as Services, albeit simple ones due to the abstraction provided by the Cloud provider over Event Bussing and User Authentication.\n\nWhile it may be tempting to create a \"global,\" \"central,\" or \"infra\" repository and place any cross-cutting dependency into it, this should be avoided as it will quickly become a dumping ground for anything needed by more than one Service, leading to a mess. Instead, very small Services should be created for each cross-cutting dependency, containing the Infrastructure as Code configuration, the CI/CD Pipeline, and any integration tests.\n```"}, {"role": "assistant", "content": "We could take things further and make these more abstract dependencies, but for now let\u2019s just expect an EventBridge EventBus (any EventBridge EventBus) and a Cognito UserPool (any Cognito UserPool).\n\nA simple representation of these ports can be shown in a Serverless Framework IaC yaml file:\n\n```yaml\nNotice here that it\u2019s environment dependent, and there is a clear switch case on the environment. This can be easily overwritten to support a temporary CI stack as per Serverless flow, and switched out manually during local development as needed.\n```\n\nCreating this explicit ports section makes the interfaces clearer and should be further explained in a consistent README section for search service of the architecture.\n## Where Should Cross-Cutting Infrastructure Dependencies Live?\n\nGoing back to our example of the Finance Service. This service relies on an EventBus and a Cognito User Pool. But where should these dependencies live?\n\nIf they live with a Service, e.g. the Finance Service, they will be deployed with it \u2014 but is that correct? Well no, just because there is a change to the Finance Service, it does not mean there should be a deployment of the EventBridge or Cognito User Pool.\n\nOur Ports have mitigated some of the interdependence that would be created, but that does not mean we should arbitrarily pick a host Service. Instead we should see these as Services, very simple services due to the abstraction our Cloud provider is giving us over Event Bussing and User Authentication.\n\nIt\u2019s tempting to create a \u201cglobal\u201d or \u201ccentral\u201d or \u201cinfra\u201d repo and dump any cross cutting dependency into this. Avoid this temptation. It will create a dumping ground for anything needed by > 1 Service and will become a mess quickly. Instead create very small Services for each cross-cutting dependency. These will simply contain the Infrastructure as Code configuration, the CI/CD Pipeline and any integration tests."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Large Language Models' (LLMs) increasing popularity has led to the development of applications based on Generative AI (GenAI). For more imaginative results, these applications utilise third-party LLM services.  Developers, with the help of GenAI applications, have the ability to use AI to generate content, code, or problem-solve in original methods. Regardless of the differences, developers rate these applications on the basis of basic software development principles, such as version control, code review, and rigorous testing. In this discussion, specific difficulties involved with GenAI applications testing will be explored, with attention drawn to the strategies used to overcome these obstacles.\n\nPrompt-centric applications are different from conventional applications in that the primary value is derived from interactions with third-party LLMs rather than the application's code itself. The primary implications of this different approach include elements of uncertainty within LLMs operations. Identical models could generate slightly different results despite similar prompts. The model's response to any given prompt cannot be accurately predicted by developers. Further adding to the complexity, models often provide responses as informal or unstructured plaintext. Creating efficient prompts requires a short feedback cycle to analyse the quality of an LLM's reply. Adjusting prompts within GenAI applications can potentially result in negative side effects and cause behaviour deviations. Therefore, implementing a rigid testing mechanism is crucial for transitioning from proof of concept to a production application.\n```"}, {"role": "assistant", "content": "# Testing GenAI Applications: Addressing the Challenges of Non-deterministic Language Models\nFabien Zucchet, aleios\n\nThe recent emergence of Large Language Models (LLMs) has given rise to a new breed of applications powered by Generative AI (GenAI). These applications leverage third-party LLM services to process inputs in a more creative way than traditional applications. With GenAI applications, developers can easily harness the power of AI to generate content, write code, or solve problems in innovative ways. However, despite their different characteristics, GenAI applications should still be subject to best practice software development principles. Developers still value version control, code review, and rigorous testing for these applications. In this article, we will explore the challenges involved in testing GenAI applications, given the underlying nature of LLMs, and highlight our approach to address these complexities.\n## The Emergence of Prompt-Centric Applications and the Need for Testing\nUnlike traditional applications, where the core value lies in the code, the main value of GenAI applications comes from interacting with third-party LLMs to process user inputs. This paradigm shift has several implications, the most prominent being the non-deterministic nature of LLMs. Running the same model twice with the same prompt can return slightly different responses. Developers have no way to predict precisely how a model will respond to a given prompt (instructions given to an LLM). In addition, the response from the model is returned as loosely structured or unstructured plain text. Efficient prompt engineering relies on a quick feedback loop to evaluate the quality of LLM responses. Additionally, updating prompts in GenAI applications can introduce unintended consequences and may lead to regressions in the application's behavior. Having a robust testing mechanism is key to bridge the gap between proof of concepts and production applications."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```For a given prompt, executing the same model twice may yield slightly dissimilar results. It is impossible for developers to accurately anticipate a model's response to a certain prompt. The model's response is also derived in a form of unstructured or loosely structured plain text. Quick feedback loops are vital for effective prompt engineering, allowing for the better assessment of LLM responses. Modifications to prompts in GenAI applications could also have unexpected effects and potentially lead to a decline in the behaviour of the application, emphasizing the importance of a robust testing method to link the theory and production applications.\n\n## Formulating Test Cases and Developing Test prompts\n\nIdentifying areas in need of significant quality assurance is the initial step before examining an application. There's no universal solution to this query as it is contingent on the specificities of the application in question. In our instance where we worked with Code Review GPT, our attention was primarily placed on the tool's review consistency. We focused on a list of frequent programming errors to observe this consistency, which the tool should recognise. Each entry on this catalogue was designed as a test case, assuring the software would consistently spot and comment on this code in the future. The application generated example code snippets derived from the plain text descriptions of the test cases to run these scenarios.\n```"}, {"role": "assistant", "content": "Running the same model twice with the same prompt can return slightly different responses. Developers have no way to predict precisely how a model will respond to a given prompt (instructions given to an LLM). In addition, the response from the model is returned as loosely structured or unstructured plain text. Efficient prompt engineering relies on a quick feedback loop to evaluate the quality of LLM responses. Additionally, updating prompts in GenAI applications can introduce unintended consequences and may lead to regressions in the application's behavior. Having a robust testing mechanism is key to bridge the gap between proof of concepts and production applications.\n## Defining Test Cases and Generating Test Prompts\nBefore testing an application, the first step is to identify what aspects require the most quality assurance. The answer to this question depends on the specific application, and there is no one-size-fits-all approach. In the case of Code Review GPT, our primary concern was the consistency of the tool's reviews. To measure this consistency, we focused on a list of common coding flaws that the tool should identify. Each item on this list became a test case, and we aimed to ensure that the tool consistently identified and commented on these patterns in the future. We ran these cases through the application by generating example code snippets based on plain text descriptions of the test cases."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The correct answer varies depending on the specific application, and does not have a universal solution. Concerning Code Review GPT, the main interest is the predictable nature of the tool's inspections. This reliability was quantified by concentrating on the standard coding faults that the instrument must identify. Each point on this catalogue served as a test case, with the goal of the appliance systematically spotting and noting these patterns for a long time. These cases were implemented into the application by crafting exemplar code snippets based on written descriptions of these instances.\n## Assessing the Quality of AI Answers\nThe final step involves assessing the produced review's quality by the LLM. Using snapshots or reference points for assessment, it's possible to accurately measure the reviews. By pitting the test results against these snapshots, a judgment on the test's successfulness can be achieved by comparing the semantic similarity of the replies. This method simplifies the challenging task of analyzing the quality of AI responses into side-by-side comparison of two scripts, making the GenAI application's coherence and performance more manageable to gauge.\n## Making the Test Sequence Universal\nThe automatic threshold testing series, including the creation of test prompts, plugging them into the model and calculating the similarity quotient with the blended snapshots, is a concept that could be beneficial in various GenAI implementations.\n```"}, {"role": "assistant", "content": "The answer to this question depends on the specific application, and there is no one-size-fits-all approach. In the case of Code Review GPT, our primary concern was the consistency of the tool's reviews. To measure this consistency, we focused on a list of common coding flaws that the tool should identify. Each item on this list became a test case, and we aimed to ensure that the tool consistently identified and commented on these patterns in the future. We ran these cases through the application by generating example code snippets based on plain text descriptions of the test cases.\n## Evaluating the Quality of AI Responses\nThe last step involves evaluating the quality of the review produced by the LLM. Using snapshots as reference points for evaluation, we can gauge the quality of the reviews accurately. By comparing the test results to these snapshots, we can determine the success of the test based on the semantic similarity of the responses. This approach reduces the complexity of evaluating the quality of AI responses down to comparing two plain text documents, making it easier to assess the consistency and performance of the GenAI application.\n## Generalizing the Test Pipeline\nThe automated threshold testing pipeline, including generating the test prompts, feeding them to the model, and computing a similarity score with the embedded snapshots, is a generic concept that can be applied to many GenAI applications."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Image object detection is a process that identifies and extracts information about entities present in an image. This involves detecting objects, activities, locations, animals, products, and more. Object detection has various applications across different industries like banking, insurance, social media, dating apps, news agencies, and financial technology companies.\n\nA social media startup required an image object detection feature. Users could select and submit their favorite photos to be featured on the startup's social media pages. The marketing team needed a way to search through the image submissions for specific themes like ocean photos, popular landmarks, animals, music concerts, and others.\n```"}, {"role": "assistant", "content": "# Serverless Image Object Detection at a Social Media Startup\n## What is Image Object Detection?\nSimply put, image object detection is the process of detecting and extracting information about entities in a given image. This involves detecting objects, activities, places, animals, products, etc. Image object detection has a wide array of use cases across a variety of industries. Major sectors such as banks, insurance, social media and dating apps, news agencies and FinTech use object detection in some form or another.\n## Our Use Case\nRecently, we were tasked with building an image object detection feature for a social media startup. The use case was simple \u2014 users should be able to select some of their favourite photos and submit them to be featured on one of the startup\u2019s social media pages. The social media marketing team needed a way to search through the image submissions for certain themes \u2014 such as photos of the ocean, popular landmarks, animals, music concerts, etc."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The process of navigating through the image submissions based on particular patterns such as the ocean, famous landmarks, animals, and music concerts is needed by the social media marketing team.  \n## Explanation of Image Object Detection\nIn a straightforward language, image object detection identifies and retrieves data about entities in a specific image. This incorporates detecting objects, activities, places, animals, products. The applications of image object detection can be found in many sectors. It is utilized in important industries like banks, insurance firms, social media and dating applications, news companies, and financial technology in different ways.  \n## Deep Learning in Image Analysis\nClassifying and analyzing images based on their content is not a simple task. Human eyesight is noteworthy, and creating a software that can replicate the human brain's capability to identify objects is extremely difficult. An entire industry of computer vision is committed to this. The process of performing object detection from scratch normally has several steps: gathering and labeling objects within the images, training machine learning models, using the trained models for analysis, optimizing the performance, and then the cycle repeats. Our goal was to develop this function swiftly and evaluate its effectiveness in a production setting as soon as feasible. Furthermore, we did not want to allocate development resources to making a solution from scratch when we could make use of available cloud services. For this, we used Serverless - it powers the entire backend of the startup and works on event-driven principles. This architecture lets us have development teams working solely on creating features that made our social media app different from others. Serverless also assists us in building highly scalable services while only paying for what we use, a crucial factor for an expanding startup. Hence, we used Amazon Rekognition, a complete serverless image and video analysis service, to execute this feature. Rekognition facilitated the development of this complex yet significant workflow in a few hours. Now, let's delve into it.\n```"}, {"role": "assistant", "content": "The social media marketing team needed a way to search through the image submissions for certain themes \u2014 such as photos of the ocean, popular landmarks, animals, music concerts, etc.\n## What is Image Object Detection?\nSimply put, image object detection is the process of detecting and extracting information about entities in a given image. This involves detecting objects, activities, places, animals, products, etc. Image object detection has a wide array of use cases across a variety of industries. Major sectors such as banks, insurance, social media and dating apps, news agencies and FinTech use object detection in some form or another.\n## Image Analysis using Deep Learning\nAnalysing images and classifying them based on scenery and objects within the image is no simple task. Human sight is nothing short of remarkable and building an application that\u2019s able to replicate the brain\u2019s ability to detect objects is immensely complex. There is an entire Computer Vision industry devoted to doing just that. Performing object detection from scratch is typically a multi-step process that involves: Collecting images and labelling the objects within Training the ML models Using the trained models to perform the analysis Performance tuning And, repeat\u2026 Our aim for this feature, like all others on the project, was to build it quickly and test its efficacy in a production environment as soon as possible. Furthermore, we didn\u2019t want to devote development resources to building a solution from the ground up when we could leverage existing cloud services. Cue Serverless \u2014 the startup\u2019s entire backend is fully Serverless and event-driven. With this architecture we\u2019re able to have teams of developers that only need to focus on features that differentiated the social media app from others. Serverless also enables us to build highly-scalable services whilst also only paying for exactly what we use \u2014 an important consideration for a scaling startup. So to achieve this feature, we used Amazon Rekognition \u2014 a fully Serverless image and video analysis service. Using Rekognition, we were able to develop this complex and critical workflow in a matter of hours. Let\u2019s dive into it."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Cue Serverless provides a fully Serverless and event-driven backend architecture for the startup. This approach allows teams of developers to focus solely on features that differentiate the social media app from others. Serverless also enables the development of highly-scalable services, with the added benefit of only paying for the resources used, which is an important consideration for a scaling startup. To achieve this feature, Amazon Rekognition, a fully Serverless image and video analysis service, was utilized. Rekognition enabled the development of this complex and critical workflow in a short period of time.\n\nAmazon Rekognition is an AWS Serverless offering that uses deep learning for image and video analysis. As a fully Serverless service, Rekognition eliminates the need to manage underlying infrastructure complexities, and users only pay for the resources they consume. It provides pre-written software for various image and video analysis tasks. Rekognition offers features such as image label detection, face detection, celebrity detection, content moderation, and text detection. The service abstracts away the complexities of building, training, and analyzing deep learning models, making image and video analysis quick and simple with minimal setup required. There was no need to build and train custom datasets or provision server capacity for scalability, as the integration was the sole focus.\n```"}, {"role": "assistant", "content": "Cue Serverless \u2014 the startup\u2019s entire backend is fully Serverless and event-driven. With this architecture we\u2019re able to have teams of developers that only need to focus on features that differentiated the social media app from others. Serverless also enables us to build highly-scalable services whilst also only paying for exactly what we use \u2014 an important consideration for a scaling startup. So to achieve this feature, we used Amazon Rekognition \u2014 a fully Serverless image and video analysis service. Using Rekognition, we were able to develop this complex and critical workflow in a matter of hours. Let\u2019s dive into it.\n## What is Amazon Rekognition?\nAmazon Rekognition is an AWS Serverless offering that uses deep learning to perform image and video analysis. Being fully Serverless means that with Rekognition we don\u2019t need to worry about the complexity of the underlying infrastructure, we pay only for what we use and it provides us with pre-written software for image and video analysis tasks. Rekognition offers a range of features, including image label detection, face detection, celebrity detection, content moderation and text detection. The best part? Rekognition abstracts away the heavy lifting of building, training and analysing deep learning models. Image and video analysis is quick and simple, with minimal set-up necessary. We didn\u2019t need to worry about building and training our own datasets and provisioning server capacity so that our service would scale. All we needed to worry about was integrating."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Rekognition is a service with various features that include image label detection, face detection, celebrity detection, content moderation and text detection. This service takes care of the complex tasks involved in building, training and analysing deep learning models. Set-up is uncomplicated and the analysis of images and video content is efficient. It eliminates the need to create and train our own datasets and provision server capacity.\n\nThe architectural layout is simple. Our mobile application transfers images from user devices into an S3 bucket. This upload in turn triggers a Lambda function, which consequently makes a call to the Rekognition API and stores the results in DynamoDB for future queries.\n\nBelow is a basic representation of our Serverless framework Infrastructure as Code file:\n\n```yaml\n//serverless.yaml\nfunctions:\n  imageLabelDetection:\n    handler: image-label-detection.handler\n    events:\n      - s3:\n          bucket: my-image-bucket\n          event: s3:ObjectCreated:*\n          existing: true\n    iamRoleStatements:\n      - Effect: Allow\n        Action: rekognition:DetectLabels\n        Resource: \"*\"\n      - Effect: Allow\n        Action: s3:GetObject\n        Resource: arn:aws:s3:::my-image-bucket\n```\n\nThe above-mentioned Lambda function invokes the Rekognition API and deposits the results into DynamoDB, but this can be changed according to one's requirements. We retrieve the S3 bucket name and the name of the image from the S3 event and pass those as parameters to the detect labels function of the Rekognition SDK. Two optional parameters (MaxLabels and MinConfidence) specify the requisite confidence level and max number of labels that need to be returned. Therefore, in this instance, a maximum of 20 labels are returned and all these labels comply with a confidence establishment of over 80%.\n\nHere is an example:\n\n```javascript\n//image-label-detecion.js\nconst AWS = require(\"aws-sdk\");\nconst rekognition = new AWS.Rekognition();\nexports.handler = async (event) => {\n  const imageDetails = event.Records[0].s3;\n  const bucketName = imageDetails.bucket.name;\n  const objectKey = imageDetails.object.key;\n  const rekognitionResp = await rekognition\n    .detectLabels({\n      Image: {\n        S3Object: {\n          Bucket: bucketName,\n          Name: objectKey,\n        },\n      },\n      MaxLabels: 20,\n      MinConfidence: 80,\n    })\n    .promise();\n  // Send to data store, e.g. DynamoDB\n  // ...\n};\n```\n```"}, {"role": "assistant", "content": "Rekognition offers a range of features, including image label detection, face detection, celebrity detection, content moderation and text detection. The best part? Rekognition abstracts away the heavy lifting of building, training and analysing deep learning models. Image and video analysis is quick and simple, with minimal set-up necessary. We didn\u2019t need to worry about building and training our own datasets and provisioning server capacity so that our service would scale. All we needed to worry about was integrating.\n## Architecture\nThe architecture is straight-forward. Our mobile app uploads images from users\u2019 phones into an S3 bucket. The upload to S3 then triggers a Lambda function which in turn calls the Rekognition API, and stores the results in DynamoDB for querying.\n\nA simplified version of our Serverless framework Infrastructure as Code file looks like this:\n\n```yaml\n//serverless.yaml\nfunctions:\n  imageLabelDetection:\n    handler: image-label-detection.handler\n    events:\n      - s3:\n          bucket: my-image-bucket\n          event: s3:ObjectCreated:*\n          existing: true\n    iamRoleStatements:\n      - Effect: Allow\n        Action: rekognition:DetectLabels\n        Resource: \"*\"\n      - Effect: Allow\n        Action: s3:GetObject\n        Resource: arn:aws:s3:::my-image-bucket\n```\n\nOur Lambda code simply calls the Rekognition API and stores the results in DynamoDB, but you can use whatever makes sense for your use case. We obtain the S3 bucket name and the image\u2019s object name from the S3 event and pass those in to the detectLabels function of the Rekognition SDK. We also pass in two optional parameters (MaxLabels and MinConfidence) to specify the confidence level threshold and maximum number of labels that we want returned. In the example below, we will only get up to 20 labels in the response and all labels will have a confidence level of more than 80%.\n\n```javascript\n//image-label-detecion.js\nconst AWS = require(\"aws-sdk\");\nconst rekognition = new AWS.Rekognition();\nexports.handler = async (event) => {\n  const imageDetails = event.Records[0].s3;\n  const bucketName = imageDetails.bucket.name;\n  const objectKey = imageDetails.object.key;\n  const rekognitionResp = await rekognition\n    .detectLabels({\n      Image: {\n        S3Object: {\n          Bucket: bucketName,\n          Name: objectKey,\n        },\n      },\n      MaxLabels: 20,\n      MinConfidence: 80,\n    })\n    .promise();\n  // Send to data store, e.g. DynamoDB\n  // ...\n};\n```"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The topic of discussion is serverless observability tools that use custom metrics and dashboards through Cloud Development Kit (CDK). One of the significant parts of serverless architectures is adequate logging. While third-party tools offer help, Cloudwatch Dashboards and Alarms possess unused capabilities. This information proposes to outline methods to obtain advanced project-wide observability from within.\n\n\u200bServerless projects, which are more or less cloud projects, are usually made up of various services doing different roles to create a final product. While such flexibility proves to be strong, it's difficult to track issues due to the vast array of moving parts. Owing to such difficulty in maintaining observability, reluctance is observed in serverless adoption, prompting the usage of many pre-made tools to gain better insights. In the past, establishing strong monitoring via AWS services was time-consuming while offering minimal direct gains.\n\nHowever, the AWS Cloud Development Kit (AWS CDK) now changes this process significantly. It's an open-source platform designed to set up Infrastructure as Code (IaC) and deploy it through AWS CloudFormation. Important programming languages for serverless architectures are compatible with CDK, with Typescript being the most noteworthy.\n\nThe Object Oriented principle is used by CDK effectively making it structurally modular and extendible. With this, creating dashboards that are used across services and teams is simplified. Compared to other IaC platforms that utilize YAML syntax, CDK offers convenient methods for creating custom constructs. The resulting ability of automated observability tool creation encourages in-house observability as a solution.\n\nOther existing observability tools maintain their importance. Ready-made solutions seem to be firmer. However, CloudWatch IaC is considered a viable option owing to its power and flexibility, specifically in environments where external tools cannot be used - albeit requiring an investment of time.\n```"}, {"role": "assistant", "content": "# Building Serverless Observability Tools With Custom Metrics and Dashboards in CDK\n\nMeaningful logging is a must for Serverless architectures, and of course, there are a lot of third-party tools that can help with observability. However, there is a lot of untapped potential in using Cloudwatch Dashboards and Alarms. This article will show you how to have powerful observability, natively, across your project.\n## Current observability solutions in Serverless\nServerless (and almost all cloud) projects comprise multiple services carrying out different functions to form your product. This modularity is incredibly powerful, but by its very nature makes it hard to track any issues \u2014 there\u2019s a tonne of moving parts and it\u2019s often difficult to tell exactly where something went wrong.\nThis lack of observability is one of the largest gripes people have when adopting serverless, and companies have relied on a huge host of pre-built tools to help better understand their products. Until now, setting up robust monitoring using AWS services has been a painful process that takes up a lot of upfront work before getting any real benefit.\nNow, The AWS Cloud Development Kit (AWS CDK) changes all of that.\nCDK is an open-source framework to create Infrastructure as Code (IaC) and provision it through AWS CloudFormation. It supports the most common Object Oriented Programming Languages, and for Serverless that\u2019s most importantly Typescript.\nCDK utilises this Object Oriented paradigm, and as a result, is instinctually modular and extendable. This allows you to abstract dashboards to use across services and teams, being incredibly easy to create custom constructs compared to other Cloud Formation-creating IaC that use YAML syntax. This has changed the creation of observability tools from arduous to automatic, and now makes native the solution for project observability.\nExisting tools still have their place and for off-the-shelf there is no better, but for highly customizable observation in environments where you can\u2019t leverage external tools, CloudWatch IaC provides a powerful and flexible option if you\u2019re willing to dedicate the time."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```CloudWatch Infrastructure as Code (IaC) lets you make custom dashboards for different services and teams. It's simpler to build custom setups than with other Foundation-creating IaC that use YAML. It makes creating observability tools straightforward and brings automatic standardization to project observability. Existing tools are still useful and are unbeatable when it comes to ready-to-use solutions. But, if you need special observation in a setup where external tools can't be used, CloudWatch IaC offers a solid and adaptable choice if you can allocate the effort.\n\nCDK is evolving and its documentation is not always easy to follow - sometimes leading to wasted time. But after prolonged use, I have managed to build flexible monitoring tools and set up automatic warnings for any issues. This article should help clarify Cloudwatch in CDK and guide you in maintaining your Serverless applications. \n\nThe items we'll discuss are:\n- Cloudwatch Dashboard infrastructure\n- Publishing your custom metrics\n- Making widgets and a dashboard to portray them\n- Setting an Alarm for a Metric\n- Sending an email with SNS when an Alarm rings\n- Keeping an email list in SSM\n- The components of a Cloudwatch Dashboard\n\nA Cloudwatch Dashboard consists of three elements:\n- Metrics \u2013 these are the data you wish to display on the dashboard\n- Widgets \u2013 they are effectively a 'graph' on the dashboard\n- Dashboards \u2013 collections of widgets \nA Dashboard can accommodate numerous Widgets, and a Widget can present several Metrics. An example of multiple metrics in a Widget is coming up.\n```"}, {"role": "assistant", "content": "This allows you to abstract dashboards to use across services and teams, being incredibly easy to create custom constructs compared to other Cloud Formation-creating IaC that use YAML syntax. This has changed the creation of observability tools from arduous to automatic, and now makes native the solution for project observability.\nExisting tools still have their place and for off-the-shelf there is no better, but for highly customizable observation in environments where you can\u2019t leverage external tools, CloudWatch IaC provides a powerful and flexible option if you\u2019re willing to dedicate the time.\n## Fair Warning\nCDK is a maturing product, and at times, the documentation can be both sparse and complicated \u2014 it\u2019s easy to get lost down a rabbit hole, which I spent an embarrassingly long amount of time doing.\nAt the end of that process, however, I\u2019ve now worked with the tool enough to be able to build incredibly flexible monitoring tools, alongside setting automated alarms should any issues arise. This article should help demystify Cloudwatch in CDK, and set you in the right direction to monitor your Serverless applications.\n\nWe will cover:\n- The infrastructure for a Cloudwatch Dashboard\n- How to publish your custom metrics\n- How to create widgets and a dashboard to represent them\n- Setting up an Alarm for a Metric\n- Sending an email with SNS when an Alarm is triggered\n- Storing an email list in SSM\n- The pieces that make up a Cloudwatch Dashboard\n\nA Cloudwatch Dashboard is made up of three parts:\n- Metrics \u2014 the actual data you want to show in the dashboard\n- Widgets \u2014 effectively a \u2018graph\u2019 in the dashboard\n- Dashboards \u2014 the collection of widgets\nA Dashboard can contain multiple Widgets, and a Widget can visualize multiple Metrics.\nAn example of multiple metrics in a Widget"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```- Setting up a Cloudwatch Dashboard for monitoring Serverless applications using CDK\n- The infrastructure required for a Cloudwatch Dashboard\n- Publishing custom metrics\n- Creating widgets and a dashboard to represent metrics\n- Configuring an alarm for a metric\n- Sending an email via SNS when an alarm is triggered\n- Storing an email list in SSM Parameter Store\n- Components that make up a Cloudwatch Dashboard\n\nA Cloudwatch Dashboard consists of:\n- Metrics - the actual data to display\n- Widgets - graphical representations of metrics\n- Dashboards - a collection of widgets\n\nA dashboard can have multiple widgets, and a widget can visualize multiple metrics.\n\nThere are two types of metrics:\n- Default metrics (like Lambda errors and invocations)\n- Custom metrics\n\nDefault metrics are easy to reference in CDK, and the official documentation provides guidance.\n\nTo create a custom metric, you need to publish a data point using the SDK's putMetricData function. This creates a data point instance, which can then be aggregated into a metric.\n```"}, {"role": "assistant", "content": "h in CDK, and set you in the right direction to monitor your Serverless applications.\n\nWe will cover:\n- The infrastructure for a Cloudwatch Dashboard\n- How to publish your custom metrics\n- How to create widgets and a dashboard to represent them\n- Setting up an Alarm for a Metric\n- Sending an email with SNS when an Alarm is triggered\n- Storing an email list in SSM\n- The pieces that make up a Cloudwatch Dashboard\n\nA Cloudwatch Dashboard is made up of three parts:\n- Metrics \u2014 the actual data you want to show in the dashboard\n- Widgets \u2014 effectively a \u2018graph\u2019 in the dashboard\n- Dashboards \u2014 the collection of widgets\nA Dashboard can contain multiple Widgets, and a Widget can visualize multiple Metrics.\nAn example of multiple metrics in a Widget\n## Creating a Metric\nThe first step in creating a dashboard is to create a metric. There are two kinds, default and custom metrics.\n### Default Metrics\nBy default, lambdas already come with some metrics you can record, such as Errors and the Number of Invocations. The whole list can be found here.\nIt\u2019s straightforward to reference these metrics in CDK, and the official documentation for doing so is quite good!\n\nHere\u2019s how you reference a metric that shows the average number of errors per minute.\n\n```typescript\ndeclare const fn: lambda.Function;  \nconst minuteErrorRate = fn.metricErrors({ statistic: 'avg', period: Duration.minutes(1), label: 'Lambda failure rate' });\n```\n\nTo instantiate a custom metric, you have to publish a data point. As publishing data during run-time isn\u2019t IaC, you have to use the SDK, using the putMetricData function. This creates an instance of a datapoint, which you can then aggregate to create a metric.\n\nHere\u2019s an example call that would facilitate a similar Error Count metric, initially creating the datapoint and then creating a metric from that."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Publishing data at runtime is not considered Infrastructure as Code (IaC), and therefore requires the utilisation of the Software Development Kit (SDK), specifically the putMetricData function. This creates a datapoint instance which can consequently be aggregated to establish a metric.\n\nThe subsequent example demonstrates a method to construct an Error Count metric, wherein a datapoint is created, followed by a metric based on that.\n\nCreating the Data Point:\n\n```typescript\nconst params = {\n  MetricData: [\n    {\n      MetricName: 'Errors Thrown',\n      Dimensions: [\n        {\n          Name: 'Error Code',\n          Value: request?.error?.code.toString() ?? '500',\n        },\n        {\n          Name: 'Function Name',\n          Value: request.context?.functionName ?? 'Unknown Function',\n        },\n      ],\n      Unit: 'None',\n      Value: 1,\n      Timestamp: new Date(),\n    },\n  ],\n  Namespace: `${process.env['STAGE']}`, //STAGE\n};\nawait cloudwatch.putMetricData(params).promise();\n```\n\nCreating the Metric:\n\nTo reference the published Metric in Infrastructure as Code (for its use in Dashboards and Alarms), the creation of the following would suffice.\n\n```typescript\nconst errorRate = new Metric({\n  namespace: stage,\n  metricName: 'Errors Thrown',\n  region: 'eu-west-1',\n  dimensionsMap: {\n    'Error Code': 500,\n    'Function Name': functionName,\n  },\n  period: Duration.minutes(1),\n  statistic: 'Average',\n});\n```\n\nIt is critical to ensure that the metricName, dimensionsMap, region and namespace are identical with the data that has been inputted.\n```"}, {"role": "assistant", "content": "As publishing data during run-time isn\u2019t IaC, you have to use the SDK, using the putMetricData function. This creates an instance of a datapoint, which you can then aggregate to create a metric.\n\nHere\u2019s an example call that would facilitate a similar Error Count metric, initially creating the datapoint and then creating a metric from that.\n### Creating the Data Point:\n\n```typescript\nconst params = {\n  MetricData: [\n    {\n      MetricName: 'Errors Thrown',\n      Dimensions: [\n        {\n          Name: 'Error Code',\n          Value: request?.error?.code.toString() ?? '500',\n        },\n        {\n          Name: 'Function Name',\n          Value: request.context?.functionName ?? 'Unknown Function',\n        },\n      ],\n      Unit: 'None',\n      Value: 1,\n      Timestamp: new Date(),\n    },\n  ],\n  Namespace: `${process.env['STAGE']}`, //STAGE\n};\nawait cloudwatch.putMetricData(params).promise();\n```\n### Creating the Metric:\n\nTo then reference the Metric you are publishing in IaC (for use in Dashboards and Alarms), you could then create the following!\n\n```typescript\nconst errorRate = new Metric({\n  namespace: stage,\n  metricName: 'Errors Thrown',\n  region: 'eu-west-1',\n  dimensionsMap: {\n    'Error Code': 500,\n    'Function Name': functionName,\n  },\n  period: Duration.minutes(1),\n  statistic: 'Average',\n});\n```\n\nThe key part to note is the metricName, dimensionsMap, region and namespace all have to be the same as the ones that you put."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Creating the Metric:\n\nTo reference the newly published Metric in Infrastructure as Code (IaC) for inclusion in Dashboards and Alarms, utilize the code below.\n\n```typescript\nconst errorRate = new Metric({\n  namespace: stage,\n  metricName: 'Errors Thrown',\n  region: 'eu-west-1',\n  dimensionsMap: {\n    'Error Code': 500,\n    'Function Name': functionName,\n  },\n  period: Duration.minutes(1),\n  statistic: 'Average',\n});\n```\n\nIt's important to ensure that metricName, dimensionsMap, region, and namespace match their initial settings.\nCreating a Dashboard & Widget\n\nOnce the metric is established, building the widget and dashboard follows. Documentation from Cloud Development Kit (CDK) can be helpful in this process.\n\nA dashboard could be created using the code:\n\n```typescript\nconst lambdaDashboard = new Dashboard(this, 'MyDashboardId', {\n  dashboardName: 'MyDashboardName',\n});\n```\n\nThis is the code for adding a widget:\n\n```typescript\nlambdaDashboard.addWidgets(\n  new GraphWidget({\n    title: `${name} Errors`,\n    width: 24,\n    left: [\n      myMetric\n    ],\n  }),\n);\n```\n```"}, {"role": "assistant", "content": "### Creating the Metric:\n\nTo then reference the Metric you are publishing in IaC (for use in Dashboards and Alarms), you could then create the following!\n\n```typescript\nconst errorRate = new Metric({\n  namespace: stage,\n  metricName: 'Errors Thrown',\n  region: 'eu-west-1',\n  dimensionsMap: {\n    'Error Code': 500,\n    'Function Name': functionName,\n  },\n  period: Duration.minutes(1),\n  statistic: 'Average',\n});\n```\n\nThe key part to note is the metricName, dimensionsMap, region and namespace all have to be the same as the ones that you put.\n### Creating a Dashboard & Widget\nCreating the metric is the difficult part, and the CDK documentation for creating a Widget and Dashboard is fairly good.\n\nFirstly, to create a dashboard you could write the following:\n\n```typescript\nconst lambdaDashboard = new Dashboard(this, 'MyDashboardId', {\n  dashboardName: 'MyDashboardName',\n});\n```\n\nAnd the following to add a Widget:\n\n```typescript\nlambdaDashboard.addWidgets(\n  new GraphWidget({\n    title: `${name} Errors`,\n    width: 24,\n    left: [\n      myMetric\n    ],\n  }),\n);\n```"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Edge Computing involves relocating computing and storage resources closer to end users, aiming to reduce latency, network traffic, bandwidth consumption, geographic distance, energy usage, and power requirements. It represents a shift from the traditional centralized data center/cloud model, where a significant portion of enterprise data processing occurs remotely.\n\nBy 2025, it is predicted that 75% of enterprise data will be generated and processed outside traditional data centers/clouds, compared to approximately 10% currently. Content Distribution Networks (CDNs) pioneered the concept of Edge Computing by distributing data storage across multiple locations nearer to users, reducing data transport time and enhancing performance.\n\nThe maturation of virtualization technologies has enabled not only data storage but also computing resources to be deployed at Edge locations, extending beyond the confines of centralized clouds and data centers. Edge Computing emphasizes real-time and instant data processing, contrasting with the \"Big Data\" focus of cloud computing.\n\nEdge Computing's reduction in latency can facilitate the realization of various applications in the Internet of Things (IoT), Artificial Intelligence (AI), and Machine Learning (ML). Examples include real-time control of autonomous devices, remote surgery, and facial recognition. The advent of 5G and faster connectivity further accelerates the adoption of Edge Computing by reducing communication latency, enabling a new generation of low-latency applications when combined with Edge Computing capabilities.\n```"}, {"role": "assistant", "content": "# Why Serverless will enable the Edge Computing Revolution\n## On the Edge\n\nEdge Computing is a model in which computing and storage move closer to the end user. Currently ~10% of enterprise data is created and processed outside the traditional Data Centre/Cloud. By 2025 that\u2019s predicted to reach 75%.\n\nContent Distribution Networks (or CDNs) represent the first wave of Edge computing. With a CDN the data needed by users is stored in multiple edge locations closer to the users \u2014 reducing the transport time and improving performance. As CDNs have become more advanced the maturity of virtualisation technologies has allowed code as well as storage to move outside the bounds of the Cloud/Data Centre. This has enabled computing resources to emerge as a service in Edge locations.\n\nThe typical usage of Edge computing is for real-time and instant data processing. As opposed to the Cloud, where \u201cBig Data\u201d is the name of the game, there is much more of a focus on realtime \u201cInstant Data\u201d as we move to the Edge.\n\nAt its core Edge computing is all about reduction \u2014 reduction in latency, traffic, bandwidth, geographic distance, energy and power.\n\nThe reduction in latency brought by Edge computing can make certain applications in IOT, AI and ML more achievable. For instance: realtime instruction of autonomous devices, remote surgery and facial recognition are just a few emerging technologies that will leverage Edge computing to some extent.\n\nThe rise in Edge computing is further spurred on by the evolution of 5G and faster connectivity and devices in general. While Edge computing is reducing latency by bringing compute closer to user, 5G is reducing the latency of the communication. Together 5G and Edge computing can bring latency down to previously unattainable levels \u2014 allowing a new generation of application use cases."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```- Edge computing allows technologies like real-time control of autonomous devices, remote surgery, and facial recognition by processing data closer to the source.\n- 5G and faster connectivity are driving the growth of edge computing by reducing communication latency.\n- Together, 5G and edge computing can enable extremely low latency, unlocking new application possibilities.\n\n- Serverless allows building apps and services without managing underlying servers.\n- It's an architectural approach to increase agility and reduce costs.\n- You send code to a cloud provider, which runs it in response to events.\n- AWS Lambda is a popular serverless service, enabling event-driven architectures.\n```"}, {"role": "assistant", "content": "For instance: realtime instruction of autonomous devices, remote surgery and facial recognition are just a few emerging technologies that will leverage Edge computing to some extent.\n\nThe rise in Edge computing is further spurred on by the evolution of 5G and faster connectivity and devices in general. While Edge computing is reducing latency by bringing compute closer to user, 5G is reducing the latency of the communication. Together 5G and Edge computing can bring latency down to previously unattainable levels \u2014 allowing a new generation of application use cases.\n## What is Serverless?\n\nServerless enables us to build applications and services without thinking about the underlying servers. It\u2019s an imperfect name for a broad spectrum of services \u2014 but at it\u2019s core it\u2019s an architectural movement to increase agility through reduced Total Cost of Ownership (TCO).\n\nIn short, Serverless is an abstraction. We\u2019ve abstracted away the details of the underlying machine to enable a model in which pure application code is sent to the Cloud Provider (e.g. AWS) and run in response to different events. Lambda, the \u201cposter child\u201d for Serverless, is the most widely known Serverless Service \u2014 the Function as a Service (FaaS) from AWS. Lambda has changed how we can build application and the range of triggers for Lambda functions has enabled elegant event-driven architectures to flourish."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```The term \"serverless\" loosely represents a wide range of services, fundamentally promoting an architectural shift aimed at enhancing agility while minimising Total Cost of Ownership (TCO). \n\nEssentially, serverless represents an abstraction whereby details concerning the physical computer are abstracted to allow for a system wherein pure application code if forwarded to a Cloud Provider (e.g. AWS) and executed in response to various events. AWS's Lambda, often cited as synonymous with Serverless, is the most prominent Serverless Service, thus, presenting the Function as a Service option (FaaS) from AWS. Lambda services have one of the more dramatic impacts on application development as the collection of Lambda function triggers has allowed for the evolution of distinct event-driven architectures.\n\nMoving on to the application of Edge Computing in practical scenarios, examples include Smart CCTV cameras, IoT devices, Medical devices and CDNs. The processing shift from centralized locations to the data source is the prevalent feature of edge computing. For instance, in the domain of CCTV systems, patterns of behaviour like a human falling could be detected almost instantly using models that have been trained for such behaviour recognition. Besides accuracy, this also minimises any delay in the recognition of a fall event, which, in an industrial background could halt machine operation instantaneously.\n\nWhile advantageous, Edge computing does come with its set of risks. The security concerns advocating 'smart' devices have escalated with the pervasive adoption of IoT irrespective of sector or industry. Despite this, the scalability potential of Edge computing systems could counterbalance some vulnerabilities like a Distributed Denial of Service (DDoS; yet, it necessitates pervasive alterations in the core computer model to manage operations in less secure conditions.\n\n```"}, {"role": "assistant", "content": "It\u2019s an imperfect name for a broad spectrum of services \u2014 but at it\u2019s core it\u2019s an architectural movement to increase agility through reduced Total Cost of Ownership (TCO).\n\nIn short, Serverless is an abstraction. We\u2019ve abstracted away the details of the underlying machine to enable a model in which pure application code is sent to the Cloud Provider (e.g. AWS) and run in response to different events. Lambda, the \u201cposter child\u201d for Serverless, is the most widely known Serverless Service \u2014 the Function as a Service (FaaS) from AWS. Lambda has changed how we can build application and the range of triggers for Lambda functions has enabled elegant event-driven architectures to flourish.\n## Real World Edge Computing\n\nSmart CCTV cameras, IoT devices, medical devices and CDNs (as discussed above) are all use cases of Edge computing. We\u2019re moving processing closer to the place the data is generated. In the case of CCTV this could be the use of instant fall detection through a model trained to recognise a person falling over \u2014 there can be almost zero latency in the inference of a fall event and in a factory setting this could halt machinery instantly.\n\nEdge computing is not without its risks. It\u2019s hard to go a day without hearing about the security issues of \u201csmart\u201d devices with the rapid adoption of IoT across all sectors. The scalability of Edge computing services can mitigate some security threats like DDoS, but the core computing model has to adapt to operating in less trusted environments."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Here are some key points about edge computing and its relation to serverless:\n\n- Processing is moving closer to where data is generated, like CCTV cameras doing instant fall detection with low latency\n- Edge computing has security risks from untrusted devices/environments, but serverless scalability can mitigate threats like DDoS\n- Serverless and edge share similar models:\n    - Abstraction from hardware details\n    - Need for secure sandboxing in untrusted environments  \n    - Handling unpredictable real-time data at scale\n- Edge devices have low resources (CPU/RAM/disk) compared to servers, so efficiency is key\n- Serverless tech and practices are well-suited for edge:\n    - Stateless, abstracted runtime environments\n    - Integration with serverless databases/services\n    - Developer experience with serverless patterns\n- Transitioning from serverless to edge is easier than traditional servers/Kubernetes\n```"}, {"role": "assistant", "content": "We\u2019re moving processing closer to the place the data is generated. In the case of CCTV this could be the use of instant fall detection through a model trained to recognise a person falling over \u2014 there can be almost zero latency in the inference of a fall event and in a factory setting this could halt machinery instantly.\n\nEdge computing is not without its risks. It\u2019s hard to go a day without hearing about the security issues of \u201csmart\u201d devices with the rapid adoption of IoT across all sectors. The scalability of Edge computing services can mitigate some security threats like DDoS, but the core computing model has to adapt to operating in less trusted environments.\n## Worlds Collide \u2014 The Serverless Edge\u2026 CentreLess?\n\nAs stated above, Serverless is all about abstraction. Developers can write application code and ignore the details of the infrastructure. This abstraction, the pay-per-use pricing model and ephemeral (short-lived) execution model of FaaS solutions have a complementary conceptual model to the Edge world.\n\nIn the Edge world:\n- We don\u2019t control the hardware \u2014 so abstraction is key\n- It\u2019s an untrusted environment \u2014 so we need rigid sandboxing\n- We\u2019re often processing unpredictable real-time data at scale \u2014 so we need an extremely scalable and elastic model for compute.\n\nThe typical computers used at Edge locations need to be small and cheap, therefore we\u2019re operating in a low resource (compared to modern Server sizes) environment when it comes to CPU, RAM and disk \u2014 so we need to have compute efficiency and small memory/storage footprints. All these points are challenges that were faced by Serverless, meaning the technology, conceptual model and best practices are complementary to Edge. Serverless has matured extensively since its inception. The tooling, both from the vendor side and the open-source space, has improved massively. Along with this, practitioners have developed best practices and mental models for working in a Serverless environment. All of these advances will enable the rise of Edge computing. Developers have got used to coding in a stateless and abstract runtime environment \u2014 as well as integrating with Serverless databases and third parties with elegant design patterns. The paradigm shift from a Serverless compute model to an Edge compute model is marginal, whereas going from a traditional Server or even Kubernetes environment is a much greater leap \u2014 both in mindset & tooling."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```- Serverless has evolved significantly since its inception, with improved tooling from vendors and open-source communities.\n- Developers have established best practices and mental models for working in a Serverless environment.\n- These advancements will enable the growth of Edge computing.\n- Developers are accustomed to coding in stateless, abstract runtime environments and integrating with Serverless databases and third-party services using elegant design patterns.\n- The shift from Serverless compute to Edge compute is relatively minor, whereas transitioning from traditional Server or Kubernetes environments is a much bigger leap in terms of mindset and tooling.\n\n- Lambda@Edge is a CloudFront (CDN) product, not part of the AWS Serverless team.\n- It allows customers to run application code closer to users by executing in the CDN layer.\n- Lambda@Edge is similar to traditional Lambda, with no infrastructure management and pay-per-use billing.\n- It is commonly used for security checks, location routing, data modification, Server-Side Rendering (SSR) of React applications, image transformations, and A/B testing.\n- Code must be initially deployed in the US-East-1 region and is then distributed across hundreds of data centers worldwide.\n- Despite the distribution across numerous locations, deploying Lambda@Edge functions is relatively simple, thanks to years of Serverless community development.\n\n```yaml\n# This code snippet is preserved as-is\n```\n```"}, {"role": "assistant", "content": "Serverless has matured extensively since its inception. The tooling, both from the vendor side and the open-source space, has improved massively. Along with this, practitioners have developed best practices and mental models for working in a Serverless environment. All of these advances will enable the rise of Edge computing. Developers have got used to coding in a stateless and abstract runtime environment \u2014 as well as integrating with Serverless databases and third parties with elegant design patterns. The paradigm shift from a Serverless compute model to an Edge compute model is marginal, whereas going from a traditional Server or even Kubernetes environment is a much greater leap \u2014 both in mindset & tooling.\n### Lambda@Edge \u2014 Serverless services are already at the Edge\n\nLambda@Edge doesn\u2019t actually sit in the \u201cServerless\u201d team of AWS. In fact it\u2019s a CloudFront (CDN) product. Lambda@Edge lets customers run application code closer to users by running in the CDN layer. It\u2019s very similar to \u201ctraditional\u201d Lambda \u2014 we don\u2019t have to manage the infrastructure and we only pay when the code is running. I often make use of Lambda@Edge for security checks, location routing and context specific data modification, SSR of React applications closer to the users, the classic image transformation use-case and even for some basic A/B testing. With Lambda@Edge code must be deployed initially in the US-East-1 region \u2014 it\u2019s then distributed across 100s of data centres across the world. Surely distributing code to 100s of locations and running it in the CDN layer in response to network requests is going to complicated?\u2026 well no, it\u2019s pretty much the same as deploying a Lambda function \u2014 which, thanks to years of Serverless community development, has become a simple task.\n\n```yaml"}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```Lambda@Edge is a feature of Amazon CloudFront that allows running Lambda functions at the edge locations of the content delivery network (CDN). It is commonly utilized for various use cases such as security checks, location-based routing, context-specific data modification, server-side rendering (SSR) of React applications closer to users, image transformation, and basic A/B testing. \n\nInitially, Lambda@Edge code must be deployed in the US-East-1 region, after which it is distributed across hundreds of data centers worldwide. Despite the distribution across numerous locations and execution in the CDN layer in response to network requests, the process is relatively straightforward and similar to deploying a traditional Lambda function, thanks to the Serverless community's development efforts.\n\nThe Serverless Framework can be used to define the Lambda@Edge function, specify that it is triggered by a CloudFront event, and then deploy it with a single command. The code can be written in Python or JavaScript, and there is no need for manual bootstrapping or virtualization concerns. For example, traffic migration from one S3 bucket to another can be accomplished with a few lines of code. The \"handler\" function and its parameters are familiar to users of traditional Lambda Functions, providing a similar developer experience.\n\nThe primary differences between Lambda@Edge and traditional Lambda Functions are the initial deployment requirement to US-East-1, constraints on execution time, CPU, and memory, and the limitation to only JavaScript or Python (no other supported languages or options for Custom Runtimes). Additionally, deployments and rollbacks take longer due to the distribution time. This demonstrates how the execution model of Serverless complements the execution environment of the edge.\n```"}, {"role": "assistant", "content": "I often make use of Lambda@Edge for security checks, location routing and context specific data modification, SSR of React applications closer to the users, the classic image transformation use-case and even for some basic A/B testing. With Lambda@Edge code must be deployed initially in the US-East-1 region \u2014 it\u2019s then distributed across 100s of data centres across the world. Surely distributing code to 100s of locations and running it in the CDN layer in response to network requests is going to complicated?\u2026 well no, it\u2019s pretty much the same as deploying a Lambda function \u2014 which, thanks to years of Serverless community development, has become a simple task.\n\n```yaml# Serverless.yml file definition for a single Lambda@Edge function \u2014 the complete IaC template needed.\n```\n\nUsing the Serverless Framework we can define our function, state it\u2019s triggered by a CloudFront event and then run a single serverless deploy command. The code itself can be written in Python or JavaScript and there is no manual bootstrapping or virtualisation concerns to deal with. For instance, we could gradually migrate traffic from one S3 bucket to another in a few lines of code. The \u201chandler\u201d function and it\u2019s parameters are very familiar to users of \u201ctraditional\u201d Lambda Functions. The developer experience is the same as developing Serverless applications with Lambda. The only difference is that it needs to be deployed to US-East-1 first and there are more constraints on the execution time, CPU and memory. Also only JS or Python (no other supported languages or options for Custom Runtimes) can be used and deploying/rollbacks take longer due to distribution time. This is a great example of how the execution model of Serverless is complementary to the execution environment of the edge."}]}
{"messages": [{"role": "system", "content": "Gob is a stylistic chatbot that is excellent at rephrasing text."}, {"role": "user", "content": "Can you format this text for the ALEIOS style and tone?\n\n```We have the option to slowly transition traffic between S3 buckets using a few codes. Those familiar with conventional Lambda Functions will recognize the \"handler\" function and its parameters. The development process is similar to creating Serverless applications with Lambda. The only distinction is that it requires initial deployment in US-East-1 and there are more restrictions on execution time, CPU, and memory. Only JS and Python can be utilized, not other supported languages or custom runtime options, and deployment/rollback process is lengthier because of distribution time. This shows how the Serverless execution model works well with the edge execution environment.\n### Firecracker - addressing various issues in Serverless that Edge computing will encounter\n\nAWS created (and released to the public) Firecracker to enhance speed and efficiency in Lambda & Fargate. Firecracker is a technology that facilitates the running of workloads in \"microVMs\". These lightweight virtual machines offer security and isolation while still providing speed and adaptability. The microVMs have a simple design, permitting security and quick start-up times. This results in a lower memory footprint, faster performance and reduced attack surface. The outcomes of using Firecracker includes increased microVM density per machine, improved security, less memory requirements and faster startup times. This is realized by utilizing Linux KVM (kernel-level virtualization with the kernel acting as a hypervisor), Rust as a fast implementation language, and a precise and minimal API. Firecracker was intended to reduce latency in a resource-limited setting for the Serverless FaaS (Lambda). The requirement for low latency, strong security isolation, and tenant density in a resource-limited environment align closely with the needs of Edge computing. The need to run code with low latency in a low trust multi-tenant setting on minimally resourced machines is quite high. Hence the fact that Lambda@Edge operates on Firecracker is not surprising. It's one instance of tech advancements made for Serverless that help Edge computing's development.\n```"}, {"role": "assistant", "content": "For instance, we could gradually migrate traffic from one S3 bucket to another in a few lines of code. The \u201chandler\u201d function and it\u2019s parameters are very familiar to users of \u201ctraditional\u201d Lambda Functions. The developer experience is the same as developing Serverless applications with Lambda. The only difference is that it needs to be deployed to US-East-1 first and there are more constraints on the execution time, CPU and memory. Also only JS or Python (no other supported languages or options for Custom Runtimes) can be used and deploying/rollbacks take longer due to distribution time. This is a great example of how the execution model of Serverless is complementary to the execution environment of the edge.\n### Firecracker \u2014 solving many challenges for Serverless that Edge computing will face\n\nAWS developed (and open-sourced) Firecracker as an enabling technology to Lambda & Fargate by improving speed and overall efficiency. Firecracker is a virtualisation technology that enables workloads to execute in \u201cmicroVMs\u201d. microVMs are lightweight VMs (Virtual Machines) that provide security and isolation while maintaining speed and flexibility. Security and low startup-times are enabled via a very minimal design. This ensures the memory footprint is kept low, they run quick and have a minimal attack surface. In short, Firecracker brings greater tenancy density (number of microVMs per machine), improved security, reduced memory requirements and rapid startup times. This is achieved by the leveraging of Linux KVM (virtualization at the kernel level \u2014 with the kernel acting as a hypervisor), using Rust as a highly performant implementation language and a very specific and minimal API. Firecracker aimed to improve latency in a resource constrained environment for the purpose of a Serverless FaaS (Lambda). The needs for low latency, strong security isolation and tenant density in a low resource environment are a massive overlap with the needs of Edge computing. Code will need to be run with low latency (especially due to the use cases highlighted above) in a low trust multi-tenant environment on small resourced machines. It\u2019s not surprising then that Lambda@Edge runs on Firecracker. It\u2019s just one example of a technological advance made for Serverless that benefits the development of Edge computing."}]}
