{
  "chunk": "<h1>Everything you need to know about DynamoDB Partitions</h1><p>Prefer video? View this post on YouTube!</p><p>DynamoDB powers some of the highest-traffic systems in the world, including Amazon.com's shopping cart, real-time bidding for ad platforms, and low-latency gaming applications. They use DynamoDB because of its fast, consistent performance at any scale.</p><p>Before I understood DynamoDB, I thought AWS had a giant supercomputer that was faster than everything else out there. Turns out that's not true. They're not defying the laws of physics -- they're using basic computer science principles to provide the consistent, predictable scaling properties of DynamoDB.</p>",
  "neutralised_chunk": "- This post offers information on DynamoDB Partitions.\n- DynamoDB powers high-traffic systems worldwide, including Amazon.com's shopping cart, real-time ad platform bidding, and low-latency gaming applications.\n- DynamoDB's popularity is due to its fast, consistent performance regardless of scale.\n- The consistent and predictable scaling properties of DynamoDB are due to basic computer science principles, not superior computing power."
}
{
  "chunk": "<p>In this post, we'll take a deep look at DynamoDB partitions -- what they are, why they matter, and how they should affect your data modeling. The most important reason to learn about DynamoDB partitions is because it will shape your understanding of why DynamoDB acts as it does. At first glance, the DynamoDB API feels unnecessarily restrictive and the principles of single-table design seem bizarre. Once you understand DynamoDB partitions, you'll see why these things are necessary.</p><p>If you want to skip to specific sections, this post covers:</p><p>Let's get started!</p>",
  "neutralised_chunk": "- This post will delve into DynamoDB partitions: what they are, why they matter, and how they should influence data modeling.\n- Understanding DynamoDB partitions is crucial for comprehending DynamoDB's behavior and the rationale behind its API restrictions and single-table design principles.\n- Initially, the DynamoDB API may seem unnecessarily restrictive, and single-table design principles may seem bizarre.\n- However, once DynamoDB partitions are understood, the reasons for these aspects become clear.\n- The post will cover the following sections:\n  - (List of sections to be covered)"
}
{
  "chunk": "<h2>Background on DynamoDB partitions\u200b</h2><p>To begin, let's understand the basics of partitions in DynamoDB. And before we do that, we need to review some basics about DynamoDB tables.</p><p>DynamoDB is a schemaless database, which means DynamoDB itself won't validate the shape of the items you write into a table. Yet a DynamoDB table is not totally without form. When creating a DynamoDB table, you must specify a primary key. Each item that you write into your table must include the primary key, and the primary key must uniquely identify each item.</p><p>There are two types of primary keys in DynamoDB. First is the simple primary key, which consists of a single element called the partition key. The image below shows an example of a table with a simple primary key:</p><p></p>",
  "neutralised_chunk": "- The section starts by discussing the basics of partitions in DynamoDB.\n- To understand this, it's necessary to review some basic concepts about DynamoDB tables.\n- DynamoDB is a schemaless database, so it doesn't verify the shape of items written into a table.\n- However, a DynamoDB table isn't completely formless, as a primary key must be specified when creating the table.\n- Each item written into the table must include this primary key, which uniquely identifies each item.\n- There are two types of primary keys in DynamoDB: a simple primary key consisting of a single element known as the partition key. An example of a table with a simple primary key is provided."
}
{
  "chunk": "<p>This table is storing Orders in an e-commerce application. Notice the primary key (outlined in blue), on the far left side of the table. It consists of a partition key called \"OrderId\" (outlined in red). Each Order item in the table contains the \"OrderId\" property, and it will be unique for each item in the table.</p><p>The second type of primary key is a composite primary key. A composite primary key consists of two elements: a partition key and a sort key. The image below shows an example of a table with a composite primary key.</p><p></p>",
  "neutralised_chunk": "- The table stores Orders for an e-commerce application.\n- The primary key, on the left side of the table, is the \"OrderId\", marking it as a partition key.\n- Each Order item in the table contains the \"OrderId\" property, which is unique for each item.\n- The second type of primary key is a composite primary key.\n- A composite primary key is made up of two elements: a partition key and a sort key.\n- An example of a table with a composite primary key is shown."
}
{
  "chunk": "<p>This table stores the same Orders as our previous table, but it also includes information about each Order's Customer. The primary key (outlined in blue) on the left side of the table now has two parts -- a partition key (outlined in red) of CustomerId and a sort key of OrderId. Like with the simple primary key, each item in this table will need to include the primary key (both elements), and it is the combination of these two elements that will uniquely identify each item in the table.</p><p>Notice how both primary key structures include a partition key element. This partition key is crucial to DynamoDB's infinite scaling capabilities.</p>",
  "neutralised_chunk": "- The table stores the same Orders as the previous table and also includes information about each Order's Customer.\n- The primary key now consists of two parts: a partition key of CustomerId and a sort key of OrderId.\n- Similar to the simple primary key, each item in this table will need to include both elements of the primary key.\n- The combination of these two elements uniquely identifies each item in the table.\n- Both primary key structures include a partition key element, which is crucial to DynamoDB's infinite scaling capabilities."
}
{
  "chunk": "<h3>Horizontal scaling and the partition key\u200b</h3><p>As noted in the introduction, your DynamoDB table isn't running on a single supercomputer powered by qubits. Rather, it's distributed across an entire fleet of computers, where a single computer holds a subset of your table's data. These subsets are called 'partitions', and they enable essentially infinite scaling of your DynamoDB table.</p><p>Let's walk through this with an example.</p><p>When you write an item to your DynamoDB table, the request will first be handled by a frontend service called the request router.</p><p></p><p>The request router doesn't store your actual data. Its job is to understand metadata about your table and its partitions, then route the request to the right partition.</p><p>This request router will parse the partition key from your item and hash it.* Then, based on the hashed value of the partition key, it will determine the partition to which this item should be written.</p>",
  "neutralised_chunk": "- DynamoDB tables are distributed across multiple computers, not a single machine.\n- Each computer holds a subset of the table's data called a 'partition'.\n- Partitions enable essentially infinite scaling of a DynamoDB table.\n- When writing an item to a table, the request is handled by a frontend service called the request router.\n- The request router does not store data but routes the request to the correct partition.\n- It parses the partition key from the item and hashes it.\n- Based on the hashed partition key value, it determines which partition to write the item to."
}
{
  "chunk": "<p>* Why does DynamoDB hash the partition key before placing it on a partition? This is to better spread data across your partitions. Imagine you had a sequential identifer as your partition key, and you wanted to iterate over sequential identifiers for one of your access patterns. If these values weren't hashed before locating on a partition, you could end up with hot partitions for these access patterns. A similar principle exists for MongoDB and is explained here.</p><p></p><p>In the image above, the request router determined that the incoming item belonged to Partition 1 and send the request there to be handled.</p><p>Reading items from DynamoDB follows the same flow. The request router will parse the partition key, then route the read request to the proper partition to read and return the data.</p><p></p>",
  "neutralised_chunk": "- DynamoDB hashes the partition key before placing it on a partition to better distribute data across partitions.\n- If sequential identifiers were used as partition keys without hashing, it could lead to hot partitions for certain access patterns.\n- MongoDB follows a similar principle, as explained in a separate resource.\n- In the provided image, the request router determined that the incoming item belonged to Partition 1 and sent the request there for handling.\n- Reading items from DynamoDB follows a similar flow: the request router parses the partition key, routes the read request to the appropriate partition, reads the data, and returns it."
}
{
  "chunk": "<p>Each partition is roughly 10GB in size, so DynamoDB will add additional partitions to your table as it grows. A small table may only have 2-3 partitions, while a large table could have thousands of partitions.</p><p></p><p>The great part about this setup is how well it scales. The request router's work of finding the proper node has time complexity of O(1).</p><p></p><p>This is the crucial fact about DynamoDB's horizontal scaling -- even with thousands of partitions, the request router's job is a fast, consistent operation that narrows your table from multiple terabytes down to a more manageable 10GB.</p>",
  "neutralised_chunk": "- DynamoDB partitions are approximately 10GB each; additional partitions are created as the table grows.\n- A small table could have 2-3 partitions, but a large one could have thousands.\n- The system scales effectively due to the request router's work in locating the correct node being an O(1) time complexity operation.\n- Crucially, regardless of the number of partitions in DynamoDB's horizontal scaling, the request router's job remains a fast, consistent operation - reducing the size of the table from multiple terabytes to a more manageable 10GB.\n"
}
{
  "chunk": "<h2>How partitions do (and don't) matter in DynamoDB\u200b</h2><p>Now that we know how partitions work in DynamoDB, let's see how they do and don't matter for you as a DynamoDB user.</p><p>We'll start with how they don't matter. On a practical basis, you don't need to know about individual partitions in your DynamoDB table. You won't need to specify the partition to use when reading or writing an item into DynamoDB. The request router handles all of this for you.</p><p>Not only that, but you can't know about individual partitions in DynamoDB. All requests go through the request router and use an (undisclosed) hash function before placing items, so you couldn't understand the various partitions even if you wanted to. Note that this is different than other NoSQL databases, such as Cassandra, where clients can be topology-aware and thus go directly to a specific partition rather that hitting a shared router first.</p>",
  "neutralised_chunk": "- Partitions in DynamoDB are transparent to users.\n- Users do not need to know or specify partitions when reading or writing items.\n- The request router handles partition selection and routing automatically.\n- The specific partitions are unknown to users, as requests go through the request router.\n- An undisclosed hash function is used to place items in partitions.\n- This is different from some NoSQL databases where clients can directly access specific partitions."
}
{
  "chunk": "<p>There is a slight performance hit of using a shared request router, as client requests will now take an additional network hop to fulfill the request. However, your client's ignorance about partitions makes it easier for you to reason about as a DynamoDB user.</p><p>First, your client doesn't need to request and maintain cluster metadata when interacting with the DynamoDB backend. This reduces the initial information your client needs to pull down when connecting as well as the continued chatter to keep aware of cluster updates.</p><p>Additionally, it makes it easier for DynamoDB to provide strong performance guarantees around your data. A few years ago, the partition performance story was more nuanced. For high-scale tables, you needed to be aware of your partitions and the request distribution across partitions. Your provisioned throughput was spread equally across all partitions in your table, so you would need to provision capacity for your highest-traffic partition.</p>",
  "neutralised_chunk": "- Using a shared request router leads to a slight performance reduction due to an additional network hop.\n- Clients are not aware of partitions easing understanding for DynamoDB users.\n- Clients do not need to request and maintain cluster metadata when interacting with DynamoDB, reducing the initial and ongoing data requirements.\n- This unawareness enhances DynamoDB's ability to provide strong performance guarantees.\n- Previously, understanding partitions and request distribution was important for high-scale tables.\n- Provisioned throughput was distributed equally across all partitions, requiring capacity planning for the busiest partition."
}
{
  "chunk": "<p>In 2018, DynamoDB announced adaptive capacity for your partitions. With adaptive capacity, your provisioned throughput is shifted around to the partitions that need it rather than equally across all partitions. DynamoDB will take the requisite actions to ensure your capacity is used efficiently. It can transparently add new partitions as your data size grows, or it can split highly-used partitions into sub-partitions to ensure it can provide consistent performance.</p><p>With these improvements, you can think less about DynamoDB partitions. As long as you are fitting within the partition throughput limit, you don't need to worry about balancing your partitions.</p>",
  "neutralised_chunk": "- In 2018, DynamoDB introduced adaptive capacity for partitions.\n- Adaptive capacity allows provisioned throughput to be dynamically allocated to partitions based on demand, rather than being equally distributed.\n- DynamoDB automatically manages capacity utilization by taking actions like:\n  - Adding new partitions as data size grows\n  - Splitting highly-used partitions into sub-partitions for consistent performance\n- With adaptive capacity, you don't need to worry about balancing partitions manually.\n- As long as you stay within the overall partition throughput limit, DynamoDB handles efficient capacity utilization."
}
{
  "chunk": "<p>While you don't need to know the details about partitions on a practical matter, understanding DynamoDB partitions helps on a theoretical matter. By understanding how DynamoDB uses partitions and why it matters, it helps to build your mental model of how DynamoDB works. After all, the principles of single-table design seem pretty wild if you're coming from a normalized, relational database model.</p><p>Once you understand the centrality of partitions in DynamoDB, the rest of DynamoDB's data modeling makes sense.</p>",
  "neutralised_chunk": "- Understanding DynamoDB partitions is helpful for building a theoretical mental model of how DynamoDB operates.\n- The principles of single-table design can seem unconventional when coming from a normalized, relational database background.\n- Grasping the importance of partitions in DynamoDB provides clarity on its data modeling approach.\n- Once partitions are understood, the rest of DynamoDB's data modeling becomes more comprehensible."
}
{
  "chunk": "<h3>Primacy of primary keys\u200b</h3><p>The most important implication of DynamoDB's partition is the primacy of primary keys. We've seen how the request router immediately uses the partition key to route a request to the proper partition. Because of this, DynamoDB pushes hard for you to include the partition key in your requests.</p><p>The DynamoDB API has three main types of actions:</p><p>Other than the wildly-inefficient (for most use cases!) Scan operation, you must include the partition key in any request to DynamoDB.</p><p>In the background section, we noted that DynamoDB is schemaless outside the primary key. Both of these characterisitcs -- general schemalessness, with an exception for the primary key -- are due to DynamoDB's partitioning scheme.</p><p>DynamoDB forces your access of items to be centered on the primary key. Thus, it needs to enforce that the primary key exists for each item so that it can be indexed appropriately.</p>",
  "neutralised_chunk": "- The importance of primary keys in DynamoDB is due to its partition structure.\n- The request router uses the partition key to route requests to the correct partition.\n- DynamoDB encourages including the partition key in requests.\n- The DynamoDB API consists of three main action types.\n- Except for the Scan operation, the partition key must be included in DynamoDB requests.\n- DynamoDB is schemaless outside the primary key due to its partitioning structure.\n- Access to items in DynamoDB is focused on the primary key, requiring its existence for proper indexing."
}
{
  "chunk": "<p>For the rest of the item, however, the shape is irrelevant. You won't have foreign key references to other tables, like you would in a relational database. Nor will you have neatly structured result sets with typed columns. Because the primary key is the only thing that matters, the rest of the data is just coming along for the ride.</p><p>DynamoDB is more like a key-value store than a spreadsheet. The key might be simple, with just a partition key, or complex, with a partition key plus a sort key, but the value is still just a blob.</p><p>One word of caution -- DynamoDB's schemaless nature does not mean your data shouldn't have a schema somewhere. You need to maintain and validate your data schemas within your application code rather than relying on your database to handle it.</p>",
  "neutralised_chunk": "- DynamoDB items have no enforced structure beyond the primary key.\n- No foreign key relationships or typed columns like relational databases.\n- Data is associated with the primary key, with no strict schema.\n- Operates as a key-value store, with keys that can be simple (partition key) or compound (partition + sort key).\n- Values are opaque blobs without enforced schemas.\n- Despite being schemaless, data should still have defined schemas maintained in application code.\n- Schema validation must be handled in the application layer, not relying on the database."
}
{
  "chunk": "<p>Note that while other NoSQL databases use the same partitioning concept to enable horizontal scaling, not all of them restrict the API accordingly. If you're not careful, you can lose the benefits of horizontal scaling by using scatter-gather queries to each of your partitions!</p><p>When thinking about data access in DynamoDB, remember this image:</p><p></p><p>You can query efficiently by the attributes in your primary key, but you cannot query efficiently by other attributes in your table. Design your table accordingly!</p>",
  "neutralised_chunk": "- Other NoSQL databases use partitioning for horizontal scaling but do not always restrict the API to maintain this benefit.\n- Careless use of scatter-gather queries to each partition can eliminate the advantages of horizontal scaling.\n- Efficient queries in DynamoDB can be performed by the attributes in the primary key, not by other attributes in the table.\n- The table design should reflect this."
}
{
  "chunk": "<h2>Consider how to split up your data\u200b</h2><p>The second implication of DynamoDB's partition is that it forces you to consider how to split up your data.</p><p>One of the biggest mistakes I see new DynamoDB users make is to put a lot of unrelated items into a single item collection. Don't do that! Rather, use a high-cardinality value for your partition key.</p><p>If you have users in your application, the userId is often a good partition key. This will ensure different users are placed in different item collections, and you can have lots of users using your application without affecting each other.</p><p>On the other hand, using something like a boolean value (true / false) or an enum (red / green / blue) as your partition key is not a good idea. You don't have high enough cardinality to really distribute your data across your data.</p><p>Careful consideration of how you split up your data can also assist with complex access patterns.</p>",
  "neutralised_chunk": "- DynamoDB's partition model necessitates considering data distribution.\n- A common mistake made by new DynamoDB users is grouping unrelated items in one item collection, which should be avoided.\n- It is recommended to use a high-cardinality value for the partition key.\n- For applications with users, \"userId\" is often a suitable partition key, as it distributes users across different item collections.\n- Using boolean values or enums for partition keys is not advisable due to the low cardinality, which hinders data distribution.\n- Thoughtful data distribution can help manage complex access patterns."
}
{
  "chunk": "<p>You'll often hear that DynamoDB can't handle full-text search or complex filtering patterns. And it's true -- if you want to perform full-text search across your entire application, you'll probably want to reach for dedicated search infrastructure.</p><p>However, your search needs often aren't that extensive. You can usually narrow your search requirements to a smaller section of your dataset. For example, you may want to allow full-text search on users within an organization. If that's the case, DynamoDB can work well. You can partition your users by organization. When a client makes a search request, you can pull in all the users for that organization then perform an in-memory search on the results before sending back to the client. If the corpus you're searching against is narrow (here, the set of users in an organization), you may not need dedicated search infrastructure.</p>",
  "neutralised_chunk": "- DynamoDB is often criticized for not handling full-text search or complex filtering patterns across the entire application dataset.\n- For extensive full-text search requirements across the entire application, dedicated search infrastructure is recommended.\n- However, search needs are often more limited in scope, confined to a smaller subset of the data.\n- For example, you may want to enable full-text search only for users within a specific organization.\n- In such cases, DynamoDB can work well by partitioning users by organization.\n- When a search request comes in, you can retrieve all users for that organization and perform an in-memory search on the results before sending them to the client.\n- If the search corpus is narrow (e.g., users within an organization), dedicated search infrastructure may not be necessary."
}
{
  "chunk": "<p>In a recent chat with Rick Houlihan, Rick told me that this is how Amazon.com retail handles most of their e-commerce search and filtering. If you search for \"sony 42 inch tv\", the backend doesn't search the entire inventory of Amazon.com. Rather, it narrows down to the TV category, then performs a more targeted search there.</p><p>You can use this same strategy with geo-lookups. If you want to provide geo-search on your dataset, figure out how you can break it down -- into countries, states, cities, geohashes, etc. Narrow your initial search space when reading from DynamoDB, then provide an in-memory filter to further reduce your options.</p><p>By understanding the role that partitions play, you can lean into the efficiency of DynamoDB by segmenting your data accordingly.</p>",
  "neutralised_chunk": "- Amazon.com retail uses strategic searching in their e-commerce operations according to Rick Houlihan.\n- When searching for a specific product, the backend refines the search area instead of going through the entire inventory.\n- A similar strategy can be applied for geo-lookups \u2013 breaking down the search into categories like countries, states, cities, geohashes, etc.\n- After narrowing down the search space in DynamoDB, an in-memory filter can further refine the options.\n- Understanding the role of partitions can improve efficiency by better segmenting data in DynamoDB.\n"
}
{
  "chunk": "<h2>No joins + denormalization\u200b</h2><p>Last but not least, DynamoDB's lack of support for joins is mostly due to the partitioning scheme.</p><p>When a relational database is performing a join operation, it's evaluating portions of two separate tables for comparison and combination. This poses two problems for DynamoDB.</p><p>First, the relevant portions of the two tables may be on two separate partitions. This adds additional network hops to perform the separate reads, as well as a combination step on a centralized node before returning to the client.</p>",
  "neutralised_chunk": "- DynamoDB's lack of support for joins is primarily due to its partitioning scheme.\n- In a relational database, joins involve evaluating and combining portions of separate tables.\n- This poses two challenges for DynamoDB:\n    - The relevant data from the tables may reside on separate partitions, requiring additional network hops for separate reads.\n    - A centralized node would be needed to combine the data from different partitions before returning to the client."
}
{
  "chunk": "<p>Second, it's possible that a client will want to join an attributes that aren't part of the primary key (and thus aren't indexed properly). This goes against DynamoDB's core philosophy around primary keys discussed above. Additionally, relational databases often include sophisticated query planners that determine the most efficient way to execute a given query. They include information about table structures and indices, as well as statistics on the contents of a particular table.</p><p>None of this is completely insurmountable in a partitioned database like DynamoDB. MongoDB offers the $lookup aggregation, which performs an outer join. Additionally, partitioned relational database options like Vitess offer support for joins. However, it adds complexity -- complexity for the DynamoDB backend and complexity for the DynamoDB user. Further, it adds performance variability, which is the bane of DynamoDB.</p>",
  "neutralised_chunk": "- Clients may wish to join attributes not part of the primary key, conflicting with DynamoDB's emphasis on primary keys.\n- Relational databases provide complex query planners that account for table structures, indices, and table content statistics.\n- These challenges are not impossible to overcome in partitioned databases like DynamoDB.\n- MongoDB provides the $lookup aggregation for outer join operations.\n- Partitioned relational databases like Vitess support joins.\n- However, these solutions increase complexity for both DynamoDB backend and users.\n- Additional complexity also introduces performance variability, a disadvantage for DynamoDB."
}
{
  "chunk": "<p>Instead, DynamoDB forces you to model around the lack of joins. This might mean denormalization or it could mean pre-joining heterogenous items into the same partition. You can choose the proper approach based on your needs. Whichever route you choose, you won't be guessing about the performance implications as your application grows.</p>",
  "neutralised_chunk": "- DynamoDB requires you to design without the use of joins.\n- This may result in denormalization or pre-joining heterogenous items into the same partition.\n- The approach can be selected according to specific requirements.\n- Regardless of the chosen approach, there will be no uncertainty about performance implications as the application scales."
}
{
  "chunk": "<h2>Conclusion\u200b</h2><p>In this post, we learned about DynamoDB partitions -- what they are, how they do (and don't) matter, and how they should affect your data modeling.</p><p>The main takeaway from this post is that you can and should learn the philosophy of DynamoDB. It's not that big of a surface area, particularly when compared to all the knobs and buttons with a relational database. By learning a few key principles, you'll build a proper mental model of DynamoDB, and it will be easier to use it correctly.</p><p>If you have questions or comments on this piece, feel free to leave a note below or email me directly.</p>",
  "neutralised_chunk": "- The post discussed DynamoDB partitions, their significance, and their effect on data modeling.\n- The key lesson is the importance of understanding the philosophy of DynamoDB.\n- Compared to relational databases, DynamoDB is not as complicated once key principles are learned, leading to an accurate mental model and easier usage.\n- Readers are invited to share any questions or comments on the piece."
}
{
  "chunk": "<h1>Understanding Eventual Consistency in DynamoDB</h1><p>One of the core complaints I hear about DynamoDB is that it can't be used for critical applications because it only provides eventual consistency.</p><p>It's true that eventual consistency can add complications to your application, but I've found these problems can be handled in most situations. Further, even your \"strongly consistent\" relational databases can result in issues if you're not careful about isolation or your choice of ORM. Finally, the benefits from accepting a bit of eventual consistency can be pretty big.</p><p>In this post, I want to dispel some of the fear around eventual consistency in DynamoDB.</p><p>We'll walk through this in three different sections:</p><p>My hope is that you can use this to make a more informed decision about when and how you can handle eventual consistency in your application.</p>",
  "neutralised_chunk": "- DynamoDB's eventual consistency model is a common criticism, with concerns about its use in critical applications.\n- Eventual consistency can introduce complications, but these can be managed in most cases.\n- Even strongly consistent relational databases can have issues related to isolation or ORM choices.\n- The benefits of accepting some eventual consistency in DynamoDB can be significant.\n- The post aims to address concerns around eventual consistency in DynamoDB.\n- The content will be covered in three sections."
}
{
  "chunk": "<h2>Background\u200b</h2><p>Before we get started, let's set the boundaries for what we're going to discuss today.</p><p>There's a lot of confusion around eventual consistency. Part of this is because the area of database and distributed system consistency is confusing -- so much so, that I wrote a huge post on the different concepts of consistency before I could write this one. Amazingly, that post covered three types of consistency, and the idea of 'eventual consistency' was not one of them. If the concepts in this background are confusing to you, you should read that post first.</p><p>At a very high level, the notion of \"eventual consistency\" refers to a distributed systems model where, to quote Wikipedia, \"if no new updates are made to a given data item, eventually all accesses to that item will return the last updated value.\"</p>",
  "neutralised_chunk": "- The discussion sets boundaries on the topic of eventual consistency.\n- Confusion around eventual consistency is prevalent, partly due to the complex nature of database and distributed system consistency.\n- An in-depth post was written covering three types of consistency, none of which was 'eventual consistency'.\n- Readers may find it helpful to read that post if the concepts in this background are unclear.\n- \"Eventual consistency\", in high level terms, refers to a distributed systems model which ensures all accesses to a particular data item return the last updated value after a certain point, given no new updates are made."
}
{
  "chunk": "<p>Note that this definition talks about what eventual consistency can promise, but the interesting part is what it doesn't promise. It doesn't promise that any read of a data item will show the most recent updates to that item. This is the core downside of eventual consistency -- you could update an item with one request, then fail to see those updates with a subsequent request.</p><p>With that in mind, let's note three key aspects about eventual consistency. These aspects are broadly applicable, but I will use a DynamoDB focus in discussing them.</p>",
  "neutralised_chunk": "- The definition of eventual consistency outlines what it guarantees, but the key aspect is what it does not guarantee.\n- It does not promise that any read of a data item will show the most recent updates to that item.\n- This is the core drawback of eventual consistency - you could update an item, but a subsequent request may not reflect those updates.\n- Three key aspects of eventual consistency will be discussed, with a focus on DynamoDB:"
}
{
  "chunk": "<h3>Eventual consistency is about replication\u200b</h3><p>Fundamentally, eventual consistency is an issue of replication.</p><p>In a single-node database, you don't have to worry about eventual consistency. Writes and reads are coming to the same instance, so it should be able to provide a consistent view of a single data item.</p><p>Once you start to replicate your data to multiple nodes, the story changes. Now, when a write comes to a node, you need to figure out how to efficiently communicate that write to the other nodes. Further, if you allow reads from the other nodes, then you think about the PACELC tradeoff discussed in the next section.</p><p>Replication adds some complexity around eventual consistency, but replication can be a good thing as well! There are a number of reasons you may want to replicate your data to multiple places, such as:</p>",
  "neutralised_chunk": "- Eventual consistency is principally related to data replication.\n- In a single-node database, eventual consistency isn't an issue as writes and reads are centralized, allowing for a consistent view of data.\n- When data is replicated across multiple nodes, managing eventual consistency becomes more complex.\n- Upon a write to a node, an efficient way to communicate the write to other nodes is needed.\n- If other nodes are allowed to read data, the PACELC tradeoff must be considered.\n- Despite challenges, replication can be beneficial and there may be several reasons to replicate data across multiple locations."
}
{
  "chunk": "<p>Redundancy. Adding replicas allows for redundancy in case your primary instance fails. In this case, you're not necessarily reading from your replicas -- you're simply maintaining a hot standby in the event of failure. Thus, there are no eventual consistency implications (outside of failure scenarios) that are implicated solely due to redundancy. This is the default mode for MongoDB replicas, which direct all read requests to the primary node unless otherwise configured.</p><p>Increase throughput. A second reason to add replication is to increase your processing throughput. In this case, you may direct all write operations to your primary node but reads can go to your secondary nodes. This is the approach used when adding read replicas to your relational database. By increasing the nodes available for read traffic, you're able to handle more requests per second. However, you do add the eventual consistency issues discussed in this post.</p>",
  "neutralised_chunk": "- Redundancy: Adding replicas allows for redundancy in case the primary instance fails. Replicas serve as hot standby backups, without being actively read from (except during failure scenarios). This does not introduce eventual consistency implications on its own. This is the default mode for MongoDB replicas, where all read requests go to the primary node unless configured otherwise.\n\n- Increase throughput: Replication can be used to increase processing throughput. Write operations go to the primary node, but reads can be distributed across secondary nodes. This approach, similar to adding read replicas in relational databases, increases the available nodes for read traffic and allows handling more requests per second. However, it introduces the eventual consistency issues discussed in the post."
}
{
  "chunk": "<p>Get data closer to users. A third reason for replication is to move data closer to your users. Your users may be global, but often your database is in a central location. If that's the case, the network latency to your database is likely the slowest part of a request. By globally distributing your data, you can vastly reduce client-side latency. This is the approach taken by Fly.io with global Postgres. This does implicate eventual consistency issues as well.</p><p>Note that these reasons aren't mutually exclusive! All three of these are at play with DynamoDB. Adding replicas in different availability zones enhances availability through redundancy. Because each replica can receive read requests, they also increase throughput. Finally, if you use DynamoDB Global Tables, you can put your data closer to your users.</p>",
  "neutralised_chunk": "- Replication is used to bring data closer to global users, reducing network latency - this might introduce eventual consistency issues.\n- Users may be global while the database could be centralized, causing slow requests due to network latency.\n- Globally distributing data can significantly reduce client-side latency - this is the method Fly.io uses with global Postgres.\n- The reasons for replication aren't mutually exclusive.\n- In DynamoDB, all three reasons are relevant: additional replicas in different availability zones increase availability and throughput, and global tables put data closer to users."
}
{
  "chunk": "<h3>Eventual consistency is not (only) about CAP\u200b</h3><p>When people think about eventual consistency, they often jump straight to the CAP theorem. And the CAP theorem is relevant when thinking about eventual consistency. If you choose an AP system, you probably have some process to reconcile the data on disparate nodes once the network partition is healed.</p><p>But the CAP theorem is limited. It only applies during times of failure and only during a specific type of failure -- network partitions between nodes. (See this post for more on when CAP applies).</p><p>Most of the time, your application is going to be running as intended. During that time, if you're allowing reads from your secondary nodes, you have to think about a different tradeoff -- one between latency and consistency. This is the second half of the PACELC theorem, which I discussed further here.</p>",
  "neutralised_chunk": "- Eventual consistency is often associated with the CAP theorem, which is relevant but not the only consideration.\n- The CAP theorem applies under specific conditions - failures and specifically network partitions between nodes.\n- In normal operational times, there is a different trade-off to consider - latency versus consistency.\n- This trade-off is part of the PACELC theorem."
}
{
  "chunk": "<p>Let's think about this with a quick example. Imagine you have a three-node system. Node A is the primary and handles the writes, but reads can be handled by any of the nodes.</p><p></p><p>When a write comes to your primary node, you have a range of options on how to handle it.</p><p>You could require that all three nodes acknowledge durably committing the update before acknowledging the write to the client. This gives you the strongest consistency, as all writes must be agreed upon by all nodes before accepting. However, it also increases latency, as you need to communicate with these remote nodes and wait for the slowest one to respond. You can think of this as comparable to a RDBMS with synchronous replication to read replicas.</p>",
  "neutralised_chunk": "- Consider a three-node system with Node A as the primary node that handles the writes; all nodes can handle reads.\n- Different choices exist for handling a write to the primary node.\n- Option one is to wait for acknowledgement from all three nodes before confirming the write to the client, providing the highest consistency.\n- This option increases latency due to the time taken to interact with remote nodes and waiting for the slowest response.\n- This scenario can be compared to an RDBMS with synchronous replication to read replicas."
}
{
  "chunk": "<p>On the other end of the spectrum, you could only commit the write to the primary node before acknowledging to the client. The primary node could then send the updates to the replica nodes asynchronously. This has much lower write latency for clients at the cost of more data inconsistencies on reads. This is more comparable to asynchronous read replicas with a relational database or with MongoDB's writeConcern: 1 setting.</p><p>More in the middle is something like MongoDB's writeConcern: \"majority\" setting, where the write must be committed to a majority of nodes before returning.</p><p></p><p>In reviewing the different flavors of eventual consistency below, we'll talk about where each one fits on the spectrum.</p>",
  "neutralised_chunk": "- For writes, one option is to commit only to the primary node before acknowledging the client, with updates sent asynchronously to replica nodes.\n- This approach has lower write latency for clients but higher risk of data inconsistencies on reads.\n- It is similar to asynchronous read replicas in relational databases or MongoDB's writeConcern: 1 setting.\n- Another option is MongoDB's writeConcern: \"majority\" setting, where the write must be committed to a majority of nodes before returning.\n- This is a middle ground between the two extremes of consistency and latency.\n- The different flavors of eventual consistency in DynamoDB will be discussed in terms of where they fall on this spectrum."
}
{
  "chunk": "<h3>Eventual consistency is (usually) about reads\u200b</h3><p>A final point to keep in mind is that eventual consistency is almost always a read problem but not a write problem.</p><p>Most replicated databases use a primary-replica setup where any piece of data has a primary node and one or more replica nodes. The primary node is responsible for all write operations and will send updates to the replica. This means you don't have an eventual consistency issue when writing data -- you'll always be operating against the latest version of the data when writing.</p><p>This is important later on when we discuss strategies for dealing with eventual consistency.</p><p>That said, some databases do allow for a multiple primary setup. Amazon Aurora has multi-master clusters where any of the instances can handle write operations. Similarly, DynamoDB Global Tables are essentially a multi-primary system where each enabled region has a primary node.</p><p>Using a multi-primary system increases complexity around eventual consistency and can lead to subtle bugs. We'll talk about this later.</p>",
  "neutralised_chunk": "- Eventual consistency is typically a challenge with reading data, not writing it.\n- Most replicated databases use a primary-replica setup with a primary node responsible for all write operations and one or more replica nodes.\n- Since the primary node updates replicas, there is no eventual consistency issue when writing data.\n- This point is relevant when discussing strategies to handle eventual consistency.\n- Some databases, like Amazon Aurora and DynamoDB Global Tables, allow for a multi-primary setup where any instance can handle write operations.\n- A multi-primary system increases the complexity around eventual consistency and can potentially lead to subtle bugs. This will be discussed later."
}
{
  "chunk": "<h3>Eventual consistency on your main table\u200b</h3><p>First, let's talk about eventual consistency on your main table.</p><p>Quick sidebar: DynamoDB tables have a primary key that uniquely identify each item. The DynamoDB API strongly pushes you toward using the primary key for your data access. But often you have items that need to be accessed in multiple distinct ways. With secondary indexes, you accomodate this. You can create secondary indexes with a different primary key, and DynamoDB will replicate your data into the secondary index with this new primary key structure. Finally, note that you can only make reads against your secondary index -- all writes must go through your main table.</p><p>Your main table is an interesting case in DynamoDB as you have the option between using strongly consistent reads or eventually consistent reads. To understand why, let's understand DynamoDB's underlying infrastructure and what happens when a write request comes in.</p>",
  "neutralised_chunk": "- The discussion begins with eventual consistency on the main table in DynamoDB.\n- DynamoDB tables have a primary key to uniquely identify each item which is heavily used for data access in the DynamoDB API.\n- Secondary indexes are created when items need to be accessed in various ways. \n- These indexes have a different primary key, with DynamoDB replicating data into the secondary index based on this new key structure. \n- Only reads can be performed against the secondary index; all writes need to occur through the main table. \n- The main table in DynamoDB allows either strongly consistent reads or eventually consistent reads. \n- Explanation requires understanding of DynamoDB's underlying structure and the process of handling a write request."
}
{
  "chunk": "<p>First, you should know that, under the hood, DynamoDB segments your data onto multiple partitions. DynamoDB will assign a record to a specific partition based on the partition key of your table, and each record belongs to one and only one partition.</p><p></p><p>But that's not the end of the story. Each partition includes three nodes: a primary and two replicas. The primary node is the only one that can handle writes, but any of the replicas can serve reads.</p><p></p><p>When a write comes to a DynamoDB table, the request router will use the partition key to direct it to the primary node for the given key. The primary node will tentatively apply the write if any write conditions are met. Then it will submit the write to both replica nodes. As soon as one of the replicas responds with a successful write, the primary node will return a successful response to the client.</p>",
  "neutralised_chunk": "- DynamoDB stores data across multiple partitions, with records assigned to a specific partition based on the partition key.\n- A table record belongs only to one partition.\n- Each partition comprises three nodes: a primary and two replicas.\n- The primary node is responsible for taking care of writes while any node may handle reads.\n- A write request in DynamoDB gets routed to the primary node using the partition key.\n- The primary node applies the write if conditions are met and submits the write to both replica nodes.\n- Once one of the replicas confirms a successful write, the primary node sends a successful response to the client."
}
{
  "chunk": "<p>By ensuring that a majority of the nodes have committed the write, DynamoDB is increasing the durability of the write. Now, even if the primary node fails immediately after the write request, it has been durably committed to another node to preserve the write.</p><p>You can also see where the potential for an inconsistent read comes in. Imagine our write request is accepted after the primary and Replica A have recorded it. When a read request comes in, it could go to our primary or either of the two replicas. If it chooses the lagging Replica B, there's a possibility we could receive a stale read that does not include the latest updates to the item.</p>",
  "neutralised_chunk": "- DynamoDB ensures write durability by requiring a majority of nodes to commit the write.\n- If the primary node fails after the write, the write is preserved on another node.\n- Inconsistent reads can occur if a read request goes to a replica that has not yet received the latest write.\n- When a write is accepted by the primary and one replica, a read request to the other lagging replica could return stale data without the latest updates."
}
{
  "chunk": "<p>In effect, DynamoDB chooses an intermediate position on the latency-consistency continuum. It doesn't go for strong consistency by durably writing to all nodes, but it also doesn't take latency to the lowest limit by writing to only one node. Part of this is due to the durability benefits of writing to multiple nodes, but it's relevant nonetheless.</p><p></p><p>Two final notes to wrap up this section.</p><p>First, while a stale read is possible on your main table, it's fairly unlikely. Remember that DynamoDB will randomly choose one of the three nodes to read from to handle a read request. Because a write will be committed to two of the three nodes before being acknowledged, this means there's a 66% chance you'll hit one of the \"strongly consistent\" nodes anyway.</p>",
  "neutralised_chunk": "- DynamoDB chooses a middle ground on the latency-consistency spectrum.\n- It neither ensures strong consistency by writing to all nodes nor minimizes latency by writing only to one node.\n- The choice is partly motivated by the durability benefits of writing to multiple nodes.\n- A stale read on the main table in DynamoDB is possible but unlikely.\n- As DynamoDB randomly selects one of the three nodes for a read request and a write is committed to two of the three nodes before it is acknowledged, there's a 66% chance of hitting a \"strongly consistent\" node."
}
{
  "chunk": "<p>Further, the lagging third node probably isn't that far behind in the normal case. The primary sent the write operation to both replicas at the same time. While it won't wait for the second replica to respond, it's probably milliseconds behind at most. This is exactly what I found in some basic testing as well.</p><p>Second, while I've been talking about your main table in this section, the same principles apply to local secondary indexes. Some of the nuances around local secondary indexes require another post, but items in a local secondary index are kept on the same partition as the items in the main table and thus have the same replication behavior.</p>",
  "neutralised_chunk": "- The third node in the replication process is likely only milliseconds behind, based on concurrent sending of the write operation to both replicas.\n- The same principles discussed for the main table also apply to local secondary indexes.\n- Items in a local secondary index are housed on the same partition as those in the main table, leading to identical replication behavior.\n- Detailed nuances of local secondary indexes warrant a separate discussion."
}
{
  "chunk": "<h3>Eventual consistency on global secondary indexes\u200b</h3><p>Now that we've covered the standard case with our main table, let's move on to the second flavor of eventual consistency -- that of global secondary indexes.</p><p>Unlike your main table or local secondary indexes, you cannot opt in to strongly consistent reads for your global secondary indexes. To understand why, we'll again look to the infrastructure underlying the implementation.</p><p>We talked in the previous section about how DynamoDB partitions your data using the partition key of your data. When you create a global secondary index on your table, DynamoDB creates completely separate partition infrastructure to handle your global secondary index:</p><p></p><p>Note that this separate infrastructure for secondary indexes is different than how other NoSQL databases handle it. With MongoDB, for example, additional indexes are implemented on each of the existing shards holding your data.</p>",
  "neutralised_chunk": "- Eventual consistency in global secondary indexes.\n- Unlike main tables and local secondary indexes, strongly consistent reads cannot be opted for global secondary indexes.\n- The reason lies in the underlying infrastructure implementation.\n- DynamoDB partitions data using the partition key.\n- For a global secondary index, DynamoDB creates separate partition infrastructure.\n- This is different from how other NoSQL databases like MongoDB handle additional indexes, which are implemented on existing shards holding the data."
}
{
  "chunk": "<p>Why does DynamoDB place global secondary indexes on new partitions? It all flows from DynamoDB's laser focus on consistent, predictable queries.</p><p>DynamoDB wants your response times to be consistent no matter the size of your database or the number of concurrent queries. One key way it does this is via its partitioning scheme using the partition key so that it can quickly find the right ~10GB partition that holds your requested data.</p><p>But a global secondary index can have a different partition key than your main table. If DynamoDB didn't reindex your data, then a query to a global secondary index would require querying every single partition in your table to answer the query. This is exactly what MongoDB does -- if you have a query that does not use the shard key on your collection, it will broadcast to every shard in your cluster.</p>",
  "neutralised_chunk": "- DynamoDB places global secondary indexes on new partitions to maintain consistent and predictable query performance.\n- DynamoDB aims for consistent response times, regardless of database size or concurrent query load.\n- It achieves this through partitioning using the partition key, enabling quick access to the relevant ~10GB partition for requested data.\n- However, global secondary indexes can have different partition keys than the main table.\n- Without reindexing, a query on a global secondary index would require scanning every partition in the table, which is inefficient.\n- This is the approach used by MongoDB, where queries not using the shard key must broadcast to all shards in the cluster.\n- By reindexing data into new partitions for global secondary indexes, DynamoDB can maintain consistent and predictable query performance."
}
{
  "chunk": "<p>Ok, so we know the choice DynamoDB makes in placing your global secondary indexes on separate partitions. Why does this mean we can't get strong consistency from these separate partitions?</p><p>Recall our tradeoff between latency and consistency. To get strong consistency from a given node, we must ensure the write is committed to the node before the write is acknowledged. Thus, our table would need to not only write to the primary and one replica of our main table, but also two nodes for each global secondary index as well!</p><p>With one global secondary index, that might not sound so bad -- what's a few milliseconds between friends? But you can have up to 20 global secondary indexes by default, and you can even request a limit increase to have more!</p><p>You don't want your writes hung up on acknowledgements from 10+ nodes, each of which has their own provisioned capacity and instance failures. This would vastly reduce the benefit of global secondary indexes.</p>",
  "neutralised_chunk": "- DynamoDB places global secondary indexes on separate partitions, which affects consistency.\n- To achieve strong consistency from a node, the write must be committed before being acknowledged.\n- This would require writing to the primary table, its replica, and two nodes for each global secondary index.\n- With one global secondary index, the additional latency might be acceptable.\n- However, DynamoDB allows up to 20 global secondary indexes by default, and more can be requested.\n- Waiting for acknowledgments from 10+ nodes, each with its own capacity and potential failures, would significantly reduce the benefit of global secondary indexes.\n- Hence, strong consistency across global secondary indexes is not feasible due to the introduced latency."
}
{
  "chunk": "<p>Instead, DynamoDB uses asynchronous replication to global secondary indexes. When a write comes in, it is not only committed to two of the three main table nodes, but it also adds a record of the operation to an internal queue. At this point, the write can be acknowledged to the client. In the background, a service is processing that queue to update the global secondary indexes.</p><p></p><p>Notice where global secondary indexes fall on the latency-consistency spectrum. They optimize heavily for write latency at the expense of consistency.</p><p></p><p>Further, and more importantly, notice the difference in flavor of eventual consistent reads on your main table as compared to reads from your global secondary index. The replication lag on your main table is likely to be barely perceptible in the normal course of business, whereas the replication lag for global secondary indexes will be more noticeable.</p>",
  "neutralised_chunk": "- DynamoDB uses asynchronous replication for global secondary indexes.\n- A write operation is committed to two of the three main table nodes and adds a record to an internal queue.\n- The write is then acknowledged to the client.\n- A background service processes the queue to update the secondary indexes.\n- Global secondary indexes prioritize write latency over consistency.\n- The difference in eventual consistent reads between the main table and the global secondary index is noted.\n- The replication lag on the main table is typically minimal, while the lag for global secondary indexes is more noticeable."
}
{
  "chunk": "<p>Finally, the replication lag on global secondary indexes can be influenced by your own actions. Global secondary indexes have capacity that is separate from your main table. If you don't provision enough capacity, or if you exceed partition throughput limits for your index, the lag can be even longer. This is different from your main table where exceeding provisioned capacity or partition throughput limits will result in an error on the write request itself.</p>",
  "neutralised_chunk": "- Global secondary indexes in DynamoDB have separate capacity from the main table.\n- If insufficient capacity is provisioned for the global secondary index, or if partition throughput limits are exceeded, replication lag can increase.\n- This behavior differs from the main table, where exceeding provisioned capacity or partition throughput limits results in an error on the write request itself."
}
{
  "chunk": "<h3>Eventual consistency on DynamoDB Global Tables\u200b</h3><p>When I first conceived of this post, I was going to discuss the Two Flavors of Eventual Consistency in DynamoDB, as I wanted to make the key point about the difference in replication lag between your main table and global secondary indexes. But as I started to write, I realized there's a third flavor of eventual consistency in DynamoDB -- that of cross-region replication using DynamoDB Global Tables.</p><p>As a quick summary: DynamoDB Global Tables are a fully managed way to replicate your tables across multiple regions. This can be used for both the redundancy reason for replication, as you have some resiliency in the face of region failure, as well as the \"get data closer to users\" reason, as users can be routed to the region nearest them.</p>",
  "neutralised_chunk": "- Eventual consistency in DynamoDB Global Tables.\n- The original plan was to discuss the two types of eventual consistency in DynamoDB, highlighting the difference in replication lag between main tables and global secondary indexes.\n- However, a third type of eventual consistency was identified - cross-region replication using DynamoDB Global Tables.\n- DynamoDB Global Tables provide a managed way to replicate tables across multiple regions.\n- Benefits of Global Tables include redundancy and resiliency against region failures, as well as bringing data closer to users by routing them to the nearest region."
}
{
  "chunk": "<p>In a way, the infrastructure for DynamoDB Global Tables is similar to the infrastructure for global secondary indexes. When a write is received in one region, it is durably committed to two of the three nodes on the main table for the region and the write is acknowledged to the client. Then, it is asynchronously replicated to separate infrastructure. Rather than being a different set of partitions in the same region, the target infrastructure is a separate table in a different region.</p><p></p><p>Similar to global secondary indexes, Global Tables optimize for latency on the latency-consistency spectrum. However, there are two additional notes to consider as compared to global secondary indexes.</p>",
  "neutralised_chunk": "- DynamoDB Global Tables infrastructure is similar to that of global secondary indexes.\n- When a write is received in one region, it is durably committed to two of the three nodes on the main table for the region, and the write is acknowledged to the client.\n- The write is then asynchronously replicated to separate infrastructure, which is a separate table in a different region, unlike global secondary indexes where it is a different set of partitions in the same region.\n- Like global secondary indexes, Global Tables optimize for latency on the latency-consistency spectrum.\n- However, there are two additional considerations compared to global secondary indexes."
}
{
  "chunk": "<p>First, the replication latency to Global Tables is likely to be longer than that to global secondary indexes. Regions are significantly further apart than instances in the same datacenter, and network latency starts to dominate. P99 latencies across regions can easily be 100 - 200 milliseconds, so you should anticipate your replication being affected accordingly.</p><p>Second, using Global Tables introduces write-based consistency issues into your application. I mentioned earlier that eventual consistency is mostly a read-based problem, but that's not the case with Global Tables. You can write to both regions, and writes will be replicated from one region to another.</p><p>This can result in a few different problems in your application, which we'll talk about in the next section.</p>",
  "neutralised_chunk": "- Global Tables in DynamoDB have longer replication latency compared to global secondary indexes due to the greater geographical distance between regions, leading to higher network latency. P99 latencies across regions can easily reach 100-200 milliseconds, impacting replication times.\n\n- Using Global Tables introduces write-based consistency issues into the application, whereas eventual consistency is primarily a read-based problem in DynamoDB.\n\n- With Global Tables, writes can occur in both regions, and these writes are replicated between regions, potentially causing various problems in the application, which will be discussed in the next section."
}
{
  "chunk": "<h2>Dealing with the effects of eventual consistency\u200b</h2><p>Now that we know the different types of eventual consistency in DynamoDB, let's talk about some strategies for dealing with eventual consistency. As always, you must understand the requirements of your application to determine which strategies work best for you.</p>",
  "neutralised_chunk": "- Discussing strategies for dealing with DynamoDB's eventual consistency.\n- Understanding the requirements of your application is key to selecting the right dealing strategies."
}
{
  "chunk": "<h3>Know the general latency characteristics for your flavor of eventual consistency\u200b</h3><p>As discussed above, there are three flavors of eventual consistency in DynamoDB, and these flavors have very different replication lags. You may find that, practically, you rarely see the effects of eventual consistency in your application.</p><p>A common preference around consistency is something called \"read your writes\". This basically says that if a process makes a write and later performs a read, that read should reflect the previous write. It doesn't offer guarantees about reads from other processes, but it at least offers some sanity for a single workflow.</p>",
  "neutralised_chunk": "- Understanding the general latency characteristics of DynamoDB's eventual consistency models is important.\n- DynamoDB has three flavors of eventual consistency with varying replication lags.\n- In practice, the effects of eventual consistency may rarely be seen in an application.\n- A common consistency preference is \"read your writes,\" which ensures that a process's read reflects its previous write.\n- \"Read your writes\" does not guarantee consistency for reads from other processes, but provides sanity for a single workflow."
}
{
  "chunk": "<p>If your database allows you to direct operations to specific nodes, then a process can handle this by reading from the same server from which it wrote. With a load-balanced NoSQL database like DynamoDB, you don't get a choice from which server you read from. However, we can get a sense of how likely it is that I will get \"read your writes\" behavior from our different flavors of eventual consistency.</p><p>I did some basic testing of \"read your writes\" behavior against your main table and a global secondary index. You can read the full results and methodology in the GitHub repo here, but the high-level takeaways are:</p><p>As you can see, even an eventually consistent read against a main table is going to get darn close to 'read your write' consistency in the vast majority of cases.</p>",
  "neutralised_chunk": "- Operations can be directed to specific nodes if the database allows.\n- In a load-balanced NoSQL database like DynamoDB, the server from which data is read cannot be chosen.\n- The likelihood of achieving \"read your writes\" behavior with different types of eventual consistency can be estimated.\n- Basic testing was done on \"read your writes\" behavior against a main table and a global secondary index.\n- Complete results and methodology are available on a GitHub repository.\n- Even with eventual consistency, a read against a main table will largely yield 'read your write' consistency in most cases."
}
{
  "chunk": "<h3>... but don't count on the general latency characteristics!\u200b</h3><p>While understanding the general characteristics is useful, you shouldn't rely on this. As Ben Kehoe notes:</p><p>Ask me in person sometime about the ticket our devs opened after they built in an assumption of a maximum time for eventual consistency on DynamoDB</p><p>The tests above are very basic tests in standard operation. You shouldn't expect these results in all scenarios.</p><p>I would expect the main table results to be more bounded than the global secondary index results, given that you still need 2 out of 3 to commit the write, and a flapping third node is likely to be replaced quickly if needed. In fact, a new paper on DynamoDB talks about how DynamoDB uses log-only replicas to keep write latency low and durability high in the event of partition failures.</p><p>A global secondary index, on the other hand, has more potential failure modes, including ones of your own making (underprovisioned throughput or exceeding partition throughput limits).</p>",
  "neutralised_chunk": "- Understanding general latency characteristics is useful but shouldn't form a basis for reliance.\n- Anecdote from Ben Kehoe suggests unexpected issues may arise from assumptions about maximum time for eventual consistency in DynamoDB.\n- Simple tests performed don't guarantee similar results in all situations.\n- It's more likely for main table results to be more stable than global secondary index results, owing to minimum node requirements for writes.\n- A recent study on DynamoDB reveals the use of log-only replicas to maintain low write latency and high durability, especially during partition failures.\n- However, global secondary indexes present more potential failure modes, like underprovisioned throughput or exceeding partition throughput limits."
}
{
  "chunk": "<h3>Use write conditions to guarantee consistency\u200b</h3><p>As mentioned in the background, eventual consistency is mostly a read-based problem (check the next point for a caveat here). All writes to DynamoDB are happening against the latest version of the item. When making a write, you can include a ConditionExpression in your operation that must evaluate to True in order for the write to process.</p><p>The canonical example of potential problems with eventual consistency is a bank transaction -- what happens if I read a bank balance that is out of date, then approve a transaction based on the stale balance?</p>",
  "neutralised_chunk": "- Write conditions in DynamoDB can ensure consistency.\n- Eventual consistency largely affects read-operations, while all writes are done against the latest version of the item.\n- A ConditionExpression can be included when writing, which needs to be true for the write to proceed.\n- Potential issues with eventual consistency can arise in scenarios like bank transactions where actions can be based on potentially out-dated information."
}
{
  "chunk": "<p>You can avoid this problem with the use of ConditionExpressions. Your ConditionExpression could require that the balance of the account is greater than the amount of the requested transaction in order to proceed. You can even combine multiple operations, each with their own ConditionExpressions, into a DynamoDB transaction in order to make these changes to multiple bank accounts with ACID semantics.</p><p>Amazingly, you're not necessarily going to avoid these issues with a relational database! Todd Warszawski and Peter Bailis have a fun paper showing how isolation levels below serializable can result in concurrency bugs during transactions. Peter Bailis and others have another fun paper showing how ORMs implement concurrency-control mechanisms that fail in subtle ways. I'm not saying this to scare you or raise FUD, but more to iterate that you really need to understand your infrastructure and your dependencies to avoid these issues altogether.</p>",
  "neutralised_chunk": "- The use of ConditionExpressions can prevent certain problems.\n- A ConditionExpression could ensure the account balance exceeds the transaction amount to proceed.\n- Multiple operations with their own ConditionExpressions can be combined into a DynamoDB transaction for ACID compliance across multiple accounts.\n- Even relational databases may not avoid these issues.\n- Todd Warszawski and Peter Bailis discuss how lower isolation levels can lead to concurrency bugs during transactions.\n- Peter Bailis and colleagues also illustrate how ORMs can subtly fail in implementing concurrency-control mechanisms.\n- It's crucial to understand one's infrastructure and dependencies to avoid such issues."
}
{
  "chunk": "<p>Write conditions won't save you from all your problems with eventual consistency, but they can help you avoid persisting data in inconsistent or invalid states. Look into strategies like pessimistic concurrency (aka locking), optimistic concurrency (aka version checking), or simply asserting data constraints in your write operations.</p>",
  "neutralised_chunk": "- Write conditions in DynamoDB can help prevent data from being saved in inconsistent or invalid states.\n- They cannot solve all problems related to eventual consistency.\n- Approaches such as pessimistic concurrency (locking), optimistic concurrency (version checking), or asserting data constraints during write operations could be used."
}
{
  "chunk": "<h3>Avoid multi-region writes where possible\u200b</h3><p>Let's address the caveat from the previous section. While eventual consistency is mostly a read problem, it can become a write problem if you're using DynamoDB Global Tables. Now you have multiple regions, each able to accept writes that are asynchronously replicated to other regions.</p><p>There are two sources of potential issues with writing to multiple regions.</p><p>First, you could lose writes that occur nearly simultaneously. Imagine a write in Region A that updates attribute X, while a concurrent write in Region B updates attribute Y. In theory, both of these writes can succeed, and DynamoDB will do its best to reconcile concurrent writes. However, you could find yourself losing updates in specific situations.</p>",
  "neutralised_chunk": "- Avoid multi-region writes with DynamoDB Global Tables when possible.\n- Eventual consistency is primarily a read issue, but can become a write issue with Global Tables across multiple regions.\n- Writing to multiple regions has two potential issues:\n  - Potential loss of nearly simultaneous writes updating different attributes.\n  - Possibility of overwrites if the same attribute is updated concurrently across regions.\n- DynamoDB tries to reconcile concurrent writes, but updates can be lost in specific situations."
}
{
  "chunk": "<p>Second, you could assert write conditions that are true in your region's copy of the data but that are false in another region's (yet-to-be-replicated) copy. Condition expressions and DynamoDB Transactions are consistent in a single region only, so writing in multiple regions can add problems.</p><p>When using DynamoDB Global Tables, I generally recommend users only write an item in a particular region. For many, that means choosing a \"write\" region and directing all writes there, even if the latency is higher. For some use cases, individual items may naturally be correlated with specific regions and thus you can ensure that each write will only ever be written in a single region.</p>",
  "neutralised_chunk": "- Write conditions that are true in one region may not be true in another region due to unrepllicated data. This can cause issues when using condition expressions and DynamoDB Transactions across multiple regions.\n- When using DynamoDB Global Tables, it is recommended to write an item in only one region.\n- This may require selecting a dedicated \"write\" region and directing all writes there, regardless of higher latency.\n- For certain use cases, items may naturally align with specific regions, thereby ensuring each write occurs in only one region."
}
{
  "chunk": "<h2>Conclusion\u200b</h2><p>In this post, I hope we dispelled some of your fear about eventual consistency. First, we learned some background about eventual consistency generally. Then, we looked at the three flavors of eventual consistency in DynamoDB. As part of this, we developed our mental models of DynamoDB's underlying infrastructure to understand why the three flavors are different. Finally, we looked at some strategies for dealing with eventual consistency in DynamoDB.</p><p>If you have any questions or corrections for this post, please leave a note below or email me directly!</p>",
  "neutralised_chunk": "- The aim of the post was to alleviate fears regarding eventual consistency.\n- The content covered background information on eventual consistency.\n- The three types of eventual consistency in DynamoDB were discussed.\n- This discussion helped clarify the differences among the three types and developed an understanding of DynamoDB's infrastructure.\n- Strategies for managing eventual consistency in DynamoDB were presented.\n- The author encouraged readers to reach out with any questions or corrections."
}
{
  "chunk": "<h1>GraphQL, DynamoDB, and Single-table Design</h1><p>I've written and spoken a lot about data modeling with DynamoDB over the years. I love DynamoDB, and I feel like I understand its pros and cons pretty well.</p><p>Recently, the topic of the compatibility of GraphQL and single-table design in DynamoDB came up again, sparked by a tweet from Rick Houlihan that led to a follow-up tweet indicating that I should update my post about single-table design where I mentioned that GraphQL might be an area where you don't want to use single-table design with DynamoDB.</p><p>Twitter is a bad medium for nuance, and I think the question of whether to use single-table design with GraphQL is a nuanced one that depends a lot on why you are choosing to use GraphQL.</p>",
  "neutralised_chunk": "- The topic is about the compatibility between GraphQL and single-table design in DynamoDB.\n- The author has previously engaged in discussions about data modeling with DynamoDB.\n- The discussion about the compatibility of GraphQL and single-table design in DynamoDB was triggered by a tweet.\n- The subsequent tweet suggested an update about a previous post concerning single-table design where GraphQL's compatibility was questionable.\n- The decision to use single-table design with GraphQL depends on the reasons for using GraphQL.\n- The limitations of discussing nuanced topics like this on Twitter are recognized."
}
{
  "chunk": "<p>If you want the TL;DR Twitter version of this post, it is: You absolutely can use single-table design with GraphQL, and I know some very smart people who are doing so. However, I don't think it's wrong to chose not to, depending on your specific needs + preferences.</p><p>Further, I think Amplify made the right choice in opting for a table-per-model default.</p><p>Post-publishing note: if you want to see some good practical tips on how to use single-table design with AppSync + GraphQL, check out this video from Adam Elmore on his setup. He also has some good conceptual points on when he does (and doesn't) use single-table design principles in his application.</p><p>If you want the full details, check out the post below. We will cover:</p><p>Let's goooooo.</p>",
  "neutralised_chunk": "- Single-table design can be used with GraphQL, and some highly skilled developers are doing so.\n- However, opting out of single-table design is not wrong, depending on specific needs and preferences.\n- Amplify made the correct choice in defaulting to a table-per-model approach.\n- For practical tips on using single-table design with AppSync + GraphQL, check out Adam Elmore's video on his setup.\n- The video also covers conceptual points on when to use (or not use) single-table design principles.\n- The post will provide full details on the following topics:"
}
{
  "chunk": "<h2>What are the benefits of GraphQL?\u200b</h2><p>I want to start off by understanding the potential benefits of GraphQL in order to understand why people are choosing to use GraphQL.</p><p>Note that I refer to these as potential benefits. People may adopt GraphQL for all of these reasons, or there may be one or two reasons that are more important. Further, they may use GraphQL in a way that eschews one or more of the other potential benefits. I don't think these decisions are necessarily wrong -- it depends a lot on context and the particular needs of a team and product.</p><p>Finally, there are a few benefits of GraphQL that I don't mention here, such as the ability to specify the exact fields you want to save on network bandwidth. It doesn't mean these aren't important for some situations. Rather, they're not that relevant to the single-table vs. multi-table debate.</p>",
  "neutralised_chunk": "- Potential benefits of adopting GraphQL:\n\n- Ability to fetch only the required data, reducing over-fetching and under-fetching\n- Single endpoint for all data needs, simplifying client-side code\n- Strongly typed schema, improving development experience and enabling better tooling\n- Aggregating data from multiple sources in a single request\n- Evolving the API without versioning, enabling gradual product iteration\n- Shifting complexity from client to server, simplifying client-side code\n\n- The specific reasons for adopting GraphQL may vary based on context, team needs, and product requirements.\n- Some potential benefits, like network bandwidth savings, are not directly relevant to the single-table vs. multi-table debate."
}
{
  "chunk": "<h3>API type safety\u200b</h3><p>The first reason people like GraphQL is the type safety from the API.</p><p>Single-page applications make requests to a backend server to retrieve data to display in the application. On a shopping application, the requested data could be a list of products in a category or the contents of a customer's shopping cart. On GitHub, the requested data could be a GitHub repository or a list of issues for the repository.</p><p>In a standard JSON-based REST API, it's likely there is a schema for this data, at least theoretically. The problem comes in reality -- many frontend developers find that the returned data shape is unreliable, and developers need to implement lots of defensive coding to ensure they have the data properties they are expecting.</p>",
  "neutralised_chunk": "- API type safety is a key advantage of GraphQL.\n- Single-page applications request data from a backend server to display information.\n- In a shopping application, this could be product lists or shopping cart contents.\n- On GitHub, it could be repositories or issues for a repository.\n- REST APIs often have a data schema in theory, but in practice the data shape can be unreliable.\n- Frontend developers need to implement defensive coding to ensure expected data properties are present."
}
{
  "chunk": "<p>GraphQL helps with this problem by having a defined, typed schema that is available from the backend. Further, most GraphQL implementations will ensure that a response from the backend adheres to this schema before sending to the frontend. This makes it significantly easier as a frontend client of a GraphQL-based service as you have greater confidence over the returned data.</p>",
  "neutralised_chunk": "- GraphQL addresses data inconsistency issues by using a defined, typed schema provided by the backend.\n- Most GraphQL implementations verify that backend responses adhere to this schema prior to frontend delivery.\n- As a frontend client using a GraphQL-based service, this provides more confidence in the returned data."
}
{
  "chunk": "<h3>Fewer network requests from the frontend\u200b</h3><p>A second reason that people like GraphQL is to reduce the number of network requests from the frontend to the backend.</p><p>Let's return to our previous examples with a single-page application. If I'm implementing a shopping cart for my single-page application with a typical REST backend, I might need to make a number of calls to render the shopping cart page:</p>",
  "neutralised_chunk": "- Reducing network requests from the frontend is another reason for using GraphQL.\n- In a single-page application, rendering a shopping cart page with a REST backend may require multiple network calls:\n    - To fetch the cart items\n    - To fetch product details for each item\n    - To fetch any additional data needed"
}
{
  "chunk": "<p>This is the dreaded N + 1 problem. The N + 1 problem is commonly used as an argument against using ORMs as evidence they can make suboptimal queries that add additional load on your database. However, we're often making this same problem on the frontend! By using single-page applications with a RESTful backend implementation, our frontend needs to make a number of requests to render the page. Further, there may be a sequential nature to this, as I can't make the N calls to fetch the item details until my first request to retrieve the shopping cart contents. Now I have a waterfall of sequential requests that make my application seem sluggish.</p><p>GraphQL reduces this problem by allowing a frontend client to make a single request that retrieves a graph of data. In a single GraphQL query, I can retrieve both my shopping cart and all of the items in the cart. As soon as this request finishes, I can paint the entire page without waiting for subsequent requests.</p><p>Notice that this doesn't necessarily eliminate the N + 1 problem entirely. As we'll see below, it could move the N + 1 problem to the backend. The only thing GraphQL does here is eliminate the N + 1 problem from the frontend application and developer.</p>",
  "neutralised_chunk": "- The N + 1 problem is commonly associated with ORMs making suboptimal database queries, adding load.\n- However, single-page applications with RESTful backends also face this issue on the frontend.\n- The frontend needs to make multiple requests to render a page, often sequentially.\n- E.g., fetching shopping cart contents first, then making N calls to fetch item details.\n- This waterfall of sequential requests can make the application seem sluggish.\n- GraphQL reduces this problem by allowing a single request to retrieve a graph of data.\n- In one GraphQL query, both the shopping cart and all items can be fetched.\n- This eliminates the need for subsequent requests, allowing the entire page to render immediately.\n- GraphQL doesn't necessarily eliminate the N + 1 problem entirely, potentially moving it to the backend.\n- GraphQL eliminates the N + 1 problem for the frontend application and developer."
}
{
  "chunk": "<h3>Flexible querying patterns\u200b</h3><p>The third and final benefit of GraphQL for the point of this post is that GraphQL provides more flexible querying options for clients.</p><p>We saw above that GraphQL can eliminate the N + 1 problem from the frontend. Another alternative to reduce calls from the frontend to the backend is the Backends for Frontends (BFF) architecture pattern. With this pattern, backend APIs are implemented specifically for the frontends that will be calling them. For example, we may have an endpoint specifically for rendering our shopping cart page that assembles both the cart contents and updated details about the specific items on the backend before returning a complete response to the frontend.</p><p>Once fully implemented, the BFF pattern looks a lot like GraphQL. I can get the full graph of data I need in a single request rather than by making multiple calls to my backend.</p><p>The main difference between BFFs and GraphQL is in flexibility.</p>",
  "neutralised_chunk": "- Flexible querying patterns in GraphQL\n- GraphQL eliminates the N+1 problem from the frontend\n- Backends for Frontends (BFF) architecture pattern reduces calls from frontend to backend\n- BFF implements backend APIs specifically for frontends\n- Example: endpoint for rendering shopping cart assembles cart contents and item details before responding\n- Fully implemented BFF pattern resembles GraphQL, providing full data graph in single request\n- Main difference between BFFs and GraphQL is flexibility"
}
{
  "chunk": "<p>BFFs are designed specifically for my frontend experiences, and thus changing my frontend experience requires changing my backend as well. This can mean pulling in additional teams, dealing with backward compatibility issues, and other factors. As a result, BFFs can be slower to evolve.</p><p>In contrast, GraphQL has provides significantly more flexibility. With GraphQL, you publish a schema that describes the shape of your data. A client can make any request against that schema they need, and the GraphQL backend implementation will do the work to assemble the data. Thus, changing requirements on the frontend (usually) do not require backend changes as well. It's as simple as rewriting the GraphQL query for a page to change the desired data.</p><p>Generally, GraphQL backends use isolated, focused resolvers that retrieve specific pieces of data. This can result in N + 1 queries on the backend. For example, imagine our shopping cart query below:</p>",
  "neutralised_chunk": "- Backend for Frontends (BFFs) are tailored for specific frontend experiences; as such, altering the frontend requires backend changes, potentially requiring additional teams and managing backward compatibility.\n- BFFs might evolve slower due to these factors.\n- GraphQL, on the other hand, provides more flexibility.\n- With GraphQL, you define a schema representing your data structure.\n- Clients can then request any data fitting the schema, and the GraphQL backend coordinates its retrieval. This means frontend changes do not usually necessitate backend modifications; altering the GraphQL query is usually sufficient.\n- Normally, GraphQL backends use specific, isolated resolvers for data retrieval, which can lead to N + 1 queries.\n- Example of this given is a shopping cart query.\n"
}
{
  "chunk": "<p>The cart resolver would resolve details about the shopping cart overall, whereas details about the items would be handled by a separate resolver. On the backend, these would be handled sequentially -- after the cart resolver finishes, the items resolver would be called to fetch the items for the cart.</p>",
  "neutralised_chunk": "- The cart resolver manages details about the overall shopping cart.\n- A separate resolver handles details about the items.\n- On the backend, these resolvers work sequentially.\n- The items resolver is called to fetch the items after the cart resolver finishes."
}
{
  "chunk": "<h3>Key takeaways\u200b</h3><p>If you look closely at the benefits above, you'll notice that GraphQL makes life quite a bit easier for frontend developers. They don't have to be as defensive about the data they get back from an API. They don't have to make a waterfall of network requests and deal with partial errors. And they can iterate on the frontend without requiring backend assistance. Because the resolvers are implemented generically, independent of the originating query, it's easy to reshape a response simply by changing the GraphQL query input.</p>",
  "neutralised_chunk": "- GraphQL provides significant benefits for frontend developers:\n  - No need to be defensive about data received from the API\n  - Avoid waterfall of network requests and handling partial errors\n  - Ability to iterate on frontend without backend assistance\n  - Easy reshaping of responses by modifying GraphQL query input\n- Resolvers are implemented generically, decoupled from originating query"
}
{
  "chunk": "<p>Because of this, I've generally seen more GraphQL adoption in engineering teams where frontend developers have more sway. There is nothing inherently wrong with this. Most engineering teams have some inherent bias toward a particular type of developer and optimize their systems accordingly. I think REST APIs make life a lot easier for backend developers -- by simplifyng the amount of data on any particular request -- by pushing a lot of that complexity to clients (often the frontend developers).</p>",
  "neutralised_chunk": "- GraphQL adoption is more common in teams where frontend developers have more influence.\n- This is not inherently wrong, as engineering teams often have biases towards certain types of developers and optimize accordingly.\n- REST APIs tend to be favored by backend developers as they simplify data requests.\n- REST APIs push complexity to clients (often frontend developers) by limiting the data in individual requests."
}
{
  "chunk": "<h3>Background on DynamoDB\u200b</h3><p>I'm not going to go deep on the details of DynamoDB, but I do want to discuss some high-level points in order to help the discussion later.</p><p>DynamoDB is a NoSQL database from AWS. It optimizes for extreme predictability and consistency of performance. It wants to provide the exact same performance for a query when you have 1 MB of data as when you have 1 GB, 1 TB, or even 1 PB of data.</p><p>To do that, DynamoDB intentionally restricts how you can use it. It forces you to use a primary key for most of your access patterns. This primary key is a combination of a hash key and a B-tree to allow for fast, predictable performance when operating on an individual item or when retrieving a range of contiguous items. Because of this primary key structure, you should see single-digit millisecond response times on individual items regardless of your database size.</p>",
  "neutralised_chunk": "- While not going into depth on DynamoDB, some high-level features need to be discussed.\n- DynamoDB is a NoSQL database from AWS, designed for extreme predictability and performance consistency.\n- DynamoDB aims to offer the same performance for a query across all data sizes, from 1 MB to 1 PB.\n- DynamoDB limits usage by enforcing the use of a primary key for most access patterns.\n- The primary key includes a hash key and a B-tree for quick, consistent performance when operating on a single item or a range of items.\n- Due to this primary key structure, response times on individual items are typically in the single-digit milliseconds, irrespective of database size."
}
{
  "chunk": "<p>Additionally, DynamoDB removes certain features provided by other databases. You can't do JOINs in DynamoDB. You can't do aggregations across multiple records in DynamoDB. Both of these operations can operate on an unbounded number of items with unpredictable performance characteristics. DynamoDB will not let you write a bad query and will not provide features that allow you to do so.</p>",
  "neutralised_chunk": "- DynamoDB lacks some features found in other databases.\n- JOIN operations are not possible in DynamoDB.\n- Similarly, aggregations across multiple records are not feasible.\n- Both of these operations can have unpredictable performance on an unbounded number of items.\n- DynamoDB does not support potentially problematic queries and lacks features that could facilitate them."
}
{
  "chunk": "<p>Finally, DynamoDB has clear, strict limits in certain areas. It won't let you create an item over 400KB as a way to force you into smaller, more targeted items. More importantly, it has a limit on the number of concurrent reads and writes against a subset of your data. If you exceed that limit for an individual second, your request will be throttled and you'll need to try again. Again, this is in pursuit of consistency and predictability. It turns performance into a binary decision -- single-digit response that either succeeded or failed -- rather than having slowly degrading application performance under load.</p><p>Because of the choices made by the DynamoDB team in order to provide that consistency, you need to model your data differently. Most importantly, you have to think about how you will actually use your data.</p>",
  "neutralised_chunk": "- DynamoDB has strict limitations in certain areas, such as a 400KB maximum item size to encourage smaller, more focused items.\n- It also has a limit on concurrent reads and writes to a data subset - exceeding this limit results in request throttling.\n- These limits are enforced to uphold consistency and predictability, converting performance into either success or failure rather than degrading under load.\n- Due to these design choices for ensuring consistency, data must be modeled differently in DynamoDB.\n- Key consideration must be given to the actual usage of the data."
}
{
  "chunk": "<p>In a relational database, you design your data in an abstract, normalized way. Then you implement the queries and potentially add indexes to assist in performance.</p><p>With DynamoDB, this pattern is flipped. You consider your access patterns first, then design your table to handle those access patterns. Your data probably won't be normalized as it would in a relational database. It's going to be handcrafted for your specific requirements.</p><p>Let's think back to our GitHub repository example before. Because DynamoDB does not have joins, fetching a repository and the ten most recent issues in the repository would take at least two requests to the database if you had two different tables. Depending on how you modeled the primary keys for the tables, it could be two sequential requests, which will increase latency. We're back to the old N + 1 problem we discussed before (or at least the 1 + 1 problem :)).</p>",
  "neutralised_chunk": "- In relational databases, data is designed in an abstract, normalized manner, and queries/indexes are implemented for performance.\n- In DynamoDB, access patterns are considered first, and the table is designed accordingly to handle those patterns.\n- Data in DynamoDB is unlikely to be normalized as in relational databases; it is tailored for specific requirements.\n- For the GitHub repository example, fetching a repository and its 10 most recent issues from separate tables in DynamoDB would require at least two requests, as DynamoDB lacks joins.\n- Depending on the primary key modeling, it could involve two sequential requests, increasing latency.\n- This relates to the N+1 problem (or the 1+1 problem in this case)."
}
{
  "chunk": "<p>This is where single-table design comes in. If you know you have an access pattern where you need the repository and the ten most recent issues, you can assemble the items next to each other in a single DynamoDB table. Then, you can use a Query operation to fetch both the repository and the issue items in a single request. You've turned your N + 1 query into a single query, and you've done it in a way that doesn't burn through CPU like you may with a SQL join operation. (For more on single-table design in the context of one-to-many relationships, check out this post on how to model one-to-many relationships in DynamoDB).</p>",
  "neutralised_chunk": "- Single-table design can be employed when you need to access related data in a specific pattern, such as a repository and its ten most recent issues.\n- The related items can be stored together in a single DynamoDB table.\n- A Query operation can then fetch both the repository and issue items in a single request.\n- This approach transforms an N+1 query into a single query, avoiding excessive CPU usage that might occur with SQL join operations.\n- For more details on single-table design for one-to-many relationships, refer to the post on modeling one-to-many relationships in DynamoDB."
}
{
  "chunk": "<p>When I talk about single-table design, I'm primarily using it to mean using DynamoDB in a way that uses the Query operation to retrieve multiple, heterogenous items in a single request. You can also use a single table with DynamoDB in a way that never retrieves heterogenous items. I often do! However, in that case, the difference between using one table or multiple tables is less stark.</p>",
  "neutralised_chunk": "- Single-table design in DynamoDB refers to using the Query operation to retrieve multiple, heterogeneous items in a single request.\n- It is possible to use a single table without retrieving heterogeneous items, but in that case, the difference between one table or multiple tables is less pronounced.\n- The author often uses a single table without retrieving heterogeneous items."
}
{
  "chunk": "<h3>The case for (and against) single-table design with GraphQL\u200b</h3><p>Now that we know about the benefits of GraphQL and of single-table design, can we merge these two in a happy marriage?</p><p>It really depends on how invested you are in flexibility as a key benefit of using GraphQL. Remember that DynamoDB relies on designing for known access patterns, while one of GraphQL's benefits is the ability to quickly iterate on access patterns. By using single-table design, you are combining multiple entities in a single table to handle patterns where you need nested, related data in a single access pattern.</p><p>Earlier, we talked about how most GraphQL implementations use isolated, focused resolvers to fetch specific pieces of data. If a complex query with nested data is requested, the GraphQL will pass it to sequential resolvers that each focus on fetching their own data.</p>",
  "neutralised_chunk": "- Discussing the combination of GraphQL and single-table design benefits.\n- The suitability of this combination depends on the value placed on GraphQL's flexibility.\n- DynamoDB designs for known access patterns, while GraphQL allows for quick iteration on access patterns.\n- Single-table design merges multiple entities in one table for nested, related data access in a single pattern.\n- Earlier discussion referenced GraphQL implementations using isolated resolvers for fetching specific data.\n- In the case of a complex query with nested data, GraphQL will pass it to sequential resolvers, each focused on their own data."
}
{
  "chunk": "<p>However, this keeps the N + 1 problem we discussed in an earlier section. We moved this problem to the backend rather than the frontend, but we haven't eliminated it entirely. And even moving this to the backend can be a performance win, as the backend is closer to the database, so multiple queries can be faster there.</p><p>We can remove or at least significantly reduce the N + 1 problem if we want. Rather than writing focused GraphQL resolvers, we can write broader, more complex resolvers. A top-level resolver could \"lookahead\" at other bits of the query to see if nested data has been requested. If it has, the resolver could fetch that nested data as part of its request to the database. In a relational database, that probably means a JOIN in your query. In DynamoDB, that means utilizing the tenets of single-table design to fetch multiple, heterogenous items in a single request.</p>",
  "neutralised_chunk": "- The N + 1 problem, earlier discussed, remains albeit shifted from the frontend to the backend.\n- This shift can enhance performance due to the backend's closer proximity to the database, making multiple queries faster.\n- The N + 1 problem can be reduced, if desired, by writing broader, more complex GraphQL resolvers.\n- A top-level resolver can check other parts of the query to see if nested data has been requested. \n- If so, the resolver can fetch the nested data during its database request.\n- In a relational database, this probably requires a JOIN in your query.\n- In DynamoDB, this means the application of single-table design principles to fetch multiple heterogeneous items in a single request."
}
{
  "chunk": "<p>However, lookaheads in your GraphQL resolvers are controversial, and some would even go to the point of calling them an anti-pattern.</p><p>Marc-Andre Giroux, the author of Production-Ready GraphQL and the implementor of large, public GraphQL APIs at GitHub and Shopify, has the following to say about lookaheads:</p><p></p><p>Note his recommendation to avoid lookahead queries and ensure that your resolvers are focused and narrow. If you follow this advice, most of the advantages of single-table design in DynamoDB are gone. You should still think about how your data is accessed and design for that, but it will be on a much narrower scope. You will think about how specific entities will be accessed, but you won't think about collections of heterogenous entities.</p>",
  "neutralised_chunk": "- Using lookaheads in GraphQL resolvers is viewed as controversial and sometimes considered an anti-pattern.\n- Marc-Andre Giroux, known for implementing large, public GraphQL APIs and writing \"Production-Ready GraphQL,\" advises against lookahead queries.\n- He recommends keeping resolvers focused and narrow.\n- Following this advice tends to eliminate most benefits of single-table design in DynamoDB.\n- Data access and design should still be considered, but with a narrower scope.\n- One should consider how specific entities will be accessed, not collections of diverse entities."
}
{
  "chunk": "<h3>Making a choice\u200b</h3><p>Given the discussion above, does that mean you should or should not use single-table design with GraphQL?</p><p>It depends! Software is about tradeoffs, and you need to find the right tradeoffs for your application.</p><p>From my very unscientific research, I'd say the majority of the GraphQL community agrees with Marc-Andre Giroux's advice. You should use isolated, focused resolvers that only fetch a specific piece of data. This gives you the benefit of GraphQL's query flexibility while taking the downside of reduced performance.</p><p>My guess is that Marc-Andre's advice is partly based on his specific experience. He helped design public-facing GraphQL APIs for GitHub and Shopify. Because these are to be used by the public rather than primarily by internal applications, they need significantly more flexibility.</p>",
  "neutralised_chunk": "- Deciding on single-table design with GraphQL depends on trade-offs and application requirements.\n- The majority of the GraphQL community aligns with Marc-Andre Giroux's advice:\n  - Use isolated, focused resolvers that fetch specific data pieces.\n  - Benefit: GraphQL query flexibility.\n  - Downside: Reduced performance.\n- Giroux's advice may stem from his experience designing public-facing GraphQL APIs for GitHub and Shopify.\n- Public APIs require significantly more flexibility than internal applications."
}
{
  "chunk": "<p>Think of the resolver complexity in the GitHub GraphQL API if you implemented lookahead functionality with full query flexibility. I looked at the GitHub GraphQL schema, and the Repository object alone is immensely complex. I wanted to count all the different relations on the Repository object, but I gave up when I was at 11 and had only made it to discussionCategories.</p><p>Handling the variety of potential nested queries against a Repository would be near impossible. Even if your GraphQL API is smaller than the GitHub one, it can still be a lot of complexity to take on all the permutations of data.</p>",
  "neutralised_chunk": "- Implementing lookahead functionality with full query flexibility in the GitHub GraphQL API would result in significant resolver complexity.\n- The Repository object in the GitHub GraphQL schema is immensely complex.\n- Counting all the different relations on the Repository object becomes overwhelming, with 11 relations identified before reaching discussionCategories.\n- Handling the variety of potential nested queries against a Repository object would be near impossible.\n- Even for smaller GraphQL APIs, managing all the permutations of data can introduce a lot of complexity."
}
{
  "chunk": "<p>Additionally, note that your problem at this point is not with GraphQL and DynamoDB. Your problem is with GraphQL, period. Marc-Andre's advice is not for a specific DynamoDB implementation. It's for all GraphQL APIs, regardless of the database used. Whether you're using DynamoDB, Postgres, or Neo4J, most GraphQL APIs are making multiple, sequential requests to the database to handle a nested query.</p><p>That said, I have seen multiple smart people advocate for single-table design principles with GraphQL. Adam Elmore has done so on Twitter, and Rich Buggy has written a nice post about single-table design with GraphQL.</p><p>In these situations, I'm guessing most of the GraphQL usage comes from clients that they control. The access patterns are known and can be planned for. They're using GraphQL for the API type safety and the fewer network requests from the frontend, but they're less concerned about infinitely flexible APIs.</p>",
  "neutralised_chunk": "- The issue being discussed is not specific to DynamoDB and GraphQL, but rather with GraphQL itself.\n- The advice provided is applicable to all GraphQL APIs, regardless of the underlying database.\n- Most GraphQL APIs make multiple, sequential requests to the database to handle nested queries, whether using DynamoDB, Postgres, or Neo4J.\n- Some advocate for single-table design principles with GraphQL, as seen from Adam Elmore and Rich Buggy.\n- In these cases, the GraphQL usage is likely from controlled clients with known access patterns.\n- They leverage GraphQL for API type safety and fewer frontend network requests, but are less concerned about infinitely flexible APIs."
}
{
  "chunk": "<p>In this same vein, I saw a tweet recently describing BBC's usage of GraphQL from a QCon talk. The talk noted that GraphQL queries need to be registered in advance before they go live. This sounds like known, specific access patterns to me! I don't know whether the BBC is optimizing their resolvers in conjunction with that, but it can be a nice way to get some of the benefits of GraphQL without going all-in on flexible queries.</p><p>In sum, you need to decide what's more important to you -- system performance or flexibility. I can't tell you what's more important because it depends a lot on your context, team, and product.</p>",
  "neutralised_chunk": "- The tweet mentioned a QCon talk about BBC's usage of GraphQL.\n- The talk noted that GraphQL queries need to be registered in advance before going live.\n- This suggests predefined, known access patterns rather than fully flexible queries.\n- It's unknown if BBC optimizes their resolvers along with this approach.\n- This can provide some GraphQL benefits without fully embracing flexible queries.\n- Teams must weigh system performance versus flexibility based on context, team, and product needs."
}
{
  "chunk": "<h2>Did the Amplify team make a mistake in its implementation?\u200b</h2><p>Now that I've (hopefully) convinced you that single-table and multi-table are both acceptable choices with GraphQL, I do want to discuss a final question that spurred this whole discussion.</p><p>AWS Amplify is a toolkit on top of AWS AppSync that makes it easy to turn a GraphQL schema into a fully managed GraphQL server, complete with databases and even the resolver code (or code-like configuration). As part of its implementation, it creates a separate DynamoDB table for each object (denoted by an @model directive) in the GraphQL schema.</p><p>Given that single-table and multi-table are both acceptable, was it right for the Amplify team to push the defaults toward multi-table? I believe yes, for two reasons.</p>",
  "neutralised_chunk": "- The discussion is whether the Amplify team implemented their solution correctly.\n- Both single-table and multi-table models are considered acceptable choices with GraphQL.\n- AWS Amplify, a toolkit on top of AWS AppSync, turns a GraphQL schema into a fully managed GraphQL server with databases and resolver code.\n- Amplify creates a separate DynamoDB table for each object marked with an @model directive in the GraphQL schema.\n- The question is whether the default setting towards multi-table was justified, and the belief is that it was, for two reasons."
}
{
  "chunk": "<p>First, as mentioned above, isolated, focused resolvers appear to be the preferred option in the GraphQL community. It's not unanimous -- you can find examples of using lookaheads to enhance performance in GraphQL -- but that's the vibe I get.</p><p>Second, single-table design is playing on God-mode, and Amplify is designed to be a developer-friendly experience. Many people using Amplify are not building Amazon-scale services that require extreme performance. They're probably building MVPs and trying to iterate quickly to get traction. There may be a time for optimization later, but it's not on day 1.</p>",
  "neutralised_chunk": "- The GraphQL community generally favors isolated, focused resolvers, despite some examples of lookaheads enhancing performance.\n- Single-table design in Amplify offers significant capabilities, aligning with its aim for a developer-friendly experience.\n- Many Amplify users are not creating large-scale services and are mainly focused on developing MVPs and rapidly iterating.\n- Optimizations can be considered later in the project's life cycle, not at the start."
}
{
  "chunk": "<p>Further, nothing precludes you from using single-table design with AppSync. As discussed, Rick has shown how to write a single-table resolver, and Rich Buggy has written a great post on this. There's a bit of asymmetry here -- someone that knows single-table design is knowledgeable to opt out of the defaults, whereas someone completely new to DynamoDB would be baffled by single-table design and would likely drop without knowing that multi-table is an option.</p><p>None of this is to excuse complaints of the difficulty of learning single-table design or new things generally. But given that I believe multi-table is a valid option for AppSync, it's a defensible choice to default to that mode.</p><p>Finally, if we're picking nits, my complaint about Amplify is that it makes it too easy to accidentally implement patterns using an inefficient DynamoDB Scan operation, which can blow up once an application goes to production.</p>",
  "neutralised_chunk": "- Single-table design can be used with AppSync, as demonstrated by Rick's example of a single-table resolver and Rich Buggy's post on the topic.\n- There is an asymmetry where someone familiar with single-table design can opt out of the defaults, but a newcomer to DynamoDB may be confused by single-table design and unaware of the multi-table option.\n- While learning single-table design or new concepts can be challenging, multi-table is a valid option for AppSync, making it a defensible choice as the default.\n- A potential issue with Amplify is that it can make it too easy to accidentally implement inefficient DynamoDB Scan operations, which can cause problems when an application goes into production."
}
{
  "chunk": "<h2>Conclusion\u200b</h2><p>I hope this post has convinced you that both single-table and multi-table design are acceptable with GraphQL and DynamoDB. More broadly, I hope it's shown that GraphQL itself has a number of benefits and some downsides, and you need to pick what is important to you in your implementation.</p><p>If you have questions or comments on this piece, feel free to blast me on Twitter, leave a note below, or email me directly.</p>",
  "neutralised_chunk": "- The post emphasizes that both single-table and multi-table design are valid approaches with GraphQL and DynamoDB.\n- GraphQL itself has its pros and cons and it's crucial to select what matters most for the implementation.\n- Feedback, whether in form of questions, comments, or critique, can be shared through Twitter, comments on the post, or direct email."
}
{
  "chunk": "<h1>How you should think about DynamoDB costs</h1><p>Last week, someone emailed me to ask about a potential cost optimization mechanism in DynamoDB. More on the specifics of that situation below, but the basic point is they were thinking about adding some additional application and architectural complexity because they were concerned about high DynamoDB costs for a particular use case.</p><p>I responded the way I always respond for these requests -- \"have you done the math?\"</p><p>One of my favorite things about DynamoDB is that you can easily do the math when considering how much it will cost you. I use this all the time in a few different ways, from getting a rough guess at how much DynamoDB will cost for my application to deciding between different approaches to solving a specific access pattern.</p><p>You can and should think about DynamoDB costs as you're designing your model to understand feasibility and weigh tradeoffs.</p>",
  "neutralised_chunk": "- The topic is about understanding DynamoDB costs.\n- A concern was raised regarding high DynamoDB costs for a specific use case, leading to a consideration for increasing application and architectural complexity.\n- A common response to such queries is to ask if cost calculations have been done.\n- One of the advantages of DynamoDB is the ease of calculating potential costs.\n- This process can be used for understanding the rough cost for an application or for choosing among solutions for a specific access pattern.\n- DynamoDB costs should be considered during model design to understand feasibility and evaluate tradeoffs."
}
{
  "chunk": "<p>In this post, I'll walk through my approach to reasoning about DynamoDB costs. I'll start with an overview about how DynamoDB pricing works -- feel free to skip this if you're already familiar. From there, I'll walk through a few examples of how I use this to make decisions about DynamoDB costs.</p><p>This post isn't comprehensive on DynamoDB costs -- it's more about how to approach this and make cost modeling part of your design process. If you want some other tips on DynamoDB costs, check out Khawaja Sham's epic thread on patterns for saving costs with DynamoDB. It has a ton of wide-ranging thoughts on DynamoDB costs.</p><p>There also was a nice Reddit thread on DynamoDB costs yesterday. Some correct stuff, some incorrect stuff, but I was happy to see a few people mention you can just do the math.</p>",
  "neutralised_chunk": "- The post discusses the author's method for understanding DynamoDB costs.\n- Begins with an overview of DynamoDB pricing (readers can skip this if already familiar).\n- Includes examples of using this knowledge for decision-making regarding DynamoDB costs.\n- The post focusses more on approach and incorporating cost modeling in the design process than being a comprehensive source on DynamoDB costs.\n- Mentions further tips on DynamoDB costs are available in Khawaja Sham's thread discussing patterns for cost-saving with DynamoDB.\n- Cites a recent Reddit thread on DynamoDB costs as another source of information, noting a mix of both correct and incorrect content."
}
{
  "chunk": "<h2>How DynamoDB pricing works\u200b</h2><p>Let's start with a basic overview of how DynamoDB pricing works. You can get bogged down in the different pricing modes (on-demand vs. provisioned?) or table storage classes (Standard or Standard-IA?), but that is secondary to understanding the core dimensions on which DynamoDB charges.</p><p>Basically, DynamoDB is charging for three things:</p><p>Read consumption: Measured in 4KB increments (rounded up!) for each read operation. This is the amount of data that is read from the table or index. I'll refer to each increment as an 'RCU' (read capacity unit) for this post, even though it's sometimes called a 'read request unit'. Thus, if you read a single 10KB item in DynamoDB, you will consume 3 RCUs (10 / 4 == 2.5, rounded up to 3).</p>",
  "neutralised_chunk": "- An overview of how DynamoDB pricing works is provided.\n- Pricing can be complex with various modes (on-demand vs provisioned) and table storage classes (Standard or Standard-IA).\n- However, the primary aspects of DynamoDB's pricing are the core dimensions of charge.\n- DynamoDB charges for three things.\n- The first is read consumption measured in 4KB increments for every read operation. This represents the quantity of data read from the table or index.\n- In the post, 'RCU' (read capacity unit) will denote each increment, although it's sometimes called 'read request unit'.\n- For instance, a single 10KB item read in DynamoDB would consume 3 RCUs (10 / 4 rounded up to 3)."
}
{
  "chunk": "<p>Write consumption: Measured in 1KB increments (also rounded up!) for each write operation. This is the size of the item you're writing / updating / deleting during a write operation. I'll refer to each write increment as a 'WCU' (write capacity unit). Thus, if you write a single 7.5KB item in DynamoDB, you will consume 8 WCUs (7.5 / 1 == 7.5, rounded up to 8).</p><p>Storage: Measured in GB-months. This is the amount of data stored in the table or index, multiplied by the number of hours in the month. I almost never think about this during modeling as read and write consumption are usually the dominant cost factors.</p><p>The great thing about this is its predictability! You don't have to wonder about how many concurrent operations a db.m6gd.large instance can handle. You don't have to worry about how much data you can store in a 1TB gp3 storage volume. You just have to worry about how much data you're reading, writing, and storing.</p>",
  "neutralised_chunk": "- Write consumption is measured in 1KB increments, rounded up for each write operation.\n- Each 1KB increment consumed during a write is called a Write Capacity Unit (WCU).\n- If writing a 7.5KB item, 8 WCUs will be consumed (7.5KB rounded up to 8KB).\n- Storage is measured in GB-months, based on data stored in the table or index multiplied by the number of hours in the month.\n- Read and write consumption are usually the dominant cost factors, with storage being less significant.\n- The billing model is predictable, eliminating concerns about concurrent operations, instance types, or storage volumes.\n- The focus is on the amount of data read, written, and stored."
}
{
  "chunk": "<p>Not only this, but you can easily do a rough calculation of your costs as you're designing your data model. I'll walk through this further in the Doin' the Math sections below, but this is pretty straightforward math you can do with an Excel spreadsheet.</p><p>A few other quick notes / quirks about DynamoDB pricing:</p><p>Notice that RCUs and WCUs are step functions -- you're charged for each increment, even if you don't use the full increment. For example, if you read 1.5KB of data, you'll be charged for 1 RCU. If you write 0.5KB of data, you'll be charged for 1 WCU.</p><p>For reads, you have the option between a strongly consistent read request and an eventually consistent read request. An eventually consistent read request consumes half as many RCUs as a strongly consistent read request. IMO, you should almost always use an eventually consistent read request. If you think otherwise, let's fight :)</p>",
  "neutralised_chunk": "- DynamoDB's pricing model allows for rough cost estimation during data modeling phase.\n- The \"Doin' the Math\" section will cover cost calculation details.\n- The cost calculation involves straightforward math that can be done using a spreadsheet.\n- RCUs (Read Capacity Units) and WCUs (Write Capacity Units) are charged in increments, even if the full increment is not utilized.\n  - Reading 1.5KB of data incurs a charge for 1 RCU.\n  - Writing 0.5KB of data incurs a charge for 1 WCU.\n- For read requests, there is an option for strongly consistent or eventually consistent reads.\n- Eventually consistent read requests consume half the RCUs compared to strongly consistent reads.\n- The recommendation is to almost always use eventually consistent read requests, unless there is a strong reason not to."
}
{
  "chunk": "<p>A DynamoDB Query allows you to read multiple items in a single request. When doing a Query operation, the items will be added together first before dividing by 4KB to determine RCU usage. Thus, using a Query to read 100 items of 100 bytes each will cost you 3 RCUs (10KB / 4 == 2.5, rounded up to 3). If you had read each of those 100 items separately, it would have cost you 100 RCUs!</p><p>For reads, you're charged for the data that's actually read, not the data returned to the client. Thus, DynamoDB Filter Expressions don't help the way you think they might. You should almost always use your key attributes to filter your results.</p>",
  "neutralised_chunk": "- A DynamoDB Query allows multiple items to be read in a single request.\n- In a Query operation, items are combined before division by 4KB to determine Read Capacity Unit (RCU) usage.\n- Reading 100 items each with 100 bytes via a Query will costs 3 RCUs due to round up of 2.5 (calculation: 10KB/4)\n- Reading each of the 100 items separately would cost 100 RCUs.\n- Charges for reads are based on the data actually read, not the data returned to the client.\n- Therefore, DynamoDB Filter Expressions may not help in the expected way.\n- Key attributes should be used to filter results as much as possible."
}
{
  "chunk": "<p>When doing a write operation, you get charged the larger of the size of item as it was before the operation or as it is after the operation. For example, if you have an existing item that is 9.5KB and you increment a single counter integer on it, you will pay the full 10 WCUs for the operation. Likewise, if you delete that 9.5KB item, you will pay 10 WCUs.</p><p>When you have a condition expression that fails, you'll still be charged WCUs for it. Further, it will be based on the size of the matching item (if any), with a minimum consumption of 1 WCU. (This one kind of makes me grumpy.)</p><p>Using DynamoDB Transactions doubles your WCU usage. It's using two-phase commit under the hood, so basically think of being charged for each phase.</p>",
  "neutralised_chunk": "- Write operations in DynamoDB are charged based on the larger item size, before or after the operation.\n- For example, incrementing a counter on a 9.5KB item will incur a charge for 10 WCUs.\n- Deleting a 9.5KB item will also incur a charge for 10 WCUs.\n- Failed condition expressions still incur WCU charges based on the matching item size (minimum 1 WCU).\n- Using DynamoDB Transactions doubles the WCU usage due to the two-phase commit process under the hood."
}
{
  "chunk": "<h3>Choosing between provisioned and on-demand\u200b</h3><p>In the examples below, we'll do some calculations to see how you can think about DynamoDB pricing. For my examples, I always calculate using DynamoDB On-Demand billing mode. On-Demand billing has a fixed price for reads and writes, so it's easy to calculate the cost of a particular access pattern. I don't have to think about things like utilization over time or providing a buffer to handle unexpected bursts of traffic. This greatly simplifies the math and gives you a directionally accurate look at your potential costs.</p><p>However, this doesn't mean you should never think about your billing mode! Once you've run the initial numbers, then you can fine-tune your calculations by looking at the difference between provisioned and on-demand.</p>",
  "neutralised_chunk": "- Choosing between provisioned and on-demand billing modes for DynamoDB.\n- Examples will use calculations based on DynamoDB On-Demand billing mode.\n- On-Demand has fixed prices for reads and writes, making cost calculation easier.\n- No need to consider utilization over time or handling traffic bursts, simplifying the math.\n- Provides a directionally accurate estimate of potential costs.\n- However, billing mode choice should still be considered after initial calculations.\n- Fine-tuning can be done by comparing provisioned and on-demand pricing."
}
{
  "chunk": "<p>At a first cut, if the on-demand numbers are negligible, you should probably stick with that. On-demand is going to be a hands-off solution that (almost completely) avoids throttling. If the potential bill won't break the bank, take the operational simplicity and move on.</p><p>However, when you're talking about real money, then you can consider whether provisioned capacity would be a worthwhile optimization. In general, on-demand billing is 6.94x as expensive as fully-utilized provisioned capacity.</p><p>This seems like a lot, but note the key qualifier there -- fully-utilized. You're not going to get anywhere near full utilization, so saving 85% is not a realistic goal. However, it's not unrealistic to think you could get to 50% utilization, which would save you 42% on your bill. For a high-traffic DynamoDB workload, that's a pretty significant savings!</p>",
  "neutralised_chunk": "- When on-demand usage is low, maintaining that should be considered for less operational complexity and avoiding throttling.\n- On-demand solution is suitable if the estimated cost is affordable.\n- However, if costs are high, exploring provisioned capacity could be an effective cost optimization.\n- On-demand billing can be nearly seven times more costly than fully-utilized provisioned capacity.\n- But achieving full utilization may not be likely, hence saving 85% might not be achievable.\n- Yet, reaching 50% utilization could lead to 42% savings on the cost, which could be significant for high-traffic DynamoDB workloads."
}
{
  "chunk": "<p>Just remember what you're taking on. You're responsible for scaling up and down to account for traffic patterns. If traffic grows over time, you're responsible to account for the new highs. If you have a spike in traffic, you're responsible for scaling up to handle it. If you have a spike in traffic that you don't expect, you're responsible for handling it. If you don't do this, you'll get throttled and your users will be unhappy.</p>",
  "neutralised_chunk": "- The responsibility for scaling to match traffic patterns falls on the user.\n- If traffic increases progressively, the user must accommodate for the new peaks.\n- In case of sudden traffic surges, it is the user's job to scale accordingly.\n- Unexpected traffic spikes must also be managed by the user.\n- Failure to adequately scale in response could result in throttling and subsequently, dissatisfied users."
}
{
  "chunk": "<h2>Doin' the math pt. 1: Tracking view counts\u200b</h2><p>Alright, enough background chatter. Let's get into some examples.</p><p>The first example is what spurred this blog post. Someone wrote me an email about potential DynamoDB costs in tracking view counts. Every time a user visits one of his pages, he wants to increment a counter for that page.</p><p>He was worried about the cost of this and asked what I thought about batching page views in Redis for the short term, then occasionally writing them back to DynamoDB. This adds some cost (Redis instance!) and some complexity (multiple sources of data! syncing back to DynamoDB!), but he was thinking about doing it out of fear of a large DynamoDB bill.</p><p>The first thing I told him -- \"Do the math!\" Look at how big your item sizes are, then try to roughly estimate your traffic. I love spreadsheets, so I usually throw this in a spreadsheet and fiddle with the numbers to see what I'm dealing with.</p>",
  "neutralised_chunk": "- The discussion focuses on tracking view counts for a webpage using DynamoDB and its associated costs.\n- This topic was initiated by an email inquiry about the costs of tracking page visits in DynamoDB, where each visit increases a counter for that page.\n- There were concerns about potential high costs, leading to a proposal of batching page views in Redis temporarily and periodically updating DynamoDB.\n- The addition of Redis would increase cost and complexity, such as managing multiple data sources and synchronizing data back to DynamoDB.\n- A recommended approach to mitigate this fear is to \"do the math\" \u2013 calculate item sizes, estimate traffic, and understand the costs using those variables. A spreadsheet could be helpful for this task."
}
{
  "chunk": "<p>In this situation, imagine his page record was pretty small -- under 1KB. If so, each write will be 1 WCU. Thus, he can estimate that his DynamoDB write costs will be $1.25 per million page views (on-demand billing is $1.25 per million WCUs).</p><p>This pricing may not be acceptable to you, but it gives you a sense of what you're dealing with. For him, it helped him realize that it wasn't something to optimize yet. He should spend his time on other things.</p><p>PS: If you are thinking 'how do I know my item size?', generate an example record in your application and then paste it into Zac Charles' DynamoDB Item Size Calculator. I love this thing.</p>",
  "neutralised_chunk": "- If the page record size is small (under 1KB), each write will consume 1 Write Capacity Unit (WCU).\n- With on-demand billing at $1.25 per million WCUs, the estimated DynamoDB write cost would be $1.25 per million page views.\n- The pricing may or may not be acceptable, but it provides an understanding of the costs involved.\n- For the given scenario, it helped realize that optimizing DynamoDB costs was not a priority at that moment.\n- To determine item size, generate a sample record and use Zac Charles' DynamoDB Item Size Calculator, which is a useful tool."
}
{
  "chunk": "<p>Note that I discussed an enhanced version of this during my 2022 talk at AWS re:Invent. I talked about 'vertical sharding', which can be a nice use of single-table design principles to split a single entity into multiple records to save on costs. For this example, imagine our underlying record was much larger -- 10KB instead of 1KB. Now, each million page views will cost us $12.50 even though we're just incrementing a counter.</p><p>Instead, we could separate our record into two items -- the page view counter and the actual item data. We can place them in the same item collection to fetch them in a single Query request when needed, but now a page view increment is only acting on the small, focused item. This would save 90% of our write costs for this example.</p>",
  "neutralised_chunk": "- An enhanced version of this concept, \"vertical sharding,\" was discussed at AWS re:Invent 2022.\n- Vertical sharding involves splitting a single entity into multiple records to reduce costs, applying single-table design principles.\n- In this example, if the underlying record was 10KB instead of 1KB, each million page views would cost $12.50, even though only incrementing a counter.\n- To optimize costs, the record could be separated into two items: the page view counter and the actual item data.\n- Both items can be placed in the same item collection for single Query retrieval when needed.\n- Now, a page view increment only acts on the small, focused item, saving 90% of write costs in this example."
}
{
  "chunk": "<h2>Doin' the math pt. 2: Do I really need that secondary index?\u200b</h2><p>Alright, that's an easy example. Let's look at another example that involves some tradeoffs between different approaches to an access pattern within DynamoDB.</p><p>We saw earlier that each write to DynamoDB is charged 1 WCU per KB of data, rounded up. However, this applies to each target to which the write will be applied. I use the word 'target' to refer to not just the base table but any secondary indexes to which the item will be written.</p><p>It's not uncommon to have 4-5 secondary indexes on a DynamoDB table. When this happens, your write costs can quickly add up. Think of our view count example above. If we had 5 secondary indexes, we'd be paying 6 WCU per page view instead of 1 WCU!</p>",
  "neutralised_chunk": "- The post explores the need for the secondary index in DynamoDB with a more complex example.\n- Each write to DynamoDB is charged at a rate of 1 WCU per KB of data.\n- This charge applies to all targets of the write, including the base table and any secondary indexes.\n- Having multiple secondary indexes on a DynamoDB table can increase write costs substantially.\n- For instance, in a scenario of tracking per-page views with 5 secondary indexes, the cost per view could increase from 1 WCU to 6 WCUs."
}
{
  "chunk": "<p>There are lots of ways to reduce costs for GSIs that you truly need (check out Khawaja's tweet about saving money on GSIs during his awesome thread on DynamoDB savings generally), but sometimes you may be better off avoiding a GSI altogether.</p><p>Let's see an example.</p><p>In DynamoDB, I tell people that you almost always want to filter and sort the exact records you need via the key attributes. The key word here is 'almost'. There are certain times when it may be better to over-read your data.</p><p>Let's work with a simple example. Imagine you're selling a SaaS service that is purchased by teams. By nature of the industry you're in, it's natural that teams will be small. It would be extremely rare that over 100 people would belong to a single team. In the vast majority of cases, teams will be fewer than 10 users.</p>",
  "neutralised_chunk": "- There are multiple methods to reduce costs for essential GSIs, but sometimes it's beneficial to avoid a GSI entirely.\n- Consider an example to understand this better.\n- In DynamoDB, it is usually recommended to filter and sort records using key attributes. However, occasionally, it might be better to read more data than necessary.\n- Imagine a scenario where a SaaS service is being sold to small teams. It's highly unlikely for a team to include more than 100 people, with most teams comprising fewer than 10 users."
}
{
  "chunk": "<p>Each User record within the team is pretty small -- 1KB of data. However, you also have a few access patterns on the User -- maybe a point lookup to fetch a User by username, and two range queries -- one to fetch all the Users with a given role, and one to fetch all Users on the team ordered by the date added to the team.</p><p>You could set up a GSI for each of these secondary access patterns. However, you could also just handle the range queries by fetching all the Users on the team and filtering them in your application code. Even in the worst case scenario, you'll be fetching 100KB of data. That's 12.5 RCUs (100KB / 4 = 25 * 0.5 for eventually consistent reads = 12.5 RCUs). For most cases, it will be much smaller than that -- likely less than a single RCU. Further, RCUs are not only 4 times bigger than WCUs (4KB vs. 1KB), they're also 5 times cheaper ($0.25 per million as opposed to $1.25 per million for WCUs). </p>",
  "neutralised_chunk": "- Each User record within a team is small, around 1KB of data.\n- There are a few access patterns for Users:\n  - Point lookup to fetch a User by username\n  - Range query to fetch all Users with a given role\n  - Range query to fetch all Users on the team ordered by date added\n- You could create a Global Secondary Index (GSI) for each secondary access pattern.\n- Alternatively, you could fetch all Users on the team and filter them in the application code.\n- In the worst case, fetching 100 Users (100KB) would require 12.5 Read Capacity Units (RCUs) for eventually consistent reads.\n- In most cases, it would be much less than 1 RCU.\n- RCUs are 4 times bigger than Write Capacity Units (WCUs) (4KB vs. 1KB).\n- RCUs are also 5 times cheaper ($0.25 per million vs. $1.25 per million for WCUs)."
}
{
  "chunk": "<p>Based on some back-of-the-napkin math, it might be better to skip the GSI and over-read data to handle long-tail access patterns. This is particularly true when the overall result set is pretty small.</p><p>Again, you should still focus on using your key attributes as much as possible. However, there are times when you can relax that default assumption to save on write costs.</p>",
  "neutralised_chunk": "- Preliminary calculations suggest that for long-tail access patterns, it could be more beneficial to over-read data than to use a GSI, particularly when the overall result set is small.\n- The focus should remain on using key attributes as much as possible. However, there are occasions where this approach can be relaxed to reduce write costs."
}
{
  "chunk": "<h2>Doin' the math pt. 3: Complex filtering\u200b</h2><p>One of the harder patterns in DynamoDB is what I call \"complex filtering\". This is when you may need to filter a dataset by a number of potential variables, all of which can be optional. DynamoDB wants to work with known access patterns, and this includes knowing the attributes you'll be filtering on. This kind of dynamism can be tricky for DynamoDB.</p><p>If this is a pattern you really need to support, you can again do some math to see if it's a feasible situation.</p><p>Another example. Imagine you have a CRM system that tracks customers and their orders. You want users to be able to filter their customers by a number of different attributes -- name, email, city, and industry segment. You also want to be able to filter by the date they were added to the system. Each of these attributes is optional in your filter.</p>",
  "neutralised_chunk": "- Complex filtering in DynamoDB involves filtering a dataset by multiple potential variables, all of which can be optional.\n- DynamoDB prefers known access patterns, including the attributes used for filtering, making dynamic filtering challenging.\n- For complex filtering requirements, performing calculations can help assess feasibility.\n- Example: A CRM system tracking customers and their orders, allowing users to filter customers by name, email, city, industry segment, and date added to the system, with each attribute being optional in the filter."
}
{
  "chunk": "<p>These customer records can be pretty big -- let's say 4KB. Because of this, it's infeasible to add all the permutations of access patterns as GSIs. This could easily be 5 GSIs. This would make each update to your User cost 24 WCUs (4KB * 6 = 24 WCUs). This is a lot of write capacity for a single record!</p><p>But we also can't use the simple over-reading approach that we used in the previous example. Because each User is 4KB, we'll be paying half an RCU per customer on an eventually consistent read. A user might have a thousand customers in their system, so we'd be paying 500 RCUs per query to fetch them all. That's a lot of RCUs!</p><p>What are our options? Well, notice that while our application allows filtering on a few different attributes, it's still a very small portion of the record on which we filter. We only need about 100 bytes of the record, not the full 4KB. </p>",
  "neutralised_chunk": "- Customer records can be large, around 4KB, making it impractical to add all access pattern permutations as GSIs.\n- With five GSIs, each User update would cost 24 WCUs.\n- The simple over-reading approach from the prior example is also impractical due to the large User size.\n- An eventually consistent read of a 4KB user would cost half an RCU. With a thousand customers, this would be 500 RCUs per query.\n- While the application allows filtering on certain attributes, these make up only a small portion of the record, approximately 100 bytes, not the full 4KB.\n"
}
{
  "chunk": "<p>We could use a secondary index and take advantage of index projections (more detail on secondary index projects from the wonderful Pete Naylor here). With an index projection, you can choose which attributes are included in the secondary index. As Pete notes, this helps us reduce writes by avoiding writes when only non-projected attributes are changing.</p><p>But it also helps in another way -- reducing our item size for complex filtering. If our item sizes are a mere 100 bytes, now we can fetch 40 customers per RCU unit. If we want to fetch 1000 customers, it only costs 40 RCUs (or 20 RCUs for eventually consistent reads). That's 4% of our original cost!</p>",
  "neutralised_chunk": "- Secondary indexes and index projections can be used to mitigate data challenges, reducing writes by excluding non-projected attribute changes.\n- Pete Naylor provides additional information on secondary index projects.\n- Index projections can also help in reducing item size for complex filtering, hence enhancing efficiency.\n- If item sizes are reduced to 100 bytes, 40 customers can be fetched per RCU unit.\n- Fetching 1000 customers only costs 40 RCUs, or 20 for eventually consistent reads, a reduction of 96% in cost."
}
{
  "chunk": "<p>Note that if you need to fetch the full records, you'll need to do a follow-up, BatchGetItem operation to hydrate them. This is an additional cost to consider, both in terms of read costs and in response latency . However, if you're doing complex filtering, you may not be fetching the full record anyway. You're probably just showing a list of customers that match the filter, and then allowing the user to click on a customer to see the full record.</p><p>This pattern can also help you see when use cases are infeasible. If a user might have 100,000 customers in their CRM, now you're looking at 4000 RCUs to fetch and filter them all. If this is going to be a common pattern for you, you'll be burning through RCUs like crazy. At this point, I usually recommend integrating something like (e.g. Rockset or Elasticsearch) to augment your core DynamoDB patterns with some complex filtering. This adds cost and complexity, but it's often the right tradeoff for your use case.</p>",
  "neutralised_chunk": "- Fetching full records in DynamoDB requires a follow-up, BatchGetItem operation.\n- This has additional costs for read operations and response latency.\n- Complex filtering may not always require fetching full records, just a list that matches the filter.\n- Further actions, such as viewing full records, are often user-initiated.\n- This operation pattern may highlight when certain use cases are impractical.\n- If high customer volumes (e.g., 100,000 in CRM) necessitate frequent fetch and filter operations, this can lead to substantial RCUs.\n- In high RCU scenarios, integrating functional elements like Rockset or Elasticsearch for complex filtering is advised.\n- These additions can increase cost and complexity, but could provide the right balance for specific use cases."
}
{
  "chunk": "<h2>Conclusion\u200b</h2><p>In this post, we learned how to think about DynamoDB costs. We walked through the background of how DynamoDB does billing and then worked through some examples of how to think about DynamoDB costs in practice.</p><p>The strongest point I want to emphasize is that you should use the predictability and legibility of DynamoDB to your advantage. Use it to understand whether an access pattern will be cost effective. Use it to weigh the tradeoffs of different approaches to DynamoDB. And, yes, use it to understand when DynamoDB might not be the right fit for a particular use case.</p>",
  "neutralised_chunk": "- Summary of the post's content: understanding DynamoDB costs.\n- Background on DynamoDB billing model.\n- Examples of evaluating DynamoDB costs in practice.\n- Key emphasis: leverage DynamoDB's predictability and transparency to:\n  - Assess cost-effectiveness of access patterns.\n  - Weigh tradeoffs of different DynamoDB approaches.\n  - Determine when DynamoDB may not be suitable for certain use cases."
}
{
  "chunk": "<h1>Event-Driven Architectures vs. Event-Based Compute in Serverless Applications</h1><p>I recently delivered serverless training to some engineers, and there was confusion between two concepts that come up in discussons of serverless architectures.</p><p>On one hand, I describe AWS Lambda as event-based compute, which has significant implications for how you write the code and design the architecture in your serverless applications.</p><p>On the other hand, many serverless applications use an event-driven architecture that relies on decoupled, asynchronous processing of events across your application.</p><p>These two concepts -- event-driven architectures and event-based compute -- sound similar and are often used together in serverless applications on AWS, but they're not the same thing. Further, the patterns you use for one will not necessarily apply if you're not using the other.</p>",
  "neutralised_chunk": "- Distinguishing between event-driven architectures and event-based compute in serverless applications.\n- AWS Lambda is described as event-based compute, impacting code writing and architecture design for serverless applications.\n- Many serverless applications use an event-driven architecture, relying on decoupled, asynchronous processing of events across the application.\n- Event-driven architectures and event-based compute are related concepts, often used together in serverless applications on AWS, but they are not the same thing.\n- Patterns used for one may not apply if the other is not being used."
}
{
  "chunk": "<p>The event-based nature of Lambda compute is what fundamentally distinguishes Lambda from other computing paradigms and is what leads to the unique constraints and demands of serverless application developers.</p><p>In this post, we'll look at both event-driven architecture and event-based compute. We'll examine the key characteristics of each as well as the implications for you in your applications.</p>",
  "neutralised_chunk": "- Lambda's event-based compute nature substantially differentiates it from other computing paradigms.\n- This distinctiveness results in specific constraints and requirements for serverless application developers.\n- The post will explore both event-driven architecture and event-based compute.\n- Key characteristics of each and their implications for applications will be inspected."
}
{
  "chunk": "<h2>What is an event-driven architecture\u200b</h2><p>Event-driven architectures are all the rage, so we'll start there.</p><p>Event-driven architectures are characterized by services that communicate (1) asynchronously (2) via events. These two elements distinguish them from other architecture patterns.</p>",
  "neutralised_chunk": "- Event-driven architectures are currently popular.\n- These architectures are unique due to asynchronous communication via events.\n- This distinguishes event-driven architectures from other architecture patterns."
}
{
  "chunk": "<h4>Event-driven architectures communicate asynchronously\u200b</h4><p>If you've worked with a frontend client calling a backend GraphQL API or a backend service calling another service via a REST API or RPC, you've used the request-response pattern. This is a common pattern for communication from one client to a service. It is a synchronous flow where the client will wait for a full response from the service indicating the result of the request. A synchronous flow is simple as you get direct feedback on what happened from your request.</p>",
  "neutralised_chunk": "- Event-driven architectures communicate asynchronously.\n- The request-response pattern, where a client calls a backend service and waits for a response, is synchronous.\n- Examples include frontend clients calling a backend GraphQL API, or a backend service calling another service via REST API or RPC.\n- In a synchronous flow, the client waits for a full response indicating the result of the request.\n- Synchronous flow is simple as you get direct feedback on the request."
}
{
  "chunk": "<p>However, relying on synchronous communiation has costs as well. It can reduce the overall performance of your application, particularly when working with slower tasks like sending emails or generating reports. Further, the availability of your services goes down. If Service A relies on synchronous responses from Service B to operate, the uptime for Service A cannot be higher than that of Service B. Service B's downtime becomes Service A's downtime.</p><p>With asynchronous patterns, these problems are reduced. Services can still communicate, but there's no expectation of an immediate response. The downstream service can process the communication on its own schedule without tying up the upstream service. This adds its own challenges around debugging, eventual consistency, and more, but it does reduce the downsides of synchronous communication.</p><p>For more on request-response vs. event-driven, check out Former AWS Developer Advocate Talia Nassi's post on the benefits of moving to event-driven architectures.</p>",
  "neutralised_chunk": "- Synchronous communication can affect the application's performance, especially with slower tasks.\n- It can lower the availability of services as they are interdependent for operation.\n- Any downtime in Service B will reflect as downtime in Service A if it relies on synchronous responses from Service B.\n- Asynchronous communication can mitigate these issues as it doesn't expect an immediate response.\n- The downstream service can process the communication based on its own schedule, freeing up the upstream service.\n- However, asynchronous communication brings its own challenges such as debugging, eventual consistency, etc.\n- For further information on the comparison between request-response and event-driven architectures, look for Talia Nassi's post on the advantages of transitioning to event-driven architectures."
}
{
  "chunk": "<h4>Event-driven architectures communicate via events\u200b</h4><p>Unsurprisingly, the \"events\" bit is the more unique part of event-driven architectures. In event-driven architectures, services will broadcast events that will be consumed by and reacted upon by other services.</p><p>Thus, events require two types of parties:</p><p>Event producers, which publish events describing something that happened within the service (a User was created, an Order was delivered, or a Login Attempt failed).</p><p>Event consumers, which subscribe to published events and react to them (updating local state, incrementing aggregates, triggering workflows, etc.).</p><p>A key feature of true event-driven architectures is that the producers and consumers are completely decoupled -- a producer shouldn't know or care who is consuming its events and how the consumers use those events in their service..</p>",
  "neutralised_chunk": "- Event-driven architectures are characterized by communication through events.\n- In these architectures, services broadcast events that are consumed and responded to by other services.\n- Events involve two types of entities:\n  - Event producers, which publish events representing occurrences within the service (e.g., User creation, Order delivery, Login Attempt failure).\n  - Event consumers, which subscribe to these events and react accordingly (e.g., updating state, incrementing aggregates, triggering workflows).\n- Producers and consumers in event-driven architectures are entirely decoupled - a producer is not aware or concerned about who is consuming its events and how those events are utilized by the consumers."
}
{
  "chunk": "<p>An event producer is like a broadcaster on the nightly news. The broadcaster will say the news that happened regardless of whether anyone is tuned in.</p><p>Contrast this with a more traditional message-driven architecture in which one component in a system may insert a message to a message queue (like SQS or RabbitMQ) for processing. A message-driven pattern is also asynchronous, like an event-driven pattern. However, the producer of the message is purposefully sending the work to a specific customer. The use of the queue helps with resiliency and with faster response times from the initial component, but it's not truly \"event-driven\" in the normal sense of the term given the tight connection between the producer of the message and the consumer.</p>",
  "neutralised_chunk": "- An event producer is compared to a news broadcaster who announces news regardless of the audience.\n- This is different from a traditional message-driven architecture, where a system component sends a message to a queue (such as SQS or RabbitMQ) for processing.\n- Both message-driven and event-driven patterns are asynchronous.\n- In a message-driven pattern, the message producer deliberately sends the workload to a specific customer.\n- The queue aids in resilience and quicker response times from the original component.\n- However, it's not genuinely \"event-driven\" due to the close relationship between the message producer and consumer."
}
{
  "chunk": "<p>If event-driven is like a TV broadcast, message-driven is more like your boss sending you an email with a request to create a report. Not only is there a specific output she wants (the report), but there's a specific person she wants to do it (you).</p><p>Some resources out there will consider message-driven workflows to be a part of an event-driven architecture. I sort of disagree but mostly don't care too much. I do agree that some implications and reasons for using message-driven patterns are similar to event-driven patterns and thus some of the lessons are similar.</p><p>The main benefits of event-driven architectures are in their flexibility and resiliency. If I want to add a new consumer of a given event, I don't have to coordinate with the producer of the event. I can start processing the events as they are published and use them in my new service.</p>",
  "neutralised_chunk": "- Event-driven architectures are likened to a TV broadcast, while message-driven workflows are compared to a specific email request from a boss asking for a report.\n- There is some debate over whether message-driven workflows should be categorized as part of event-driven architecture.\n- Both patterns share similar reasons for use, and any distinctions might not be of great consequence.\n- Event-driven architectures offer notable flexibility and resilience.\n- Adding a new consumer to an event-driven architecture does not require coordination with the event producer.\n- The new consumer can start processing events as they are published for use in a new service."
}
{
  "chunk": "<p>Event-driven architectures have been around for a while. If you talk to a developer that spent time at an enterprise shop from the 90s and 2000s, you'll likely hear complaints about the enterprise service bus. More recently, the rise of Apache Kafka and the very effective evangelism from Jay Kreps (one of the original Kafka creators) about the effectiveness of logs & streams has breathed new life into event-driven architectures.</p><p>There are a lot of flavors of event-driven architectures out there, including purist patterns like event sourcing and CQRS. I generally recommend avoiding those -- they're fun to think about and imagine all the possibilities, but I've seen them turn into a maintenance and debugging headache.</p><p>This was only a cursory review of event-driven architectures. If you want more on this, AWS Developer Advocate David Boyne is the person to follow on all things event-driven. Lots of great stuff on event-driven architectures, including a great set of visuals on Serverless Land.</p>",
  "neutralised_chunk": "- Event-driven architectures have been around for a long time, with developers from the 90s and 2000s having experience with enterprise service buses.\n- The rise of Apache Kafka and effective evangelism by Jay Kreps (Kafka creator) has revived interest in event-driven architectures using logs and streams.\n- There are many variations of event-driven architectures, including purist patterns like event sourcing and CQRS.\n- It is generally recommended to avoid purist patterns as they can become maintenance and debugging challenges, despite being interesting concepts.\n- This is a brief overview of event-driven architectures.\n- For more information, AWS Developer Advocate David Boyne is a recommended resource, with extensive content on event-driven architectures, including visuals on Serverless Land."
}
{
  "chunk": "<h2>What is event-based compute\u200b</h2><p>Now that we understand a bit about event-driven architectures, let's turn to event-based compute to see how it differs.</p><p>There are two key characteristics of event-based compute:</p><p>First, the existence of a compute instance is intimately tied to the occurence of an event to be processed.</p><p>Second, the compute acts on a single event at a time.</p><p>That's sort of abstract, so let's make it more concrete.</p><p>Imagine you create a Lambda function. You write a bit of code, create a ZIP file, upload it to AWS, and set up the configuration in the Lambda service. Configuring this doesn't actually start your code, like it might with an EC2 instance or a Fargate container. By default, there will be no actual instances of your Lambda compute running. Your Lambda function has potential, but it hasn't realized it yet.</p>",
  "neutralised_chunk": "- Event-based compute is a concept related to event-driven architectures.\n- It has two key characteristics:\n  1. The existence of a compute instance is tied to the occurrence of an event to be processed.\n  2. The compute acts on a single event at a time.\n- Example: A Lambda function.\n  - You write code, create a ZIP file, upload it to AWS, and configure it in the Lambda service.\n  - This configuration does not start your code running by default.\n  - No actual instances of your Lambda compute are running initially.\n  - Your Lambda function has potential but has not realized it yet."
}
{
  "chunk": "<p>To make your function a reality, you need to hook it up to an event source. There are a ton of services that integrate with Lambda. The most popular sources are probably API Gateway (HTTP requests), SQS (queue processing), EventBridge (event bus), and Kinesis / DynamoDB Streams (stream processing).</p><p>Note that, on the Lambda docs page linked above, it uses the term 'event-driven' for a lot of event sources, including many that I would not describe as event driven! More on this below.</p><p>Once you've configured your event source and an event has flowed through the configured service, then your function will spring to life. The Lambda service will create an instance of your function and pass the triggering event to it for processing by your function. Your function will process the event as desired and return a response back to the event trigger.</p>",
  "neutralised_chunk": "- Lambda functions need to be connected to an event source to be invoked.\n- Popular event sources include API Gateway (HTTP requests), SQS (queue processing), EventBridge (event bus), and Kinesis/DynamoDB Streams (stream processing).\n- The Lambda docs use the term 'event-driven' for many event sources, even some that may not be considered truly event-driven.\n- When an event occurs in the configured service, Lambda creates an instance of the function and passes the triggering event to it for processing.\n- The function processes the event as desired and returns a response back to the event trigger."
}
{
  "chunk": "<p>For optimization purposes, the Lambda service may keep your function instance running for a bit to serve other events that occur in a short time period. However, the specifics of this is (mostly) out of your control.</p><p>Importantly, the purpose of an instance of running compute is to handle a single, specific event. This distinguishes Lambda from traditional instances or containers which are created to be available to handle requests as they arrive or to poll for messages from a queue or stream. It even distinguishes Lambda from something like creating a Fargate task on a schedule. While the creation of the task is based on an event, the task doesn't naturally have awareness of the event that created it while executing.</p>",
  "neutralised_chunk": "- Lambda service may keep a function instance running for a short time to handle other events for optimization.\n- The control over this behavior is largely outside users' reach.\n- An instance of running compute in Lambda is primarily created to handle a single specific event.\n- This separates Lambda from traditional instances or containers, which are made to handle on-demand requests, or to poll for messages from a queue/stream.\n- It also distinguishes Lambda from scheduling a Fargate task, as the task does not inherently know the event that initiated it while executing."
}
{
  "chunk": "<h3>Implications of event-based compute\u200b</h3><p>We now know how AWS Lambda is event-based computing -- so what? How does this actually affect our applications?</p><p>In my mind, this is the most crucial shift about AWS Lambda. It leads to some of the following preferences from engineers using Lambda:</p><p>Note that none of these preferences is universal -- there's a lot of diversity across Lambda-based architectures. However, they are preferences that you tend to see, particularly in comparison to architectures that use other methods of compute.</p><p>There are two main implications from Lambda's event-based nature that you should keep front of mind.</p><p>First, you need to think about statelessness and rapid initialization more than usual. In an instance-based or container-based application, you can initialize your application, establish database connections, build local caches, and perform other initialization work before making your application available to handle requests.</p>",
  "neutralised_chunk": "- Event-based computing with AWS Lambda has significant implications for application architecture.\n- Engineers using Lambda tend to have certain preferences:\n  - Stateless functions\n  - Rapid initialization\n  - Decoupled architectures\n  - Asynchronous processing\n  - Granular decomposition of workloads\n- These preferences are not universal, but are common in Lambda-based architectures compared to other compute methods.\n- Two main implications of Lambda's event-based nature:\n  - Greater emphasis on statelessness and rapid initialization.\n  - In instance-based or container-based applications, initialization work can be done before handling requests.\n  - With Lambda, initialization must happen quickly for each invocation."
}
{
  "chunk": "<p>This is not the case with Lambda compute. Remember, you may be initializing your compute in response to an event. There might be an active HTTP request with a real live user on the other end of it, waiting to see the latest Tweets or to buy some Taylor Swift tickets. You need to make sure your code can load and execute quickly without a bunch of setup.</p><p>Specifically, this means keeping your function code small and shallow. Try to avoid loading and initializing nested dependencies across many files. Depending on your tolerance for punishment, you can look at using esbuild or other tools to bundle your code into a single file and reduce init-time disk reads.</p>",
  "neutralised_chunk": "- Lambda compute differs in that it may be initialized in response to an event.\n- It could involve an active HTTP request from a user waiting on an activity, like viewing tweets or purchasing tickets, requiring the code to load and execute quickly.\n- This necessitates keeping the function code small and not too layered.\n- Avoiding an extensive load and initialization of nested dependencies across numerous files is recommended.\n- Tools like esbuild can be used to bundle code into a single file to decrease initialization-time disk reads, depending on your preference for complexity."
}
{
  "chunk": "<p>Also, consider tips to improve performance on subsequent requests. There are kinds of work that you must do on your cold-start initialization, like establishing network connections to databases or other resources or retrieving dynamic config. However, you should understand how to reuse those resources across multiple requests to avoid making every request to your compute perform like a cold start.</p><p>Second, scaling is about more instances of compute than concurrency within a single instance. Within a Lambda function instance, you will be processing one and only one event at a time. If multiple events occur at the same time, the Lambda service will spin up more function instances to handle the events. However, it will always be a single event at a time.</p>",
  "neutralised_chunk": "- Be mindful of ways to enhance performance for subsequent requests.\n- Certain tasks like establishing network connections or retrieving dynamic configurations are required during the cold-start initialization.\n- Reusing these resources can prevent every request from behaving like a cold start.\n- Scaling involves increasing the number of compute instances rather than concurrency within a single instance.\n- Within a Lambda function instance, only one event is processed at a time.\n- If multiple events occur simultaneously, the Lambda service creates additional function instances to handle these events.\n- Every function instance deals with only one event at a time."
}
{
  "chunk": "<p>This will change not only how you write your application code but potentially even the language you choose. The inimitable Ben Kehoe once said that Node is the wrong runtime for serverless because of its async-first approach. This approach is helpful for a traditional backend application where you can handle multiple web requests or operate on batches of queue messages in parallel. However, because Lambda works on a single event at a time, you usually aren't doing a lot of asynchronous work. You're processing a single event in order and thus can benefit from a more straightforward procedural style.</p><p>Ben's post is nearly 6 years old at this point, and some of the changes in Node.js since then (specifically async / await) help to sand off the rough edges of Node's model in a Lambda world. But regardless of the language you choose, you should consider how the single-event model affects your application.</p>",
  "neutralised_chunk": "- The choice of application code writing and potentially even coding language may change.\n- Ben Kehoe suggested that Node is not suitable for serverless due to its asynchronous-first approach.\n- This approach works well for traditional backend apps dealing with parallel web requests or batch queue messages processing, contrary to Lambda's one-event-at-a-time model.\n- Noble's model doesn't usually involve a lot of asynchronous work with Lambda - it processes a single event sequentially.\n- A more procedural style can benefit single event processing.\n- Though Node.js has evolved over six years, including async/await features, the single-event model's implications must be considered when choosing a language."
}
{
  "chunk": "<p>Because Lambda scales horizontally across function instances rather than vertically within function instances, you can't share resources across concurrent requests within your compute layer. The easy example here is a pool of database connections, which a traditional web server would use to share across many requests. This leads to the preference noted above where Lambda users prefer databases without connection limits (like DynamoDB) or to implement database pooling outside of their compute (such as via Amazon RDS Proxy).</p>",
  "neutralised_chunk": "- Lambda scales horizontally across function instances, preventing shared resources across concurrent requests within the compute layer.\n- A common example is a database connection pool, typically shared across many requests on a traditional web server.\n- This leads Lambda users to prefer databases without connection limits like DynamoDB, or implement database pooling outside their compute, such as Amazon RDS Proxy."
}
{
  "chunk": "<p>Further, because your compute is working on a single event at a time and is billed as long as it is active, you want to limit the time your application is doing nothing. No more using setTimeout() in your web server to handle a background job. More commonly, you'll want to avoid regular long-polling within your function. If you're waiting on something to be done before moving forward, see if you can turn it into an event itself. If you can't do that, try to implement the polling trigger outside of your compute.</p>",
  "neutralised_chunk": "- For serverless applications, avoid having compute resources idle while waiting, as you are billed for active time.\n- Avoid using setTimeout() in web servers for background jobs.\n- Avoid regular long-polling within the function.\n- If waiting for something before proceeding, try to turn it into an event.\n- If polling is required, implement the polling trigger outside of the compute function.\n- The goal is to minimize idle time and optimize billing for active compute time."
}
{
  "chunk": "<h2>Confusion over \"event\"\u200b</h2><p>Now we know about event-driven architecture and event-based compute, how do they interact? Do you have to use event-driven architectures with AWS Lambda? Can you use event-based compute with Kubernetes?</p><p>As mentioned above, I think the confusion in this area is not helped by the Lambda documentation that describes services to interact with Lambda. That document describes a number of sources as \"event-driven\" that do not fall under the traditional definition of event-driven, such as HTTP requests from API Gateway. Further, integration with most of the services that do utilize event-driven patterns are not defined as event-driven, such as DynamoDB Streams, Kinesis Streams, and Apache Kafka.</p><p>Luckily for you, I've made a handy Venn diagram to show how these patterns overlap.</p><p></p><p>In general, the rules are:</p><p>AWS Lambda functions are always event-based compute</p>",
  "neutralised_chunk": "- There's confusion about the interaction between event-driven architecture and event-based compute, including their usage with AWS Lambda and Kubernetes.\n- The Lambda documentation, which describes services interacting with Lambda, contributes to this confusion.\n- The documentation labels certain sources as 'event-driven' that don't match the traditional definition, like HTTP requests from API Gateway.\n- Likewise, many services using event-driven patterns (DynamoDB Streams, Kinesis Streams, Apache Kafka) are not characterized as event-driven.\n- A Venn diagram has been prepared to illustrate the overlap between these patterns.\n- AWS Lambda functions are always categorized under event-based compute."
}
{
  "chunk": "<p>You may use event-driven patterns with your AWS Lambda functions</p><p>You can use event-driven or non-event-driven patterns with compute other than AWS Lambda</p><p>I'm not trying to cover all the cases here. There are other tools (OpenFaaS, Knative) that allow for event-based compute. I'm less familiar with them, but many of the same principles are likely to apply.</p>",
  "neutralised_chunk": "- AWS Lambda functions can utilize event-driven patterns.\n- Compute methods other than AWS Lambda can use either event-driven or non-event-driven patterns.\n- The text doesn't cover all scenarios.\n- Other tools like OpenFaaS and Knative also allow for event-based compute.\n- The author has less familiarity with these, but assumes many of the same principles would apply."
}
{
  "chunk": "<h3>Lambda + AWS services: which are event-driven and why?\u200b</h3><p>After publishing this post, Emily Shea had a great suggestion to elaborate on why certain AWS services are or are not event-driven when connected with Lambda. Below is a quick overview. There are caveats here, but I try to outline the general shape of how a service is used with Lambda.</p><p>API Gateway + Lambda: Generally not event-driven. This is a request-response pattern as the client will receive a response from your Lambda function indicating the result of the request. If you're using API Gateway in a traditional REST API sense (e.g. \"Get User\", \"Add to Cart\", etc), it's not event-driven. It is synchronous and coupled.</p>",
  "neutralised_chunk": "- Emily Shea suggested clarification on why some AWS services may or may not be event-driven with Lambda.\n- A brief overview regarding the interaction of these services with Lambda is provided.\n- API Gateway + Lambda: Generally, this is not event-driven. It follows a request-response pattern where the client gets a response from the Lambda function after a request.\n- Traditionally, like in REST API operations (e.g., \"Get User\", \"Add to Cart\"), API Gateway is not event-driven but is synchronous and tightly coupled."
}
{
  "chunk": "<p>That said, you can use API Gateway as an entrypoint to an event-driven system. For example, a service or a frontend client could publish an event to an API Gateway endpoint, which would send it through to a Kinesis Stream or EventBridge bus. The big tell here will be the response code or body. If you get a response code of 202 Accepted or a response body that only includes an eventId or messageId property, it could indicate the API is a frontend to an event-driven system.</p><p>SQS + Lambda: Generally not event-driven. It is asynchronous, which differs from the synchronous request-response pattern above. However, a queue-based pattern usually has a specific task that the producer desires (e.g. \"Send a welcome email after user creation.\"). Because of this, it's a \"message-driven\" pattern. May also be called \"point-to-point\".</p>",
  "neutralised_chunk": "- API Gateway can act as an entry point to an event-driven system. \n- A service or frontend client can publish an event to an API Gateway endpoint which goes on to a Kinesis Stream or EventBridge bus.\n- The response code or body can serve as an indication of whether the API is a frontend to an event-driven system.\n- If a response code of 202 Accepted or a response body containing an eventId or messageId property is received, it could suggest the API being a frontend to an event-driven system.\n- SQS + Lambda is typically not event-driven but asynchronous.\n- However, this differs from the synchronous request-response pattern.\n- In a queue-based pattern, the producer usually has a specific task in mind (e.g. \"Send a welcome email after user creation.\").\n- This is referred to as a \"message-driven\" pattern and might also be called \"point-to-point\"."
}
{
  "chunk": "<p>There are caveats here as well. SQS might be used as a buffering / throttling mechanism as part of an event-driven system. For example, a consumer to SNS or EventBridge (discussed below) may push messages to SQS and process from there. SQS can provide greater control and visibility over throttling and retry logic.</p><p>SNS + Lambda: Event-driven! \ud83c\udf89 At least in theory. An SNS topic can have many consumers, and a publisher to an SNS topic usually doesn't have a specific outcome in mind. It is an asynchronous, decoupled pattern.</p><p>EventBridge + Lambda: Also event-driven! Basically the same points as SNS here. In practice, EventBridge is probably even more likely to be event-driven than SNS given its singular focus on an event bus with many different event types.</p>",
  "neutralised_chunk": "- SQS could be utilized as a buffering/throttling mechanism in an event-driven system, like a consumer to SNS or EventBridge may push messages to SQS for processing.\n- Using SQS can offer better control and visibility over throttling and retry logic.\n- SNS and Lambda, in theory, can create an event-driven system. An SNS topic can have multiple consumers, and a publisher to an SNS topic doesn't need to anticipate a specific outcome.\n- Similarly, EventBridge and Lambda can create an event-driven system. EventBridge, with its focus on an event bus with diverse event types, could be even more likely to be event-driven than SNS.\n"
}
{
  "chunk": "<p>Kinesis + Lambda: Often event-driven! Similar to SNS and EventBridge, but a stream-based solution like Kinesis allows for batch processing instead of individual events as a time. My hunch is that Kinesis is somewhat less likely to be used for 'pure' event-driven architectures as the asynchronous batch processing it enables is good for situations even when the producer and consumer know about each other.</p><p>Step Functions + Lambda: Not event-driven! A Step Function state machine executes a specific, defined, multi-step workflow. It may have some asynchronous elements, but there's no de-coupling of producers and consumers here! That said, you may want to introduce some event publishing in between steps of your state machine execution. Yan Cui's post on Choreography vs. Orchestration covers this topic and is one of my favorite serverless posts.</p><p>Thanks to Emily Shea for the suggestion and to Maik Wiesm\u00fcller for notes on the original writeup. \ud83d\ude4f</p>",
  "neutralised_chunk": "- Kinesis and Lambda are often used in event-driven architectures, similar to SNS and EventBridge.\n- Unlike SNS and EventBridge, Kinesis allows for batch processing of events.\n- Kinesis may not be as suitable for pure event-driven architectures due to its batch processing capability.\n- Step Functions and Lambda are not event-driven, and instead execute defined multi-step workflows.\n- Even though Step Functions are not event-driven, events can be published between steps of the state machine execution.\n- The post acknowledges Emily Shea for suggested content and Maik Wiesm\u00fcller for contributions to the original text."
}
{
  "chunk": "<h2>Conclusion\u200b</h2><p>In this post, we reviewed the difference between event-driven architectures and event-based compute. We saw that architectures with all types of compute can benefit from event-driven architectures (in the right situations!). Further we saw that all use of Lambda is relying on event-based compute. We also saw some of the implications of event-based compute and the ways in which it affects the design of your application.</p><p>If you have any questions or corrections for this post, please leave a note below or email me directly!</p>",
  "neutralised_chunk": "- The post covered the differences between event-driven architectures and event-based compute.\n- The benefits of event-driven architectures can be leveraged across various types of compute, given the right conditions.\n- All uses of Lambda depend on event-based compute.\n- The implications of event-based compute and its impact on application design have been discussed.\n- Feedback, questions, or corrections regarding the post are welcome."
}
{
  "chunk": "<h1>Why I (Still) Like the Serverless Framework over the CDK</h1><p>Over the past year or two, I've seen the AWS CDK turn a lot of my friends into converts. And in the very recent past, a few of these people have written up their (mostly positive) experiences with the CDK. See Maciej Radzikowski on why he stopped being a CDK skeptic here or Corey Quinn's list of the CDK's hard edges which, ultimately, is still favorable. For a more skeptical view of the CDK, check out Mike Roberts' excellent thoughts here.</p><p>Some of these articles describe similar concerns I have, but none of them quite nails my thoughts on the matter. I thought I'd throw my hat in the ring and describe why I still prefer the Serverless Framework over the CDK.</p><p>In this post, I'll start off with my biases and background to help frame my thinking. Next, I'll discuss four reasons why I think the Serverless Framework is a better abstraction for building serverless applications on AWS. Finally, I'll describe some areas that the CDK does do well.</p>",
  "neutralised_chunk": "- Background and biases framing the author's perspective on Serverless Framework vs AWS CDK.\n- Four reasons why the Serverless Framework is considered a better abstraction for building serverless applications on AWS.\n- Areas where the CDK performs well.\n- Comparison of the Serverless Framework and AWS CDK for serverless application development on AWS.\n- Contrasting views from other developers on their experiences with the CDK.\n- The author's preference for the Serverless Framework over the CDK, despite the CDK's growing popularity."
}
{
  "chunk": "<h2>My Background and Preferences\u200b</h2><p>Let's start with some facts that factor into my preferences. Your situation might not overlap with mine and, thus, some of my points may not be applicable to you.</p><p>First and foremost, I worked for Serverless, Inc., creators of the Serverless Framework, for about two and a half years. This is relevant! I learned a ton about AWS + serverless while working there, and I built a lot of knowledge around the Serverless Framework in particular. I don't have any financial interest in Serverless, Inc., but I do have friends that I like and respect there. In this case, familiarity breeds affinity.</p>",
  "neutralised_chunk": "- Background and personal preferences influence the author's views.\n- The author worked for Serverless, Inc., creators of the Serverless Framework, for around 2.5 years.\n- This experience provided significant knowledge about AWS and serverless, particularly with the Serverless Framework.\n- Although no longer financially invested, the author has friends at Serverless, Inc. and an affinity for the company.\n- Personal circumstances may differ, so some points may not be applicable to all readers."
}
{
  "chunk": "<p>Second, I am primarily building \"little-s\" serverless applications on AWS. The term \"serverless\" has gotten pretty slippery now that the marketers have descended, but I generally take this to mean \"applications which use primarily managed services glued together with AWS Lambda functions.\" Yes, it's true that Lambda != serverless, or that you can do \"functionless\" in your serverless application. I'm more cautious around functionless, but if that's what you're about, have at it! That said, the vast majority of serverless applications on AWS include Lambda at the core.</p>",
  "neutralised_chunk": "- The focus is on developing small scale serverless applications on AWS.\n- The term \"serverless\" is commonly interpreted as applications primarily using managed services combined with AWS Lambda functions.\n- Acknowledgement that Lambda does not equate to serverless, and one can create functionless serverless applications.\n- There is some caution towards functionless applications, but it is a viable option.\n- Despite variations, most serverless applications on AWS feature Lambda at the core."
}
{
  "chunk": "<p>More importantly for the discussion below is what my applications don't have. I very rarely have a VPC, which requires a lot of complicated, boilerplate-y configuration that is easy to screw up if you don't know what you're doing. I don't have security groups, or autoscaling pools, or ECS service definitions that might make using the CDK more pressing. I also don't have a need to create a lot of resources that are similar but slightly different, another area where the CDK could help.</p>",
  "neutralised_chunk": "- The applications discussed rarely use a VPC due to its complex, boilerplate configuration.\n- There are no security groups, autoscaling pools, or ECS service definitions.\n- The need to create similar yet subtly different resources is not present, which could otherwise be an area where the CDK offers help."
}
{
  "chunk": "<p>Finally, I have a pretty strong bias for boring in my programming. This claim might seem a little rich for some people, given that I advocate using AWS Lambda and DynamoDB for most applications. You might think something ought to be more commonplace before we call it boring. But I'd actually argue, in solidarity with Brian LeRoux, that Lambda and DynamoDB are fairly boring at this point. They may be newer than some technologies, but it's actually easier to wrap your head around their operating model and, more importantly, their failure modes than those of other technologies.</p><p>Another aspect of this boring bias is my preference against cleverness. My first programming language was Python, where explicit over implicit is written into the language itself. It took me a while to learn this lesson, but now I dislike too much indirection in programming. Be kind to your future self -- keep it simple.</p><p>These are my biases, and they may be very different from your own. If that's the case, feel free to take the rest of this post with a grain of salt.</p>",
  "neutralised_chunk": "- Preference for simplicity and avoiding complexity in programming (\"boring\" approach).\n- Advocating AWS Lambda and DynamoDB as relatively straightforward technologies with understandable operating and failure models.\n- Favoring explicit over implicit coding, stemming from Python background.\n- Avoiding excessive indirection or cleverness in programming.\n- Prioritizing simplicity to benefit future self.\n- Acknowledging personal biases and inviting readers to consider them when evaluating the content."
}
{
  "chunk": "<h3>A standard structure for every application\u200b</h3><p>We're already a few hundred words into this post, and I haven't even described how the Serverless Framework and the AWS CDK differ.</p><p>Both tools are basically pre-processors for AWS CloudFormation. Their core difference is in the process they use to get to CloudFormation.</p><p>With the Serverless Framework, you're writing a single, declarative YAML configuration file called serverless.yml that will describe your application. It will look something like the following:</p><p>It has a number of top-level blocks, including functions for creating Lambda functions and resources for provisioning other AWS resources.</p>",
  "neutralised_chunk": "- Both Serverless Framework and AWS CDK are pre-processors for AWS CloudFormation.\n- The core difference lies in their approach to generating CloudFormation templates.\n- Serverless Framework uses a declarative YAML configuration file called serverless.yml to describe the application.\n- The serverless.yml file contains top-level blocks for functions (Lambda) and resources (other AWS services).\n- It provides a standard structure for defining serverless applications."
}
{
  "chunk": "<p>The key abstraction is really in the functions block, as it focuses on simplifying the process around creating Lambda functions and hooking up event sources to these functions. This is the core of most serverless applications, and simplifying this process is a big win. We'll talk about this further in the next section.</p><p>The CDK takes a different approach. Rather than a declarative configuration file, the CDK has you write imperative code in a general-purpose programming language (TypeScript, Python, Golang, etc.) to describe the infrastructure you want. With this structure, you can use things like functions to abstract common operations or loops to create multiple resources with similar configuration. Further, you can have abstracted modules from other files or even other NPM packages that encapsulate resource creation logic.</p>",
  "neutralised_chunk": "- The key abstraction lies in the functions block, which simplifies creating Lambda functions and connecting event sources to them.\n- This is the core of most serverless applications, and simplifying this process is a significant benefit.\n- The CDK takes a different approach by using imperative code in a general-purpose programming language (TypeScript, Python, Golang, etc.) to describe the desired infrastructure, instead of a declarative configuration file.\n- With this structure, you can use functions to abstract common operations, loops to create multiple resources with similar configurations, and abstracted modules from other files or NPM packages to encapsulate resource creation logic."
}
{
  "chunk": "<p>I like the Serverless Framework because I always know where I am. If I come to an existing project that I've never seen before, I can look for the serverless.yml file and understand the structure of the application -- what functions there are, where they're located in the repository, how they're triggered, and what supporting resources they use.</p><p>If you're from the Rails ecosystem, you might think of the Serverless Framework's approach as 'convention over configuration'. Mohamed Said recently made the same point about Laravel applications:</p><p>The biggest reason why following the Laravel convention is so important is that I can look into a team's work after weeks and I know exactly where to look to find things.</p>",
  "neutralised_chunk": "- The Serverless Framework offers an understanding of the application structure with the help of a serverless.yml file. This can reveal functions, their location in the repo, triggers, and used resources.\n- The framework operates on a 'convention over configuration' approach, similar to the Rails ecosystem.\n- Adhering to conventions allows easy comprehension of a project even after weeks, as seen in the use of Laravel applications."
}
{
  "chunk": "<p>In contrast, every CDK repo I've looked at turns into a murder mystery. I'm digging through application code that is mixed with infrastructure code, trying to find my function handlers, my service code, anything that will help me solve The Case of the Missing IAM Permission.</p><p>Some people have discussed the long-term issues around CDK maintainability -- what if people aren't updating your constructs or staying on top of CDK version updates?</p><p>Those issues are real, but I'm way more worried about how you maintain developer knowledge around a specific CDK application. There are real hurdles to onboarding a new developer into your custom structure. They don't just need to know the CDK, they need to know your team's specific implementation of the CDK.</p><p>The knowledge of your application that is built up within your team is hidden, and it will result in slower ramp-up times for new developers.</p>",
  "neutralised_chunk": "- CDK repositories can be complex to navigate, with application and infrastructure code intertwined.\n- Locating specific code, such as function handlers or service code, can be challenging.\n- There are concerns about long-term maintenance issues with CDK, like the updating of constructs or version updates.\n- A bigger concern is maintaining developer knowledge for a specific CDK application.\n- Onboarding a new developer into a custom CDK structure has challenges because they need to grasp the team's specific implementation.\n- The team's hidden knowledge of the application could lead to slower onboarding times for new developers."
}
{
  "chunk": "<h3>The Serverless Framework abstracts the right things\u200b</h3><p>I mentioned in the last section that both the Serverless Framework and the CDK are abstractions over CloudFormation. In my opinion, the Serverless Framework does a better job of abstracting the right things to make it easier to work with serverless applications.</p><p>There are two hard things about deploying serverless applications:</p><p>The Serverless Framework has rightly identified that functions and events are core to serverless applications and has made them the core abstraction. Let's look at another example functions block in a serverless.yml file:</p>",
  "neutralised_chunk": "- The Serverless Framework and the CDK are both abstractions over CloudFormation.\n- The Serverless Framework is believed to be superior, offering easier handling of serverless applications.\n- The complexity of deploying serverless applications is acknowledged.\n- The Serverless Framework focuses on functions and events, considering them as core to serverless applications.\n- An example functions block in a serverless.yml file will be examined next."
}
{
  "chunk": "<p>When I run serverless deploy to deploy my stack, it's going to build and configure a function on AWS for each function in my functions block (here, there are three: createUser, processQueue, and fanout). For each function, it may install your dependencies, run a build process, zip up the directory, upload it to S3, and register the function and its basic configuration with the Lambda service. You don't really need to know much about that packaging process, unless you have specific needs.</p><p>To be fair to the CDK, some of their function abstractions, like the NodeJsFunction construct do most of this too. Anecdotally from friends, I've heard that the function build part of CDK is under-loved compared to the core infrastructure deployment part, but it's functional.</p><p>One example of this is from Roman here:</p><p>That\u2019s good defaults I think. How\u2019s your overall experience with these lambda constructs? I find it still problematic to build, test and publish ts lambdas</p>",
  "neutralised_chunk": "- Running serverless deploy builds and configures an AWS function for each function in the functions block (createUser, processQueue, and fanout are given as examples).\n- The process may include installing dependencies, running a build process, zipping the directory, uploading it to S3, and registering the function with basic configurations on the Lambda service.\n- Understanding the packaging process is not crucial unless there are specific needs.\n- The CDK's function abstractions, such as the NodeJsFunction construct, also perform most of these tasks.\n- There's an opinion that the function build part of CDK is not as well-developed as the core infrastructure deployment part, though it's functional.\n- A user named Roman found it problematic to build, test, and publish TypeScript lambdas using lambda constructs."
}
{
  "chunk": "<p>The bigger difference between the two is in the second hard part of serverless applications -- configuring events to trigger your functions.</p><p>In the serverless.yml example above, there's a consistent format for potential trigger. Each function has an events property where you can configure an array of events (though it's usually one per function). Each configured event has a type and any required properties for the event.</p><p>Thus, configuring a Lambda function to connect to API Gateway to handle HTTP requests feels very similar to configuring a function to consume from an SQS queue or to handle SNS notifications.</p><p>And sometimes these configurations are quite complex! Creating a Lambda function to handle an API Gateway endpoint requires creating 6-8 resources, including two sub-resources on an API Gateway instance (a Resource and a Method), an IAM role for your function, plus a resource-based permission on your function so API Gateway can invoke it.</p>",
  "neutralised_chunk": "- Configuring events to trigger functions is a key challenge in serverless applications.\n- The serverless.yml example shows a consistent format for potential triggers.\n- Each function has an events property to configure an array of events (usually one per function).\n- Each event has a type and required properties specified.\n- Configuring a Lambda function for API Gateway HTTP requests is similar to configuring for SQS queues or SNS notifications.\n- These configurations can be complex, e.g., API Gateway requires 6-8 resources like sub-resources, IAM role, and permissions."
}
{
  "chunk": "<p>That's a lot! With the Serverless Framework, that's abstracted away from you. Further, it's not abstracted in a way that harms you (as we'll discuss in the next section).</p><p>If you compare configuring these three events types in the CDK, you'll see there are three completely different ways to manage it.</p><p>With API Gateway, you need to create the API Gateway REST API instance. Then, you need to create a resource on the API (or multiple resources, if this is a nested route). Finally, you need to add a method on your created resource that integrates with your created Lambda function. Check an example of this from Borislav Hadzhiev here.</p><p>If you're doing an SQS integration, the format is completely different. Under the hood, certain Lambda integrations use event source mappings, and the CDK exposes that directly to you. Thus, connecting a Lambda function to an SQS queue looks like this:</p><p>Source: CDK docs</p>",
  "neutralised_chunk": "- The Serverless Framework abstracts complex tasks without causing disadvantages, which will be discussed further.\n- Comparing the configuration of three event types in the CDK reveals three distinct management methods.\n- API Gateway requires creating the REST API instance, creating a resource on the API, and adding a method on the created resource that integrates with the Lambda function.\n- For SQS integration, the format changes entirely. Certain Lambda integrations use event source mappings, exposure of which is done by the CDK.\n- Thus, connecting a Lambda function to an SQS queue follows a different process.\n- Source: CDK docs."
}
{
  "chunk": "<p>Notice that you create a Lambda function, create an SQS Queue, and then create a third thing to connect them together. Notice that this third thing, the event source mapping, is attached to the Lambda function.</p><p>If we go to the third type of event source, connecting to an SNS topic, we find a third pattern:</p><p>Source: CDK docs</p><p>Similar to SQS, we create a Lambda function and the related resource (an SNS topic). However, to connect them, we attach the function to the resource (rather than the resource to the function, like in SQS).</p><p>In each of these three examples, CDK more accurately reflects the resources required to create and configure these integrations. However, I think it retains some complexity that can easily be abstracted away. For the most part, I don't need to know about event source mappings vs. API Gateway Lambda integrations (though I do need to know about the various types of error handling across different services!).</p>",
  "neutralised_chunk": "- Lambda functions are created, along with an SQS Queue.\n- A third element, an event source mapping, is created to connect the Lambda function and SQS Queue, attached to the Lambda function.\n- For SNS topic integration, a Lambda function and an SNS topic are created.\n- The Lambda function is attached to the SNS topic, rather than the other way around like with SQS.\n- In all three cases, CDK accurately represents the resources required for these integrations.\n- However, there is complexity that could be abstracted away, such as event source mappings vs. API Gateway Lambda integrations.\n- While the specifics of error handling across different services need to be understood, the details of connecting resources may not always be necessary."
}
{
  "chunk": "<p>I think this leads to a confusing mental model of how serverless applications on AWS should be built. You should think of it as a Lambda-centric model (or perhaps \"compute-centric\", if you're a functionless fanatic) where you're using lots of managed services but tying them together via Lambda. With the Serverless Framework, Lambda functions and their events are the core, and the supplemental resources are the leaf nodes. With CDK, it puts all resources on the same footing, making it harder to organize how your application works.</p>",
  "neutralised_chunk": "- The typical way of building serverless applications on AWS can be confusing.\n- The build should be Lambda-centric or compute-centric if going functionless, using numerous managed services tied together via Lambda.\n- In the Serverless Framework, Lambda functions and events are the core and the additional resources are secondary.\n- CDK treats all resources equally, which can create organizational challenges for your application functioning."
}
{
  "chunk": "<h3>The CDK abstracts the wrong things\u200b</h3><p>On the flip side of the previous section, I also think that the CDK abstracts the wrong things.</p><p>But first, I have a confession. In the previous section, I made the Serverless Framework event configuration look slightly easier than it is. For both the SQS and SNS integrations, we're referring to additional infrastructure in our application -- an SQS queue and an SNS topic.</p><p>While the Serverless Framework looks at functions and events as the core of your serverless applications, it also realizes you will need additional supporting infrastructure to do anything meaningful. That's where the resources block of your serverless.yml comes in.</p><p>In the resources block, you can configure any additional AWS resource you want via straight CloudFormation. For our SQS- and SNS-enabled application above, our resources block would look as follows:</p>",
  "neutralised_chunk": "- The Cloud Development Kit (CDK) may not abstract the optimal features.\n- The previous section possibly oversimplified the Serverless Framework event configuration.\n- The SQS and SNS integrations reference additional infrastructure in an application, namely, an SQS queue and an SNS topic.\n- The Serverless Framework acknowledges the need for extra supporting infrastructure for serverless applications through a resources block in serverless.yml.\n- The resources block allows users to configure any additional AWS resources through CloudFormation."
}
{
  "chunk": "<p>The part I love about this is that you're actually writing CloudFormation. You're learning the tool that you'll be using under the hood.</p><p>There are a lot of complaints about CloudFormation, but they mostly center on the verbosity of it -- there's just so much I have to write. And it's boilerplate!</p><p>But as we saw in the last section, most of the boilerplate CloudFormation has been abstracted from you in functions and events. The resources you create in the resources block are pretty flat and direct. There's not a lot you can abstract away in SQS queues, SNS topics, or DynamoDB tables. (For the latter, AWS SAM tried with the SimpleTable resource, but it turns out the other properties of DynamoDB tables are pretty useful!)</p><p>Additionally, your Lambda functions will need to have the requisite IAM permissions to interact with your resources, and you'll need to specify those in IAM-policy format using the iam block in serverless.yml.</p>",
  "neutralised_chunk": "- Writing CloudFormation is a beneficial aspect as it helps learn the underlying tool.\n- Common complaints about CloudFormation relate to its verbosity and extensive boilerplate.\n- Much of the boilerplate in CloudFormation is abstracted by functions and events.\n- The resources created are straightforward, with little scope for abstraction in SQS queues, SNS topics, and DynamoDB tables.\n- AWS SAM attempted abstraction with the SimpleTable resource for DynamoDB, but the other properties are useful.\n- Lambda functions require IAM permissions to interact with resources, which need to be specified in IAM-policy format in the iam block in serverless.yml."
}
{
  "chunk": "<p>I think part of the attraction of CDK is that it abstracts both raw CloudFormation and IAM. Rather than writing declarative YAML, you're invoking a method with some properties. And rather than detailing the specific IAM statements you need, you're doing something like myTable.grantReadWriteData(myFunction) to allow your function to read and write to your DynamoDB table.</p><p>But at some point, you need to learn those fundamentals. If your deployment fails, it's going to fail in a CloudFormation-specific way. If your permissions don't work, it's going to tell you the specific resource and IAM action combination that was disallowed. If that is completely foreign to you, you're not going to be able to debug effectively.</p><p>AWS is a massive ecosystem -- thousands of services with intricate connections, complex auth requirements, and loads of pitfalls. You can do some amazing things, but it's truly daunting to newcomers.</p>",
  "neutralised_chunk": "- CDK abstracts away raw CloudFormation and IAM details, allowing developers to invoke methods with properties instead of writing declarative YAML.\n- For granting permissions, CDK simplifies the process by using methods like grantReadWriteData to allow a function access to a DynamoDB table, rather than specifying individual IAM statements.\n- However, at some point, developers need to understand the underlying fundamentals of CloudFormation and IAM.\n- If a deployment fails, the error will be specific to CloudFormation, requiring knowledge of its terminology and concepts for effective debugging.\n- Similarly, if permissions are incorrect, the error will reference specific resources and IAM actions, which may be unfamiliar to those without prior knowledge.\n- AWS is a vast ecosystem with thousands of services, intricate connections, complex authentication requirements, and numerous pitfalls, making it truly daunting for newcomers.\n- While CDK provides an abstraction layer, understanding the underlying fundamentals is essential for effectively working with AWS and troubleshooting issues."
}
{
  "chunk": "<p>I was able to massively upskill my AWS ability through the Serverless Framework, and I've seen the same for others. With the Serverless Framework, you can get started with a simple function and event connection quickly, without learning the specifics of CloudFormation and IAM. Then, you can gradually layer in new pieces -- as you want persistent storage, you add a DynamoDB table resource and the needed PutItem and GetItem permissions. It's all incremental, so you're learning as you go, rather than tossing you into the deep end once something inevitably goes wrong.</p>",
  "neutralised_chunk": "- The Serverless Framework enabled significant AWS skills improvement.\n- It allows for a quick start with a simple function and event connection, without needing to learn CloudFormation and IAM details initially.\n- As needs grow, new components like DynamoDB tables and associated permissions can be incrementally added.\n- Learning happens gradually, as new features are introduced, rather than being overwhelmed by complexity upfront.\n- The incremental approach avoids being thrown into the deep end when issues arise."
}
{
  "chunk": "<h3>The CDK enables our bad impulses\u200b</h3><p>The previous section was about how the Serverless Framework helps beginners to AWS. But you, dear reader, may not be a beginner. Maybe you think you don't need the training wheels that the Serverless Framework provides. I'm here to tell you that you may need the structure that the Serverless Framework provides more than anyone.</p><p>CDK proponents love to mention that you can write code for your infrastructure. But one of my biggest problems with the CDK is that you can write code for your infrastructure. I don't mean to turn into a jaded old man, but I've seen some stuff in CDK applications. Stuff that would give you nightmares.</p><p>As engineers, we can be too clever for our own good. We can fall in love with abstraction, with DRY-ing up our code, or with beautiful code. But too often, these elegant edifices are impenetrable to outsiders or difficult to maintain over time.</p>",
  "neutralised_chunk": "- The CDK may encourage harmful tendencies.\n- The Serverless Framework provides helpful structure, which is beneficial even for non-beginners.\n- The ability to code your infrastructure is a double-edged sword in the CDK.\n- Real-world CDK applications may feature elements that cause concerns.\n- Engineers can unnecessarily complicate things with abstraction, excessive focus on DRY principles, or obsession over writing aesthetically pleasing code.\n- These sophisticated constructs can create barriers for newcomers or make long-term maintenance difficult."
}
{
  "chunk": "<p>There are two specific problems I've seen with the CDK, though I'm sure there are others.</p><p>The first is that it's too easy to create a mess of resources without understanding the underlying purpose or intent. The most common place I see this is with an inexplicable explosion of CloudFormation Stacks. One CDK application I saw used seven different stacks for a pretty simple service (~8 HTTP endpoints, an S3 bucket, a DynamoDB table, and a few state machine definitions). Note that these weren't CloudFormation Nested Stacks, which are inherently tied to one another, but were separate, independent stacks, which made deployment (and failures in deployment) much more complicated.</p>",
  "neutralised_chunk": "- There are a couple of prominent issues with the CDK, although other issues may exist.\n- The first issue is the potential for a clutter of resources without comprehension of their underlying purpose or intent.\n- An example of this issue is the common occurrence of an undue increase in CloudFormation Stacks.\n- An illustration is a CDK application that used seven different stacks for a relatively minor service involving around 8 HTTP endpoints, an S3 bucket, a DynamoDB table, and several state machine definitions.\n- These stacks were not CloudFormation Nested stacks, which are inherently interlinked, but were separate, independent stacks leading to complexity in deployment and deployment failures."
}
{
  "chunk": "<p>The second is in over-emphasizing abstractions that can be reused across teams. This is a core concept in CDK -- there are different levels of 'constructs' from low-level CloudFormation resources (\"L1 constructs\") to highly specific patterns (\"L3 constructs\"). People get fired up about this because they think they can write this One Resource to Rule Them All to solve all their problems, but the reality is a lot trickier.</p><p>I think the average software developer (and I include myself in this group!) is actually not that great at writing reusable, library code. This is doubly true for the average CDK-adjacent developer (again, I'm in this group!) who is either a DevOps-y shaped engineer that's doing some product work or a full-stack-y engineer that's learning just enough AWS to be dangerous. Few developers boast a deep knowledge of cloud infrastructure combined with the ability to create useful abstractions.</p>",
  "neutralised_chunk": "- The second issue is overemphasizing reusable abstractions across teams, a core CDK concept with different levels of constructs.\n- People believe they can write a single resource to solve all problems, but reality is more complex.\n- The average software developer, including DevOps and full-stack engineers, struggles to write truly reusable, library code.\n- Few developers possess deep knowledge of cloud infrastructure combined with the ability to create useful abstractions."
}
{
  "chunk": "<p>Taylor Otwell, creator of Laravel, shared a screenshot of an HN discussion recently that hit on similar points:</p><p>Suspect this is also very true for Laravel projects. \ud83d\ude05 pic.twitter.com/reyAWYkH6U</p><p>Good, solid abstractions are hard to make. But they're sooo tempting to try. In most cases, you're better off relying on proven, tested abstractions rather than trying to write your own.</p>",
  "neutralised_chunk": "- Taylor Otwell, the creator of Laravel, shared a screenshot of a Hacker News discussion related to Laravel projects.\n- The discussion highlighted the difficulty in creating good, solid abstractions.\n- Otwell acknowledged that the points made in the discussion are likely true for Laravel projects as well.\n- Creating effective abstractions is challenging, but the temptation to try is strong.\n- In most cases, it is better to rely on proven, tested abstractions rather than attempting to write new ones from scratch."
}
{
  "chunk": "<h2>What the CDK does well\u200b</h2><p>I've spent a lot of time talking about what I don't love about the CDK, but it can't be all bad. The CDK is getting a lot of adoption in the AWS circles in which I travel, and many people whom I respect are loving the CDK. I do want to outline some of the benefits. And because I refuse to have any fun, I'll also point out the downside of each one.</p><p>Given that, what is the CDK doing well that is accounting for its popularity? There are three core areas I see.</p><p>First and foremost, the CDK removes some of the drudgery of infrastructure-as-code. As much as I like the Serverless Framework, writing YAML can be boring. Moving from declarative template-land into imperative coding-land can be a lot more fun. Plus, there are some benefits around auto-complete, and the L2 constructs can help with the low-level bits of gluing these things together.</p>",
  "neutralised_chunk": "- The CDK (AWS Cloud Development Kit) is gaining significant adoption despite some criticisms.\n- Benefits of the CDK:\n  - Removes drudgery of infrastructure-as-code\n    - Writing YAML can be boring\n    - Imperative coding can be more enjoyable\n    - Benefits of auto-complete and L2 constructs for low-level gluing\n  - However, the imperative approach can be more verbose and complex"
}
{
  "chunk": "<p>I'm cautious about this benefit, because fun does not necessarily equal good. Further, as mentioned above, I think the Serverless Framework abstracts a lot of the true drugery of little-s serverless applications. However, we have to acknowledge this impetus as a real factor. And if you're a more experienced AWS engineer that understands IAM, CloudFormation, and the IaC process, you're not losing as much by going to the CDK.</p><p>A second benefit of the CDK is an easier way to create lots of similar resources. While on the excellent aws.fm podcast, Matthew Bonig noted he had a situation where a team was making hundreds of similar RDS instances. The copy-pasta required to create all of these or to encode best practices can be difficult, and the CDK makes this much easier than a more declarative format.</p>",
  "neutralised_chunk": "- Fun as a benefit must be approached with caution as it doesn't necessarily equate to quality.\n- The Serverless Framework simplifies many aspects of small serverless applications.\n- The drive to use the CDK, especially for experienced AWS engineers familiar with IAM, CloudFormation, and the IaC process, is recognized as a valid reason.\n- Another advantage of CDK is that it simplifies the process of creating many similar resources.\n- As highlighted in the aws.fm podcast, using CDK can reduce the tedium of creating hundreds of similar RDS instances, providing an easier alternative to a more declarative format."
}
{
  "chunk": "<p>That said, I've only had this situation come up one time in my ~6 years of doing serverless development, and even that was for a resource that had 6-7 similar instances. The slight copy-pasta required to maintain it wasn't a burden. But if this applies to you, go for it!</p><p>A third, more common, benefit is the ability to abstract truly boilerplate stuff. If you're creating a VPC or certain other AWS solutions that require a ton of configured resources, being able to abstract that in a single construct that has sane defaults is a big benefits.</p><p>The main downside to this is that you still are responsible for owning that infrastructure! The CDK can make it easier on Day 1, but that infra is still yours on Day 2.</p><p>It reminds me of this tweet from Gwen Shapira:</p>",
  "neutralised_chunk": "- Encountered a single issue in approximately six years of serverless development related to maintaining resources with similar instances. \n- Addressing the issue was not particularly demanding. \n- A significant advantage is the ability to abstract repetitive elements. \n- When creating solutions like a VPC that require numerous configured resources, abstraction into a single construct with reasonable defaults is highly beneficial. \n- However, the ownership of the infrastructure remains with the user, despite simplifications offered by the CDK.\n- A relevant tweet from Gwen Shapira is mentioned."
}
{
  "chunk": "<p>All AWS EKS \"get started\" docs tell me to run CloudFormations. I ended up with 32 resources - mostly network related. I have no idea why they all exist and what each does. How will I debug things if they break? or help someone else if my stuff doesn't work for them?</p><p>Gwen is talking about a standard CloudFormation template, but the point is the same. If you're using something off the shelf to avoid learning about it, it's a ticking time bomb as to when you'll actually have to learn it. And at that point, the pressure will be higher.</p><p>Finally, I like the experimentation that the CDK is doing. Tech and software moves in fits and starts, and it's hard to predict what will work. There have been lots of tech trends that I've been skeptical of or downright hostile to. Sometimes I was wrong!</p><p>Infrastructure as code is not a solved problem yet. Let a thousand flowers bloom.</p>",
  "neutralised_chunk": "- AWS EKS \"get started\" documentation recommends running CloudFormation templates.\n- These templates create numerous resources, mostly network-related, without explaining their purpose.\n- When issues arise, it becomes challenging to debug or assist others due to lack of understanding.\n- Using off-the-shelf solutions without learning the underlying concepts can lead to difficulties down the line.\n- The AWS CDK (Cloud Development Kit) promotes experimentation and exploration of new approaches.\n- Technology and software evolve through trial and error, making it difficult to predict what will work.\n- Initial skepticism or hostility towards new trends may sometimes be proven wrong.\n- Infrastructure as code is still an unsolved problem, and various approaches should be explored."
}
{
  "chunk": "<h2>Conclusion\u200b</h2><p>In this post, I described why I still prefer the Serverless Framework for building serverless applications on AWS. I prefer the way it constrains our impulses, its convention over configuration, and the way it incrementally teaches the cloud to users. I also did a quick run through of things the CDK does well.</p><p>I realize opinions will differ on this, and that's OK! I'm here to describe the tradeoffs as I see them and in light of my situation and preferences. If yours differ, then your conclusion might as well. Hit me up and let me know why I'm wrong :)</p><p>Thanks to Matt Bonig for his comments on this post. Be sure to check out The CDK Book, an excellent resource he wrote with three other smart people. All mistakes are mine.</p><p>If you have any questions or corrections for this post, please leave a note below or email me directly!</p>",
  "neutralised_chunk": "- The post explains the preference for the Serverless Framework for building serverless applications on AWS, due to the way it constrains impulses, leans towards convention over configuration, and how it educates users about the cloud.\n- The CDK's strengths were also briefly acknowledged.\n- The conclusions drawn are subjective and based on the author's experiences and preferences; different perspectives may lead to different conclusions.\n- The author appreciates alternative views and invites feedback.\n- Errors in the post are the author's responsibility.\n- Any questions or corrections can be directed to the author via comment or email.\n- The post also acknowledges and thanks Matt Bonig for his input and for his co-authored resource, The CDK Book."
}
{
  "chunk": "<h1>Key Takeaways from the DynamoDB Paper</h1><p>In 2007, a group of engineers from Amazon published The Dynamo Paper, which described an internal database used by Amazon to handle the enormous scale of its retail operation. This paper helped launch the NoSQL movement and led to the creation of NoSQL databases like Apache Cassandra, MongoDB, and, of course, AWS's own fully managed service, DynamoDB.</p><p>Fifteen years later, the folks at Amazon have released a new paper about DynamoDB. Most of the names have changed (except for AWS VP Swami Sivasubramanian, who appears on both!), but it's a fascinating look at how the core concepts from Dynamo were updated and altered to provide a fully managed, highly scalable, multi-tenant cloud database service.</p><p>In this post, I want to discuss my key takeaways from the new DynamoDB Paper.</p><p>There are two main areas I found interesting from my review:</p><p>This post will be at a higher-level than the paper, though I strongly recommend reading the paper itself. It's really quite approachable. Additionally, Marc Brooker has written a nice review post that includes some interesting systems-level thoughts on the paper.</p>",
  "neutralised_chunk": "- The 2007 Dynamo Paper from Amazon engineers helped launch the NoSQL movement and led to the creation of databases like Apache Cassandra, MongoDB, and DynamoDB.\n- A new paper on DynamoDB has been released by Amazon, providing insights into how the core concepts from Dynamo were adapted for a fully managed, highly scalable, multi-tenant cloud database service.\n- The post highlights key takeaways from the new DynamoDB Paper, focusing on two main areas of interest.\n- The post aims to provide a higher-level overview of the paper, while recommending readers to review the original paper and Marc Brooker's review for additional details.\n- Background: The Dynamo Paper described an internal database used by Amazon to handle the enormous scale of its retail operation, leading to the development of DynamoDB and other NoSQL databases."
}
{
  "chunk": "<h2>Product-level takeaways from the DynamoDB Paper\u200b</h2><p>Both the Dynamo paper and the DynamoDB paper describe some incredible technical concepts, but I'm equally impressed by the discussion of user needs. In both papers, there is a deep review of existing practices to see what is important and what should be re-thought around core user needs.</p><p>In the Dynamo paper, we saw this in the articulation that much of the higher-level querying functionality provided by an RDBMS is unused by Amazon's services. Werner Vogels expanded on this later as he mentioned that 70% of database operations were single-record lookups using a primary key, and another 20% read a set of rows but only hit a single table.</p>",
  "neutralised_chunk": "- The Dynamo and DynamoDB papers detail impressive technical concepts along with in-depth discussions on user needs.\n- Both papers examine established practices to evaluate their significance and potential need for reconsideration.\n- The Dynamo paper revealed the lesser use of certain higher-level querying functions of an RDBMS within Amazon's services.\n- Werner Vogels later emphasized that 70% of database operations were single-record primary key lookups and another 20% read multiple rows from a single table."
}
{
  "chunk": "<p>The Dynamo paper also noted that the traditional guarantee of strong consistency, while critical in some circumstances, was not necessary for all applications. In many cases, the enhanced availability and reduced write latency achieved by relaxing consistency requirements was well worth the tradeoff.</p><p>Just as the Dynamo paper re-examined some shibboleths from traditional database systems, the DynamoDB paper explores user needs around what was needed to make the Dynamo learnings applicable more broadly. In doing so, DynamoDB was able to institute a set of clear product priorities that distinguish DynamoDB from many other database offerings.</p><p>There are three important notes on user needs that I took from the paper:</p>",
  "neutralised_chunk": "- The original Dynamo paper highlighted that the strong consistency guarantee isn't necessary for all applications.\n- Enhanced availability and reduced write latency achieved by relaxing consistency requirements can provide significant benefits.\n- The DynamoDB paper re-assessed traditional database systems concepts and evaluated user needs to broaden Dynamo's application.\n- As a result, DynamoDB established unique product priorities setting it apart from many other databases.\n- The paper provides three critical observations on user needs."
}
{
  "chunk": "<h3>The importance of consistent performance\u200b</h3><p>One point that the DynamoDB paper hammers over and over is that, for many users, \"consistent performance at any scale is often more important than median request service times.\" Stated differently, it's better to have a narrower range between median and tail latency than it is to reduce median (or even p90 or p95) latency.</p><p>This is a surprising point to some people who think \"DynamoDB is super fast.\" The story is more nuanced than that.</p><p>In my DynamoDB talks, I often show the following:</p><p></p><p>In this chart, we see that RDBMS latency will get worse as the amount of data in your database increases, whereas DynamoDB will provide consistent performance as your data increases. This same relationship holds as the number of concurrent requests increases.</p>",
  "neutralised_chunk": "- Consistent performance at any scale is often more important than median request service times for many users.\n- It's better to have a narrower range between median and tail latency than reducing median, p90, or p95 latency.\n- DynamoDB's performance story is more nuanced than just being \"super fast.\"\n- RDBMS latency worsens as the amount of data in the database increases, while DynamoDB provides consistent performance as data grows.\n- The same relationship holds as the number of concurrent requests increases.\n- The key point is DynamoDB's consistent performance at scale, versus varying latency in RDBMS as data and concurrency grow."
}
{
  "chunk": "<p>The chart above is a rough sketch, but the overall point stands. At certain levels of data and transaction volume, an RDBMS will have faster response times than DynamoDB. Conceptually, this is easy to understand. A request to DynamoDB will pass through a number of systems -- a request router, a metadata system, an authentication system -- before making it to the underlying physical storage node that holds the requested data. While these systems are highly optimized, each one of them adds latency to the overall request. Conversely, a single-node RDBMS can skip a lot of that work and operate directly on local data.</p><p>So MySQL might beat DynamoDB at the median, but that's not the full story. There are two reasons you should also consider the full spectrum of your database's performance.</p>",
  "neutralised_chunk": "- At certain data and transaction volumes, an RDBMS may have faster response times than DynamoDB.\n- This is conceptually understandable, as a DynamoDB request passes through multiple systems (request router, metadata system, authentication system) before reaching the physical storage node, adding latency.\n- In contrast, a single-node RDBMS can operate directly on local data, avoiding much of this overhead.\n- While MySQL may outperform DynamoDB at the median, it's important to consider the full spectrum of database performance for two reasons."
}
{
  "chunk": "<p>First, tail latencies are important, particularly in architectures with lots of components and sub-components. If a single call to your backend results in a lot of calls to underlying services, then each request is much more likely to experience the tail latency from some service, resulting in a slower response.</p><p>Second, the consistency and predictability of DynamoDB's performance profile leads to less long-term maintenance burden on your service. You don't have to come back to investigate, tune, and refactor as your performance inevitably declines. You know you'll get the same performance in your test environment as you will five years after launch, allowing you to focus on more value-generating features for your users.</p>",
  "neutralised_chunk": "- Tail latencies are crucial, especially in architectures with many components and sub-components.\n- If a single backend call triggers multiple calls to underlying services, the request is more likely to experience tail latency from some service, resulting in slower responses.\n- DynamoDB's consistent and predictable performance profile reduces long-term maintenance burden on the service.\n- There is no need to investigate, tune, and refactor as performance declines over time.\n- The performance in the test environment will match the performance years after launch, allowing focus on value-generating features for users."
}
{
  "chunk": "<h3>Fully managed over self-managed\u200b</h3><p>If you're reading this blog, you're probably drinking the cloud Kool-Aid and may even be fully into the serverless world. In the serverless world, we're as focused as possible building the key differentiators of our business while offloading the undifferentiated heavy lifting to others.</p><p>But the internal experience of Amazon retail and the creators of Dynamo (not DynamoDB) really drives this home.</p><p>Recall that the Dynamo paper was hugely influential in the industry, and Amazon's internal Dynamo system was a big improvement in terms of availability and scalability for the enormous scale at which Amazon was operating.</p><p>Despite this improvement, many internal engineers chose to eschew running Dynamo themselves in favor of using Amazon SimpleDB, which was AWS's first foray into the hosted NoSQL database market.</p>",
  "neutralised_chunk": "- The topic discusses the advantages of fully managed services over self-managed services in cloud and serverless contexts.\n- Serverless environments aim to focus on unique business elements and delegate undifferentiated tasks.\n- The transformation within Amazon retail and the creators of Dynamo (not DynamoDB) illustrates this idea.\n- The Dynamo system had a substantial impact and significantly improved Amazon's scalability and availability at a vast scale.\n- Despite these advancements, many engineers preferred using Amazon SimpleDB (AWS's initial entry into the hosted NoSQL database market) over managing Dynamo themselves."
}
{
  "chunk": "<p>If you've never heard of Amazon SimpleDB, you're not alone. DynamoDB is essentially a successor to SimpleDB and is superior in nearly every aspect. AWS rarely markets it anymore, and it's mostly a mascot for how AWS will never deprecate a service, even when better options exist.</p><p>SimpleDB has some truly surprising downsides, such as the fact that the entire database cannot exceed 10GB in size. This is a huge limitation for most applications but particularly for a company who has applications so large that they had to design a completely new database. Yet engineers were choosing to use batches of multiple SimpleDB tables to handle their needs, likely sharding at the application layer to keep each database under 10GB.</p><p>This had to add significant complexity to application logic. Despite this, engineers still chose to use SimpleDB over operating their own Dynamo instance.</p><p>This revealed preference by Amazon engineers helped spur the development of Amazon DynamoDB, a database that combined the scalability of Dynamo with the fully managed nature of SimpleDB.</p>",
  "neutralised_chunk": "- Amazon SimpleDB is a lesser-known service; it is succeeded, and in most aspects, surpassed, by DynamoDB.\n- Despite being less promoted by AWS, SimpleDB signifies AWS' policy of never deprecating a service, even if superior ones exist.\n- SimpleDB has major drawbacks, one of them is a strict 10GB size limit for the entire database.\n- This limit was a significant issue, especially for large-scale applications - yet, engineers opted to use multiple SimpleDB tables, likely implementing sharding at the application layer.\n- While this added complexity to the application logic, engineers still preferred SimpleDB over operating their own Dynamo instance.\n- This pattern influenced the development of Amazon DynamoDB, a database merging the scalability of Dynamo with the fully managed characteristic of SimpleDB."
}
{
  "chunk": "<h3>User data isn't as evenly distributed as you want\u200b</h3><p>The final user takeaway is that you have to work with the users you're given, not the users you want. In an ideal world, users would have steady, predictable traffic that spread data access evenly across a table's keyspace. The reality is much different.</p><p>The original Dynamo paper used the concept of consistent hashing to distribute your data across independent partitions of roughly 10GB in size. (Partitions are discussed in more depth below). It uses the partition key of your items to place data across the partitions, which allows for predictable performance and linear horizontal scaling.</p><p>Further, unlike the original Dynamo system, DynamoDB is a multi-tenant system. Your partitions are co-located with partitions from tables of other DynamoDB users.</p>",
  "neutralised_chunk": "- User data often isn't distributed as evenly as desired.\n- Ideally, users have stable and predictable traffic that evenly distributes data access across keyspaces, but actual scenarios differ.\n- The original Dynamo paper outlined consistent hashing to distribute data across around 10GB-sized independent partitions.\n- This system uses an item's partition key to organize data across partitions, enabling predictable performance and seamless horizontal scaling.\n- Unlike the original Dynamo system, DynamoDB is a multi-tenant system.\n- Partitions of different DynamoDB users can be co-located."
}
{
  "chunk": "<p>Originally, the DynamoDB team built a system to avoid noisy neighbor issues where high traffic to one partition results in a reduced experience for unrelated partitions on the same storage node. However, as the system developed, they realized that the initial system to manage this led to a subpar experience for those with spiky, unbalanced workloads.</p><p>As the AWS team built out DynamoDB, they realized they needed to evolve the access control system that managed whether a partition was allowed to service a request. We look more into the technical aspects of this evolution below.</p>",
  "neutralised_chunk": "- Initially, DynamoDB had a system to prevent noisy neighbor issues, where high traffic to one partition could impact other unrelated partitions on the same storage node.\n- However, this initial system led to a subpar experience for workloads with spiky or unbalanced traffic patterns.\n- As DynamoDB evolved, AWS realized the need to enhance the access control system that managed whether a partition could service a request.\n- The technical aspects of this evolution are explored further below."
}
{
  "chunk": "<h2>Technical takeaways from the DynamoDB Paper\u200b</h2><p>The product-level learnings are fascinating, but this is ultimately a technical paper. The work the DynamoDB team is doing at massive scale is impressive, and many of the technical learnings apply even to those without DynamoDB's scale.</p><p>I had three technical takeaways that were most interesting from the paper:</p>",
  "neutralised_chunk": "- The DynamoDB paper provides interesting technical learnings.\n- The work done by the DynamoDB team, even at a massive scale, can be applied to smaller scales.\n- There are three key technical takeaways from the paper."
}
{
  "chunk": "<h3>Using log replicas to improve durability and availability\u200b</h3><p>One of the more interesting points was how DynamoDB uses something called log replicas to assist during periods of instance failure. To understand log replicas, we first need some background on the underlying architecture of DynamoDB's storage.</p><p>Start of DynamoDB storage background section</p><p>Under the hood, DynamoDB is splitting your data into partitions, which are independent storage segments of roughly 10GB in size. DynamoDB uses the partition key to assign your items to a given partition, which allows DynamoDB to scale horizontally as your database grows while still keeping related items together. DynamoDB is running a massive fleet of storage nodes which are handling partitions from many different user tables.</p><p></p>",
  "neutralised_chunk": "- DynamoDB uses log replicas to improve durability and availability during instance failures\n- To understand log replicas, we need background on DynamoDB's storage architecture\n- DynamoDB splits data into partitions, which are independent 10GB storage segments\n- The partition key is used to assign items to a given partition\n- This allows DynamoDB to scale horizontally as the database grows while keeping related items together\n- DynamoDB runs a large fleet of storage nodes handling partitions from many different user tables"
}
{
  "chunk": "<p>An individual partition is actually a set of three partition instances in different availability zones which form a replication group. One of the instances is the leader for a given partition and is responsible for handling all writes. When a write comes in, the leader writes it locally and ensures it is commited to at least one additional replica before returning to the client. This increases durability in the event of failure, as the loss of one node will not result in loss of data.</p><p></p>",
  "neutralised_chunk": "- An individual partition in DynamoDB is a set of three partition instances across different availability zones, forming a replication group.\n- One instance acts as the leader for a given partition, responsible for handling all writes.\n- When a write operation occurs, the leader writes the data locally and ensures it is committed to at least one additional replica before responding to the client.\n- This replication approach increases durability, as the loss of one node will not result in data loss."
}
{
  "chunk": "<p>On each storage partition are two data structures -- the B-tree that contains the indexed data on the partition along with a write-ahead log (WAL) that contains an ordered list of updates applied to that partition. A write-ahead log is a commonly used tactic in databases to enhance the durability and latency of write operations. Updating the B-tree is slower as it involves random I/O and may include re-writing multiple pages on disk, whereas updating the write-ahead log is an append-only operation that is much faster (P.S. the write-ahead log is basically the concept behind Kafka and related systems!).</p>",
  "neutralised_chunk": "- Every storage partition contains two data structures: the B-tree of indexed data and a write-ahead log (WAL) listing updates applied to the partition.\n- Write-ahead logs, commonly used in databases, enhance the durability and latency of write operations.\n- Updating the B-tree is slower due to random I/O and potential re-writing of multiple disk pages.\n- Write-ahead log updates, being append-only, are much faster.\n- The concept of write-ahead logs is fundamental to Kafka and similar systems."
}
{
  "chunk": "<p>Note that in addition to the performance difference of an individual operation against these structures, there's also a vast difference in the size of these two structures. The B-tree can be 10+ GB in size (accounting for the 10GB size of a partition along with the index overhead), whereas the write-ahead log is only a few hundred megabytes (the full history of the write-ahead log is periodically synced to S3, which is used to power point-in-time restore and other features).</p><p>End of DynamoDB storage background section</p>",
  "neutralised_chunk": "- Individual operations against B-trees and write-ahead logs present a significant performance difference.\n- There's a considerable size difference between the two structures. The B-tree can be over 10GB in size (included the partition's 10GB size and the index overhead), while the write-ahead log is merely a few hundred megabytes.\n- The write-ahead log's full history is periodically synced to S3, enabling point-in-time restore and other features.\n- This concludes the section on DynamoDB storage background."
}
{
  "chunk": "<p>When you're running a system as large as DynamoDB, instances are failing all the time. When a storage node fails, you now have potentially thousands of partition replicas that you need to relocate to a non-failing node. During this failure period, every replication group that had a replica on that node is now down to two replicas. Because two replicas are required to acknowledge a given write, you're now increasing the latency distribution of your writes as well as the probability of an availability event if another replica fails.</p><p>To reduce the period where only two replicas are live, DynamoDB uses log replicas. A log replica is a member of a replication group that contains only the write-ahead log. By skipping the B-tree for the partition, the DynamoDB subsystem can quickly spin up a log replica by copying over the last few hundred MB of the log. This log replica can be used to acknowledge writes but not to serve reads.</p>",
  "neutralised_chunk": "- In large systems like DynamoDB, instances frequently fail.\n- When a storage node fails, thousands of partition replicas need to be relocated to a non-failing node.\n- During the failure period, every replication group with a replica on that node is down to two replicas.\n- Two replicas are required to acknowledge a write, so write latency increases and the probability of an availability event rises if another replica fails.\n- To reduce the time with only two live replicas, DynamoDB uses log replicas.\n- A log replica is a replication group member containing only the write-ahead log.\n- By skipping the B-tree for the partition, DynamoDB can quickly spin up a log replica by copying the last few hundred MB of the log.\n- The log replica can acknowledge writes but not serve reads."
}
{
  "chunk": "<p>While this log replica is helping the replication group, the DynamoDB subsystem can work in the background to bring up a full replica member to replace the failed one.</p><p>It's so interesting to see the incremental tweaks the team has made to continually push the envelope on durability, availability, and latency. Most of these aren't altering the core promises that the system makes, such as making a different choice on the CAP theorem. Rather, they're steady improvements to the reliability and performance of a database.</p><p>First, DynamoDB uses the standard combination of a write-ahead log with the indexed storage to improve durability and reduce latency on write requests.</p>",
  "neutralised_chunk": "- DynamoDB uses a log replica to temporarily assist the replication group while a full replica member is brought up to replace a failed one.\n- The DynamoDB team has continuously made incremental improvements to enhance durability, availability, and latency.\n- These improvements do not alter the core promises of the system, such as the choice made in the CAP theorem.\n- They represent steady enhancements to the reliability and performance of the database.\n- DynamoDB employs a standard combination of a write-ahead log with indexed storage to improve durability and reduce write request latency."
}
{
  "chunk": "<p>Second, DynamoDB discards the typical single-node setup of a RDBMS and moves to a partitioned system. This reduces recovery time (and hence availability) as recovering a 10GB partition is much faster than recovering a 200GB table. Further, this replication is across three different availability zones (AZs) so that an entire AZ can go down without affecting availability of the system.</p><p>Then, DynamoDB relaxes the consistency requirements to require that only two of the three nodes in a replication group acknowledge the write. At the cost of some occasionally stale data, DynamoDB is able to enhance the availability and reduce the latency of writes.</p><p>Finally, DynamoDB uses log replicas to improve availability during periods of node failure.</p>",
  "neutralised_chunk": "- DynamoDB uses a partitioned system instead of a single-node setup like RDBMSs.\n- This partitioning reduces recovery time and improves availability, as recovering a smaller partition is faster than a large table.\n- The data is replicated across three availability zones, ensuring availability even if an entire zone fails.\n- DynamoDB relaxes consistency requirements, only requiring two out of three replicas to acknowledge writes.\n- This trade-off of occasional stale data allows for improved write availability and reduced latency.\n- DynamoDB utilizes log replicas to further enhance availability during node failures."
}
{
  "chunk": "<h3>Decoupling partitions from throughput\u200b</h3><p>In the previous section, we discussed how partitions are used to segment data within a table and allow for horizontal scaling. Additionally, we saw how DynamoDB co-locates partitions from different customers on the same storage nodes to allow for greater efficiency of the DynamoDB service.</p><p>A second interesting technical takeaway is the slow, steady improvements to the \"admission control\" system for these partitions. Admission control refers to the process in which DynamoDB determines whether a request can succeed based on the amount of capacity available. In determining this, DynamoDB is checking capacity across two axes:</p><p>The first one is a cost decision, as DynamoDB wants to make sure you're paying for the service they're delivering. The second one is a performance decision, as they want to avoid noisy neighbor issues from co-located partitions.</p>",
  "neutralised_chunk": "- Partitions in a table are used for data segmentation and horizontal scaling.\n- DynamoDB places partitions from different customers on the same storage nodes to increase service efficiency.\n- DynamoDB has gradually improved its \"admission control\" system, the process to check if a request can proceed based on available capacity.\n- The admission control checks capacity across two different factors:\n  - Making sure the provided service is properly paid for.\n  - Avoiding performance issues due to the co-location of partitions."
}
{
  "chunk": "<p>The first iteration of admission control was purely at a partition level. DynamoDB would divide the total provisioned throughput by the number of partitions and allocate that amount to each partition evenly. This was the easiest system, as you didn't have to coordinate across lots of partitions on a second-by-second basis. However, it led to issues with unbalanced workloads and the \"throughput dilution\" problem. This could lead to situations where requests to hot partitions were being throttled even though the table wasn't using anywhere near its provisioned capacity.</p><p>To fix this problem, DynamoDB wanted to decouple admission control from partitions but realized this would be a big lift. To handle this, they moved in stages.</p>",
  "neutralised_chunk": "- Initially, DynamoDB's admission control was at the partition level.\n- Total provisioned throughput was divided equally among partitions.\n- This was a simple system without coordination across partitions.\n- However, it led to issues with unbalanced workloads and throughput dilution.\n- Hot partitions could be throttled even when the table was underutilized.\n- To address this, DynamoDB aimed to decouple admission control from partitions.\n- But this was a significant change, so they moved in stages."
}
{
  "chunk": "<p>First, they improved the partition-level admission control system. While each partition was limited to prevent over-consumption of resources on an individual node, they also realized that storage nodes were often running under full capacity. To help with temporary spikes in traffic to individual partitions, DynamoDB added short-term bursting that would let a partition use additional throughput if it was available for the given storage node. This improvement was mostly focused on the second axis of access control -- protecting against noisy neighbors.</p>",
  "neutralised_chunk": "- There were improvements made to the partition-level admission control system.\n- Each partition was restricted to prevent overuse of resources on a single node.\n- Storage nodes were often found to run under full capacity.\n- To handle temporary traffic increases to individual partitions, short-term bursting was added to DynamoDB, letting a partition use extra throughput if available on the storage node.\n- This improvement mainly addressed the second axis of access control \u2013 protection from noisy neighbors."
}
{
  "chunk": "<p>A second initial improvement helped with the other axis of access control -- the provisioned throughput for an individual table. As mentioned, a table with skewed access patterns might consume all the throughput for one partition while still being below the total provisioned throughput for the table. To help with this, DynamoDB added adaptive capacity, where throughput from sparsely used partitions could be shifted to highly used partitions.</p><p>These two changes, while still maintaining the general partition-based access control scheme, alleviated a significant amount of pain based on uneven access patterns of data.</p>",
  "neutralised_chunk": "- A second improvement addressed the provisioned throughput for an individual table, another aspect of access control.\n- Skewed access patterns could lead to a table consuming all the throughput for one partition while remaining below the total provisioned throughput for the table.\n- DynamoDB introduced adaptive capacity to allow throughput from sparsely used partitions to be redirected to highly used partitions.\n- These modifications, while preserving the general partition-based access control scheme, significantly reduced issues caused by uneven data access patterns."
}
{
  "chunk": "<p>Later, DynamoDB moved to a global access control system which decoupled throughput from partitions entirely. This changed adaptive capacity from a slower, 'best efforts' system to a nearly instant system to spread your throughput across your partitions. This flexibility led to amazing other improvements, including the ability to separate particularly hot items onto their own partitions, to provide DynamoDB On-Demand billing, and to 'overload' storage nodes based on predicted workloads of the underlying partitions.</p><p>All of this is recounted in more detail in Section 4 of the paper, and it is well worth your own read to understand the details.</p>",
  "neutralised_chunk": "- DynamoDB transitioned to a global access control system, decoupling throughput from partitions.\n- This changed adaptive capacity from a slower, 'best efforts' approach to a near-instant system for distributing throughput across partitions.\n- This flexibility enabled several improvements:\n  - Separating hot items onto their own partitions\n  - Introducing DynamoDB On-Demand billing\n  - Overloading storage nodes based on predicted partition workloads\n- Section 4 of the referenced paper provides more detailed information on these changes and is recommended for further understanding."
}
{
  "chunk": "<h3>The use of asynchronous caches\u200b</h3><p>The last technical takeaway was in DynamoDB's use of asynchronous caches. By \"asynchronous cache\", I'm meaning a system that caches data locally but then rehydrates the cache behind the scenes, asynchronously, to ensure it stays up to date.</p><p>We all know caches as a way to reduce latency by storing the results of an expensive call. In both cases mentioned in the paper, individual request router instances are storing the results of external calls locally to avoid a slow network request. But there are two more subtle points that are pretty interesting. In reviewing these, we should note how DynamoDB treats \"external\" systems (also called \"dependencies\") from \"internal\" systems.</p>",
  "neutralised_chunk": "- DynamoDB utilizes asynchronous caches.\n- Asynchronous caches mean a system that stores data locally and refreshes it in the background to keep it updated.\n- Caching is used to decrease latency by saving the results of costly calls.\n- In scenarios given in the study, individual request router objects store the outcomes of external calls locally to evade a slow network query.\n- Two interesting points can be observed when reviewing DynamoDB's treatment of \"external\" (or \"dependencies\") versus \"internal\" systems."
}
{
  "chunk": "<p>DynamoDB uses other AWS services, such as IAM to authenticate requests or KMS to encrypt and decrypt data. Both of these services are external dependencies as they're not under the control of the DynamoDB team. Here, DynamoDB will cache the results of calls to these services as a way to increase availability. These results are periodically refreshed asynchronously to ensure freshness. This allows DynamoDB to keep working (somewhat) even if these external services are having issues themselves. Without this, DynamoDB's availability would necessarily be lower than those of IAM and KMS.</p><p>DynamoDB also uses asynchronous caches for 'internal' systems. DynamoDB has a metadata system that tracks table information and locations for each DynamoDB partition. When a request comes to a DynamoDB request router, it needs to find the relevant partition for the given item to forward the request to the storage node.</p>",
  "neutralised_chunk": "- DynamoDB utilizes other AWS services like IAM for authentication and KMS for encryption/decryption, which are external dependencies not controlled by the DynamoDB team.\n- DynamoDB caches the results of calls to these external services and periodically refreshes them asynchronously to maintain availability, even if the external services experience issues.\n- Without caching, DynamoDB's availability would be limited by the availability of services like IAM and KMS.\n- DynamoDB also employs asynchronous caches for internal systems.\n- DynamoDB has a metadata system that tracks table information and partition locations.\n- When a request arrives at a DynamoDB request router, it needs to find the relevant partition for the item to forward the request to the appropriate storage node."
}
{
  "chunk": "<p>This metadata information doesn't change frequently, so the request routers heavily cache this data. The paper notes that the cache hit rate is 99.75% (!!), which is quite good. However, a high cache hit rate can also lead to problems where slight decreases in traffic can result in significantly more load to the underlying service. Decreasing the metadata cache hit rate from 99.75% to a still-amazing 99.5% results in twice as many requests to the underlying metadata service.</p><p>The DynamoDB team found that the metadata service had to scale in line with the request router service, as new request routers had empty caches that resulted in a lot of calls to the metadata service. This led to instability in the overall system.</p>",
  "neutralised_chunk": "- Metadata information doesn't frequently change, leading to heavy caching by request routers.\n- The paper states a 99.75% cache hit rate, which is exceptionally high.\n- A high cache hit rate can lead to issues, such as increased service load due to minor traffic reductions.\n- Lowering the metadata cache hit rate from 99.75% to 99.5% doubles requests to the underlying metadata service.\n- The DynamoDB team observed that the metadata service had to scale with the request router service.\n- New request routers with empty caches caused many calls to the metadata service, destabilizing the overall system."
}
{
  "chunk": "<p>To increase resiliency of its internal systems, DynamoDB uses asynchronous cache refreshing to provide constant load to the underlying metadata system. While the request routers would cache locally with a high hit rate, each hit results in an associated request to the metadata service to refresh the cached data.</p><p>By pairing a local cache hit with an asynchronous request to the metadata service, it ensures a more consistent rate of traffic to the metadata service. Both a cache hit and a cache miss result in a request to the metadata service, so increasing the number of request routers with cold caches doesn't result in a burst of new traffic to the metadata service.</p>",
  "neutralised_chunk": "- DynamoDB employs asynchronous cache refreshing to enhance the resiliency of its internal systems and maintain a constant load on the underlying metadata system.\n- While request routers cache data locally with a high hit rate, each cache hit triggers an associated asynchronous request to the metadata service to refresh the cached data.\n- By coupling a local cache hit with an asynchronous request to the metadata service, DynamoDB ensures a consistent rate of traffic to the metadata service.\n- Both cache hits and cache misses result in a request to the metadata service, preventing bursts of new traffic to the metadata service when adding new request routers with cold caches."
}
{
  "chunk": "<p>There's a lot of other really interesting information about the metadata caching system that I won't cover here, but I thought these two uses of asynchronous caches were interesting. Both used local, instance-based caching to reduce latency but also coupled with asynchronous refreshing to decouple availability from external dependencies and to increase the resiliency of internal services.</p>",
  "neutralised_chunk": "- Metadata caching system has interesting details not covered here.\n- Two uses of asynchronous caches are highlighted.\n- Local, instance-based caching was used to reduce latency.\n- Asynchronous refreshing was coupled with local caching.\n- Asynchronous refreshing helped decouple availability from external dependencies.\n- Asynchronous refreshing increased resiliency of internal services."
}
{
  "chunk": "<h2>Conclusion\u200b</h2><p>Once again, Amazon has helped to push forward our understanding of deep technical topics. Just as the Dynamo paper was revoluationary in designing new database architectures, the DynamoDB paper is a masterful lesson in running and evolving large-scale managed systems.</p><p>In this post, we looked at the core learnings from a user-needs perspective in the DynamoDB paper. Then, we looked at three technical learnings from the paper.</p><p>The paper has a number of other interesting points that we didn't cover, such as how DynamoDB monitors client-side availability by instrumenting internal Amazon services, the strategies used to deploy new versions of DynamoDB against an enormous fleet of instances, and the mechanisms used to protect against data errors, both in flight and at rest. If you write up an examination of those points, let me know and I'll link to them here!</p><p>If you have any questions or corrections for this post, please leave a note below or email me directly!</p>",
  "neutralised_chunk": "- Conclusion: Amazon continues to advance understanding of deep technical topics, as seen in the influential Dynamo and DynamoDB papers.\n- The post covered core user-needs learnings from the DynamoDB paper.\n- It also examined three key technical learnings from the paper.\n- Other interesting points not covered include:\n  - Monitoring client-side availability through instrumentation of internal Amazon services\n  - Strategies for deploying new versions against a massive instance fleet\n  - Mechanisms to protect against data errors in transit and at rest\n- Invitation for further examination and discussion of these points.\n- Request for questions, corrections, or feedback on the post."
}
