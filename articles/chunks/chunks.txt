<h1>Everything you need to know about DynamoDB Partitions</h1><p>Prefer video? View this post on YouTube!</p><p>DynamoDB powers some of the highest-traffic systems in the world, including Amazon.com's shopping cart, real-time bidding for ad platforms, and low-latency gaming applications. They use DynamoDB because of its fast, consistent performance at any scale.</p><p>Before I understood DynamoDB, I thought AWS had a giant supercomputer that was faster than everything else out there. Turns out that's not true. They're not defying the laws of physics -- they're using basic computer science principles to provide the consistent, predictable scaling properties of DynamoDB.</p>
<p>In this post, we'll take a deep look at DynamoDB partitions -- what they are, why they matter, and how they should affect your data modeling. The most important reason to learn about DynamoDB partitions is because it will shape your understanding of why DynamoDB acts as it does. At first glance, the DynamoDB API feels unnecessarily restrictive and the principles of single-table design seem bizarre. Once you understand DynamoDB partitions, you'll see why these things are necessary.</p><p>If you want to skip to specific sections, this post covers:</p><p>Let's get started!</p>
<h2>Background on DynamoDB partitions​</h2><p>To begin, let's understand the basics of partitions in DynamoDB. And before we do that, we need to review some basics about DynamoDB tables.</p><p>DynamoDB is a schemaless database, which means DynamoDB itself won't validate the shape of the items you write into a table. Yet a DynamoDB table is not totally without form. When creating a DynamoDB table, you must specify a primary key. Each item that you write into your table must include the primary key, and the primary key must uniquely identify each item.</p><p>There are two types of primary keys in DynamoDB. First is the simple primary key, which consists of a single element called the partition key. The image below shows an example of a table with a simple primary key:</p><p></p>
<p>This table is storing Orders in an e-commerce application. Notice the primary key (outlined in blue), on the far left side of the table. It consists of a partition key called "OrderId" (outlined in red). Each Order item in the table contains the "OrderId" property, and it will be unique for each item in the table.</p><p>The second type of primary key is a composite primary key. A composite primary key consists of two elements: a partition key and a sort key. The image below shows an example of a table with a composite primary key.</p><p></p>
<p>This table stores the same Orders as our previous table, but it also includes information about each Order's Customer. The primary key (outlined in blue) on the left side of the table now has two parts -- a partition key (outlined in red) of CustomerId and a sort key of OrderId. Like with the simple primary key, each item in this table will need to include the primary key (both elements), and it is the combination of these two elements that will uniquely identify each item in the table.</p><p>Notice how both primary key structures include a partition key element. This partition key is crucial to DynamoDB's infinite scaling capabilities.</p>
<h3>Horizontal scaling and the partition key​</h3><p>As noted in the introduction, your DynamoDB table isn't running on a single supercomputer powered by qubits. Rather, it's distributed across an entire fleet of computers, where a single computer holds a subset of your table's data. These subsets are called 'partitions', and they enable essentially infinite scaling of your DynamoDB table.</p><p>Let's walk through this with an example.</p><p>When you write an item to your DynamoDB table, the request will first be handled by a frontend service called the request router.</p><p></p><p>The request router doesn't store your actual data. Its job is to understand metadata about your table and its partitions, then route the request to the right partition.</p><p>This request router will parse the partition key from your item and hash it.* Then, based on the hashed value of the partition key, it will determine the partition to which this item should be written.</p>
<p>* Why does DynamoDB hash the partition key before placing it on a partition? This is to better spread data across your partitions. Imagine you had a sequential identifer as your partition key, and you wanted to iterate over sequential identifiers for one of your access patterns. If these values weren't hashed before locating on a partition, you could end up with hot partitions for these access patterns. A similar principle exists for MongoDB and is explained here.</p><p></p><p>In the image above, the request router determined that the incoming item belonged to Partition 1 and send the request there to be handled.</p><p>Reading items from DynamoDB follows the same flow. The request router will parse the partition key, then route the read request to the proper partition to read and return the data.</p><p></p>
<p>Each partition is roughly 10GB in size, so DynamoDB will add additional partitions to your table as it grows. A small table may only have 2-3 partitions, while a large table could have thousands of partitions.</p><p></p><p>The great part about this setup is how well it scales. The request router's work of finding the proper node has time complexity of O(1).</p><p></p><p>This is the crucial fact about DynamoDB's horizontal scaling -- even with thousands of partitions, the request router's job is a fast, consistent operation that narrows your table from multiple terabytes down to a more manageable 10GB.</p>
<h2>How partitions do (and don't) matter in DynamoDB​</h2><p>Now that we know how partitions work in DynamoDB, let's see how they do and don't matter for you as a DynamoDB user.</p><p>We'll start with how they don't matter. On a practical basis, you don't need to know about individual partitions in your DynamoDB table. You won't need to specify the partition to use when reading or writing an item into DynamoDB. The request router handles all of this for you.</p><p>Not only that, but you can't know about individual partitions in DynamoDB. All requests go through the request router and use an (undisclosed) hash function before placing items, so you couldn't understand the various partitions even if you wanted to. Note that this is different than other NoSQL databases, such as Cassandra, where clients can be topology-aware and thus go directly to a specific partition rather that hitting a shared router first.</p>
<p>There is a slight performance hit of using a shared request router, as client requests will now take an additional network hop to fulfill the request. However, your client's ignorance about partitions makes it easier for you to reason about as a DynamoDB user.</p><p>First, your client doesn't need to request and maintain cluster metadata when interacting with the DynamoDB backend. This reduces the initial information your client needs to pull down when connecting as well as the continued chatter to keep aware of cluster updates.</p><p>Additionally, it makes it easier for DynamoDB to provide strong performance guarantees around your data. A few years ago, the partition performance story was more nuanced. For high-scale tables, you needed to be aware of your partitions and the request distribution across partitions. Your provisioned throughput was spread equally across all partitions in your table, so you would need to provision capacity for your highest-traffic partition.</p>
<p>In 2018, DynamoDB announced adaptive capacity for your partitions. With adaptive capacity, your provisioned throughput is shifted around to the partitions that need it rather than equally across all partitions. DynamoDB will take the requisite actions to ensure your capacity is used efficiently. It can transparently add new partitions as your data size grows, or it can split highly-used partitions into sub-partitions to ensure it can provide consistent performance.</p><p>With these improvements, you can think less about DynamoDB partitions. As long as you are fitting within the partition throughput limit, you don't need to worry about balancing your partitions.</p>
<p>While you don't need to know the details about partitions on a practical matter, understanding DynamoDB partitions helps on a theoretical matter. By understanding how DynamoDB uses partitions and why it matters, it helps to build your mental model of how DynamoDB works. After all, the principles of single-table design seem pretty wild if you're coming from a normalized, relational database model.</p><p>Once you understand the centrality of partitions in DynamoDB, the rest of DynamoDB's data modeling makes sense.</p>
<h3>Primacy of primary keys​</h3><p>The most important implication of DynamoDB's partition is the primacy of primary keys. We've seen how the request router immediately uses the partition key to route a request to the proper partition. Because of this, DynamoDB pushes hard for you to include the partition key in your requests.</p><p>The DynamoDB API has three main types of actions:</p><p>Other than the wildly-inefficient (for most use cases!) Scan operation, you must include the partition key in any request to DynamoDB.</p><p>In the background section, we noted that DynamoDB is schemaless outside the primary key. Both of these characterisitcs -- general schemalessness, with an exception for the primary key -- are due to DynamoDB's partitioning scheme.</p><p>DynamoDB forces your access of items to be centered on the primary key. Thus, it needs to enforce that the primary key exists for each item so that it can be indexed appropriately.</p>
<p>For the rest of the item, however, the shape is irrelevant. You won't have foreign key references to other tables, like you would in a relational database. Nor will you have neatly structured result sets with typed columns. Because the primary key is the only thing that matters, the rest of the data is just coming along for the ride.</p><p>DynamoDB is more like a key-value store than a spreadsheet. The key might be simple, with just a partition key, or complex, with a partition key plus a sort key, but the value is still just a blob.</p><p>One word of caution -- DynamoDB's schemaless nature does not mean your data shouldn't have a schema somewhere. You need to maintain and validate your data schemas within your application code rather than relying on your database to handle it.</p>
<p>Note that while other NoSQL databases use the same partitioning concept to enable horizontal scaling, not all of them restrict the API accordingly. If you're not careful, you can lose the benefits of horizontal scaling by using scatter-gather queries to each of your partitions!</p><p>When thinking about data access in DynamoDB, remember this image:</p><p></p><p>You can query efficiently by the attributes in your primary key, but you cannot query efficiently by other attributes in your table. Design your table accordingly!</p>
<h2>Consider how to split up your data​</h2><p>The second implication of DynamoDB's partition is that it forces you to consider how to split up your data.</p><p>One of the biggest mistakes I see new DynamoDB users make is to put a lot of unrelated items into a single item collection. Don't do that! Rather, use a high-cardinality value for your partition key.</p><p>If you have users in your application, the userId is often a good partition key. This will ensure different users are placed in different item collections, and you can have lots of users using your application without affecting each other.</p><p>On the other hand, using something like a boolean value (true / false) or an enum (red / green / blue) as your partition key is not a good idea. You don't have high enough cardinality to really distribute your data across your data.</p><p>Careful consideration of how you split up your data can also assist with complex access patterns.</p>
<p>You'll often hear that DynamoDB can't handle full-text search or complex filtering patterns. And it's true -- if you want to perform full-text search across your entire application, you'll probably want to reach for dedicated search infrastructure.</p><p>However, your search needs often aren't that extensive. You can usually narrow your search requirements to a smaller section of your dataset. For example, you may want to allow full-text search on users within an organization. If that's the case, DynamoDB can work well. You can partition your users by organization. When a client makes a search request, you can pull in all the users for that organization then perform an in-memory search on the results before sending back to the client. If the corpus you're searching against is narrow (here, the set of users in an organization), you may not need dedicated search infrastructure.</p>
<p>In a recent chat with Rick Houlihan, Rick told me that this is how Amazon.com retail handles most of their e-commerce search and filtering. If you search for "sony 42 inch tv", the backend doesn't search the entire inventory of Amazon.com. Rather, it narrows down to the TV category, then performs a more targeted search there.</p><p>You can use this same strategy with geo-lookups. If you want to provide geo-search on your dataset, figure out how you can break it down -- into countries, states, cities, geohashes, etc. Narrow your initial search space when reading from DynamoDB, then provide an in-memory filter to further reduce your options.</p><p>By understanding the role that partitions play, you can lean into the efficiency of DynamoDB by segmenting your data accordingly.</p>
<h2>No joins + denormalization​</h2><p>Last but not least, DynamoDB's lack of support for joins is mostly due to the partitioning scheme.</p><p>When a relational database is performing a join operation, it's evaluating portions of two separate tables for comparison and combination. This poses two problems for DynamoDB.</p><p>First, the relevant portions of the two tables may be on two separate partitions. This adds additional network hops to perform the separate reads, as well as a combination step on a centralized node before returning to the client.</p>
<p>Second, it's possible that a client will want to join an attributes that aren't part of the primary key (and thus aren't indexed properly). This goes against DynamoDB's core philosophy around primary keys discussed above. Additionally, relational databases often include sophisticated query planners that determine the most efficient way to execute a given query. They include information about table structures and indices, as well as statistics on the contents of a particular table.</p><p>None of this is completely insurmountable in a partitioned database like DynamoDB. MongoDB offers the $lookup aggregation, which performs an outer join. Additionally, partitioned relational database options like Vitess offer support for joins. However, it adds complexity -- complexity for the DynamoDB backend and complexity for the DynamoDB user. Further, it adds performance variability, which is the bane of DynamoDB.</p>
<p>Instead, DynamoDB forces you to model around the lack of joins. This might mean denormalization or it could mean pre-joining heterogenous items into the same partition. You can choose the proper approach based on your needs. Whichever route you choose, you won't be guessing about the performance implications as your application grows.</p>
<h2>Conclusion​</h2><p>In this post, we learned about DynamoDB partitions -- what they are, how they do (and don't) matter, and how they should affect your data modeling.</p><p>The main takeaway from this post is that you can and should learn the philosophy of DynamoDB. It's not that big of a surface area, particularly when compared to all the knobs and buttons with a relational database. By learning a few key principles, you'll build a proper mental model of DynamoDB, and it will be easier to use it correctly.</p><p>If you have questions or comments on this piece, feel free to leave a note below or email me directly.</p>
<h1>Understanding Eventual Consistency in DynamoDB</h1><p>One of the core complaints I hear about DynamoDB is that it can't be used for critical applications because it only provides eventual consistency.</p><p>It's true that eventual consistency can add complications to your application, but I've found these problems can be handled in most situations. Further, even your "strongly consistent" relational databases can result in issues if you're not careful about isolation or your choice of ORM. Finally, the benefits from accepting a bit of eventual consistency can be pretty big.</p><p>In this post, I want to dispel some of the fear around eventual consistency in DynamoDB.</p><p>We'll walk through this in three different sections:</p><p>My hope is that you can use this to make a more informed decision about when and how you can handle eventual consistency in your application.</p>
<h2>Background​</h2><p>Before we get started, let's set the boundaries for what we're going to discuss today.</p><p>There's a lot of confusion around eventual consistency. Part of this is because the area of database and distributed system consistency is confusing -- so much so, that I wrote a huge post on the different concepts of consistency before I could write this one. Amazingly, that post covered three types of consistency, and the idea of 'eventual consistency' was not one of them. If the concepts in this background are confusing to you, you should read that post first.</p><p>At a very high level, the notion of "eventual consistency" refers to a distributed systems model where, to quote Wikipedia, "if no new updates are made to a given data item, eventually all accesses to that item will return the last updated value."</p>
<p>Note that this definition talks about what eventual consistency can promise, but the interesting part is what it doesn't promise. It doesn't promise that any read of a data item will show the most recent updates to that item. This is the core downside of eventual consistency -- you could update an item with one request, then fail to see those updates with a subsequent request.</p><p>With that in mind, let's note three key aspects about eventual consistency. These aspects are broadly applicable, but I will use a DynamoDB focus in discussing them.</p>
<h3>Eventual consistency is about replication​</h3><p>Fundamentally, eventual consistency is an issue of replication.</p><p>In a single-node database, you don't have to worry about eventual consistency. Writes and reads are coming to the same instance, so it should be able to provide a consistent view of a single data item.</p><p>Once you start to replicate your data to multiple nodes, the story changes. Now, when a write comes to a node, you need to figure out how to efficiently communicate that write to the other nodes. Further, if you allow reads from the other nodes, then you think about the PACELC tradeoff discussed in the next section.</p><p>Replication adds some complexity around eventual consistency, but replication can be a good thing as well! There are a number of reasons you may want to replicate your data to multiple places, such as:</p>
<p>Redundancy. Adding replicas allows for redundancy in case your primary instance fails. In this case, you're not necessarily reading from your replicas -- you're simply maintaining a hot standby in the event of failure. Thus, there are no eventual consistency implications (outside of failure scenarios) that are implicated solely due to redundancy. This is the default mode for MongoDB replicas, which direct all read requests to the primary node unless otherwise configured.</p><p>Increase throughput. A second reason to add replication is to increase your processing throughput. In this case, you may direct all write operations to your primary node but reads can go to your secondary nodes. This is the approach used when adding read replicas to your relational database. By increasing the nodes available for read traffic, you're able to handle more requests per second. However, you do add the eventual consistency issues discussed in this post.</p>
<p>Get data closer to users. A third reason for replication is to move data closer to your users. Your users may be global, but often your database is in a central location. If that's the case, the network latency to your database is likely the slowest part of a request. By globally distributing your data, you can vastly reduce client-side latency. This is the approach taken by Fly.io with global Postgres. This does implicate eventual consistency issues as well.</p><p>Note that these reasons aren't mutually exclusive! All three of these are at play with DynamoDB. Adding replicas in different availability zones enhances availability through redundancy. Because each replica can receive read requests, they also increase throughput. Finally, if you use DynamoDB Global Tables, you can put your data closer to your users.</p>
<h3>Eventual consistency is not (only) about CAP​</h3><p>When people think about eventual consistency, they often jump straight to the CAP theorem. And the CAP theorem is relevant when thinking about eventual consistency. If you choose an AP system, you probably have some process to reconcile the data on disparate nodes once the network partition is healed.</p><p>But the CAP theorem is limited. It only applies during times of failure and only during a specific type of failure -- network partitions between nodes. (See this post for more on when CAP applies).</p><p>Most of the time, your application is going to be running as intended. During that time, if you're allowing reads from your secondary nodes, you have to think about a different tradeoff -- one between latency and consistency. This is the second half of the PACELC theorem, which I discussed further here.</p>
<p>Let's think about this with a quick example. Imagine you have a three-node system. Node A is the primary and handles the writes, but reads can be handled by any of the nodes.</p><p></p><p>When a write comes to your primary node, you have a range of options on how to handle it.</p><p>You could require that all three nodes acknowledge durably committing the update before acknowledging the write to the client. This gives you the strongest consistency, as all writes must be agreed upon by all nodes before accepting. However, it also increases latency, as you need to communicate with these remote nodes and wait for the slowest one to respond. You can think of this as comparable to a RDBMS with synchronous replication to read replicas.</p>
<p>On the other end of the spectrum, you could only commit the write to the primary node before acknowledging to the client. The primary node could then send the updates to the replica nodes asynchronously. This has much lower write latency for clients at the cost of more data inconsistencies on reads. This is more comparable to asynchronous read replicas with a relational database or with MongoDB's writeConcern: 1 setting.</p><p>More in the middle is something like MongoDB's writeConcern: "majority" setting, where the write must be committed to a majority of nodes before returning.</p><p></p><p>In reviewing the different flavors of eventual consistency below, we'll talk about where each one fits on the spectrum.</p>
<h3>Eventual consistency is (usually) about reads​</h3><p>A final point to keep in mind is that eventual consistency is almost always a read problem but not a write problem.</p><p>Most replicated databases use a primary-replica setup where any piece of data has a primary node and one or more replica nodes. The primary node is responsible for all write operations and will send updates to the replica. This means you don't have an eventual consistency issue when writing data -- you'll always be operating against the latest version of the data when writing.</p><p>This is important later on when we discuss strategies for dealing with eventual consistency.</p><p>That said, some databases do allow for a multiple primary setup. Amazon Aurora has multi-master clusters where any of the instances can handle write operations. Similarly, DynamoDB Global Tables are essentially a multi-primary system where each enabled region has a primary node.</p><p>Using a multi-primary system increases complexity around eventual consistency and can lead to subtle bugs. We'll talk about this later.</p>
<h3>Eventual consistency on your main table​</h3><p>First, let's talk about eventual consistency on your main table.</p><p>Quick sidebar: DynamoDB tables have a primary key that uniquely identify each item. The DynamoDB API strongly pushes you toward using the primary key for your data access. But often you have items that need to be accessed in multiple distinct ways. With secondary indexes, you accomodate this. You can create secondary indexes with a different primary key, and DynamoDB will replicate your data into the secondary index with this new primary key structure. Finally, note that you can only make reads against your secondary index -- all writes must go through your main table.</p><p>Your main table is an interesting case in DynamoDB as you have the option between using strongly consistent reads or eventually consistent reads. To understand why, let's understand DynamoDB's underlying infrastructure and what happens when a write request comes in.</p>
<p>First, you should know that, under the hood, DynamoDB segments your data onto multiple partitions. DynamoDB will assign a record to a specific partition based on the partition key of your table, and each record belongs to one and only one partition.</p><p></p><p>But that's not the end of the story. Each partition includes three nodes: a primary and two replicas. The primary node is the only one that can handle writes, but any of the replicas can serve reads.</p><p></p><p>When a write comes to a DynamoDB table, the request router will use the partition key to direct it to the primary node for the given key. The primary node will tentatively apply the write if any write conditions are met. Then it will submit the write to both replica nodes. As soon as one of the replicas responds with a successful write, the primary node will return a successful response to the client.</p>
<p>By ensuring that a majority of the nodes have committed the write, DynamoDB is increasing the durability of the write. Now, even if the primary node fails immediately after the write request, it has been durably committed to another node to preserve the write.</p><p>You can also see where the potential for an inconsistent read comes in. Imagine our write request is accepted after the primary and Replica A have recorded it. When a read request comes in, it could go to our primary or either of the two replicas. If it chooses the lagging Replica B, there's a possibility we could receive a stale read that does not include the latest updates to the item.</p>
<p>In effect, DynamoDB chooses an intermediate position on the latency-consistency continuum. It doesn't go for strong consistency by durably writing to all nodes, but it also doesn't take latency to the lowest limit by writing to only one node. Part of this is due to the durability benefits of writing to multiple nodes, but it's relevant nonetheless.</p><p></p><p>Two final notes to wrap up this section.</p><p>First, while a stale read is possible on your main table, it's fairly unlikely. Remember that DynamoDB will randomly choose one of the three nodes to read from to handle a read request. Because a write will be committed to two of the three nodes before being acknowledged, this means there's a 66% chance you'll hit one of the "strongly consistent" nodes anyway.</p>
<p>Further, the lagging third node probably isn't that far behind in the normal case. The primary sent the write operation to both replicas at the same time. While it won't wait for the second replica to respond, it's probably milliseconds behind at most. This is exactly what I found in some basic testing as well.</p><p>Second, while I've been talking about your main table in this section, the same principles apply to local secondary indexes. Some of the nuances around local secondary indexes require another post, but items in a local secondary index are kept on the same partition as the items in the main table and thus have the same replication behavior.</p>
<h3>Eventual consistency on global secondary indexes​</h3><p>Now that we've covered the standard case with our main table, let's move on to the second flavor of eventual consistency -- that of global secondary indexes.</p><p>Unlike your main table or local secondary indexes, you cannot opt in to strongly consistent reads for your global secondary indexes. To understand why, we'll again look to the infrastructure underlying the implementation.</p><p>We talked in the previous section about how DynamoDB partitions your data using the partition key of your data. When you create a global secondary index on your table, DynamoDB creates completely separate partition infrastructure to handle your global secondary index:</p><p></p><p>Note that this separate infrastructure for secondary indexes is different than how other NoSQL databases handle it. With MongoDB, for example, additional indexes are implemented on each of the existing shards holding your data.</p>
<p>Why does DynamoDB place global secondary indexes on new partitions? It all flows from DynamoDB's laser focus on consistent, predictable queries.</p><p>DynamoDB wants your response times to be consistent no matter the size of your database or the number of concurrent queries. One key way it does this is via its partitioning scheme using the partition key so that it can quickly find the right ~10GB partition that holds your requested data.</p><p>But a global secondary index can have a different partition key than your main table. If DynamoDB didn't reindex your data, then a query to a global secondary index would require querying every single partition in your table to answer the query. This is exactly what MongoDB does -- if you have a query that does not use the shard key on your collection, it will broadcast to every shard in your cluster.</p>
<p>Ok, so we know the choice DynamoDB makes in placing your global secondary indexes on separate partitions. Why does this mean we can't get strong consistency from these separate partitions?</p><p>Recall our tradeoff between latency and consistency. To get strong consistency from a given node, we must ensure the write is committed to the node before the write is acknowledged. Thus, our table would need to not only write to the primary and one replica of our main table, but also two nodes for each global secondary index as well!</p><p>With one global secondary index, that might not sound so bad -- what's a few milliseconds between friends? But you can have up to 20 global secondary indexes by default, and you can even request a limit increase to have more!</p><p>You don't want your writes hung up on acknowledgements from 10+ nodes, each of which has their own provisioned capacity and instance failures. This would vastly reduce the benefit of global secondary indexes.</p>
<p>Instead, DynamoDB uses asynchronous replication to global secondary indexes. When a write comes in, it is not only committed to two of the three main table nodes, but it also adds a record of the operation to an internal queue. At this point, the write can be acknowledged to the client. In the background, a service is processing that queue to update the global secondary indexes.</p><p></p><p>Notice where global secondary indexes fall on the latency-consistency spectrum. They optimize heavily for write latency at the expense of consistency.</p><p></p><p>Further, and more importantly, notice the difference in flavor of eventual consistent reads on your main table as compared to reads from your global secondary index. The replication lag on your main table is likely to be barely perceptible in the normal course of business, whereas the replication lag for global secondary indexes will be more noticeable.</p>
<p>Finally, the replication lag on global secondary indexes can be influenced by your own actions. Global secondary indexes have capacity that is separate from your main table. If you don't provision enough capacity, or if you exceed partition throughput limits for your index, the lag can be even longer. This is different from your main table where exceeding provisioned capacity or partition throughput limits will result in an error on the write request itself.</p>
<h3>Eventual consistency on DynamoDB Global Tables​</h3><p>When I first conceived of this post, I was going to discuss the Two Flavors of Eventual Consistency in DynamoDB, as I wanted to make the key point about the difference in replication lag between your main table and global secondary indexes. But as I started to write, I realized there's a third flavor of eventual consistency in DynamoDB -- that of cross-region replication using DynamoDB Global Tables.</p><p>As a quick summary: DynamoDB Global Tables are a fully managed way to replicate your tables across multiple regions. This can be used for both the redundancy reason for replication, as you have some resiliency in the face of region failure, as well as the "get data closer to users" reason, as users can be routed to the region nearest them.</p>
<p>In a way, the infrastructure for DynamoDB Global Tables is similar to the infrastructure for global secondary indexes. When a write is received in one region, it is durably committed to two of the three nodes on the main table for the region and the write is acknowledged to the client. Then, it is asynchronously replicated to separate infrastructure. Rather than being a different set of partitions in the same region, the target infrastructure is a separate table in a different region.</p><p></p><p>Similar to global secondary indexes, Global Tables optimize for latency on the latency-consistency spectrum. However, there are two additional notes to consider as compared to global secondary indexes.</p>
<p>First, the replication latency to Global Tables is likely to be longer than that to global secondary indexes. Regions are significantly further apart than instances in the same datacenter, and network latency starts to dominate. P99 latencies across regions can easily be 100 - 200 milliseconds, so you should anticipate your replication being affected accordingly.</p><p>Second, using Global Tables introduces write-based consistency issues into your application. I mentioned earlier that eventual consistency is mostly a read-based problem, but that's not the case with Global Tables. You can write to both regions, and writes will be replicated from one region to another.</p><p>This can result in a few different problems in your application, which we'll talk about in the next section.</p>
<h2>Dealing with the effects of eventual consistency​</h2><p>Now that we know the different types of eventual consistency in DynamoDB, let's talk about some strategies for dealing with eventual consistency. As always, you must understand the requirements of your application to determine which strategies work best for you.</p>
<h3>Know the general latency characteristics for your flavor of eventual consistency​</h3><p>As discussed above, there are three flavors of eventual consistency in DynamoDB, and these flavors have very different replication lags. You may find that, practically, you rarely see the effects of eventual consistency in your application.</p><p>A common preference around consistency is something called "read your writes". This basically says that if a process makes a write and later performs a read, that read should reflect the previous write. It doesn't offer guarantees about reads from other processes, but it at least offers some sanity for a single workflow.</p>
<p>If your database allows you to direct operations to specific nodes, then a process can handle this by reading from the same server from which it wrote. With a load-balanced NoSQL database like DynamoDB, you don't get a choice from which server you read from. However, we can get a sense of how likely it is that I will get "read your writes" behavior from our different flavors of eventual consistency.</p><p>I did some basic testing of "read your writes" behavior against your main table and a global secondary index. You can read the full results and methodology in the GitHub repo here, but the high-level takeaways are:</p><p>As you can see, even an eventually consistent read against a main table is going to get darn close to 'read your write' consistency in the vast majority of cases.</p>
<h3>... but don't count on the general latency characteristics!​</h3><p>While understanding the general characteristics is useful, you shouldn't rely on this. As Ben Kehoe notes:</p><p>Ask me in person sometime about the ticket our devs opened after they built in an assumption of a maximum time for eventual consistency on DynamoDB</p><p>The tests above are very basic tests in standard operation. You shouldn't expect these results in all scenarios.</p><p>I would expect the main table results to be more bounded than the global secondary index results, given that you still need 2 out of 3 to commit the write, and a flapping third node is likely to be replaced quickly if needed. In fact, a new paper on DynamoDB talks about how DynamoDB uses log-only replicas to keep write latency low and durability high in the event of partition failures.</p><p>A global secondary index, on the other hand, has more potential failure modes, including ones of your own making (underprovisioned throughput or exceeding partition throughput limits).</p>
<h3>Use write conditions to guarantee consistency​</h3><p>As mentioned in the background, eventual consistency is mostly a read-based problem (check the next point for a caveat here). All writes to DynamoDB are happening against the latest version of the item. When making a write, you can include a ConditionExpression in your operation that must evaluate to True in order for the write to process.</p><p>The canonical example of potential problems with eventual consistency is a bank transaction -- what happens if I read a bank balance that is out of date, then approve a transaction based on the stale balance?</p>
<p>You can avoid this problem with the use of ConditionExpressions. Your ConditionExpression could require that the balance of the account is greater than the amount of the requested transaction in order to proceed. You can even combine multiple operations, each with their own ConditionExpressions, into a DynamoDB transaction in order to make these changes to multiple bank accounts with ACID semantics.</p><p>Amazingly, you're not necessarily going to avoid these issues with a relational database! Todd Warszawski and Peter Bailis have a fun paper showing how isolation levels below serializable can result in concurrency bugs during transactions. Peter Bailis and others have another fun paper showing how ORMs implement concurrency-control mechanisms that fail in subtle ways. I'm not saying this to scare you or raise FUD, but more to iterate that you really need to understand your infrastructure and your dependencies to avoid these issues altogether.</p>
<p>Write conditions won't save you from all your problems with eventual consistency, but they can help you avoid persisting data in inconsistent or invalid states. Look into strategies like pessimistic concurrency (aka locking), optimistic concurrency (aka version checking), or simply asserting data constraints in your write operations.</p>
<h3>Avoid multi-region writes where possible​</h3><p>Let's address the caveat from the previous section. While eventual consistency is mostly a read problem, it can become a write problem if you're using DynamoDB Global Tables. Now you have multiple regions, each able to accept writes that are asynchronously replicated to other regions.</p><p>There are two sources of potential issues with writing to multiple regions.</p><p>First, you could lose writes that occur nearly simultaneously. Imagine a write in Region A that updates attribute X, while a concurrent write in Region B updates attribute Y. In theory, both of these writes can succeed, and DynamoDB will do its best to reconcile concurrent writes. However, you could find yourself losing updates in specific situations.</p>
<p>Second, you could assert write conditions that are true in your region's copy of the data but that are false in another region's (yet-to-be-replicated) copy. Condition expressions and DynamoDB Transactions are consistent in a single region only, so writing in multiple regions can add problems.</p><p>When using DynamoDB Global Tables, I generally recommend users only write an item in a particular region. For many, that means choosing a "write" region and directing all writes there, even if the latency is higher. For some use cases, individual items may naturally be correlated with specific regions and thus you can ensure that each write will only ever be written in a single region.</p>
<h2>Conclusion​</h2><p>In this post, I hope we dispelled some of your fear about eventual consistency. First, we learned some background about eventual consistency generally. Then, we looked at the three flavors of eventual consistency in DynamoDB. As part of this, we developed our mental models of DynamoDB's underlying infrastructure to understand why the three flavors are different. Finally, we looked at some strategies for dealing with eventual consistency in DynamoDB.</p><p>If you have any questions or corrections for this post, please leave a note below or email me directly!</p>
<h1>GraphQL, DynamoDB, and Single-table Design</h1><p>I've written and spoken a lot about data modeling with DynamoDB over the years. I love DynamoDB, and I feel like I understand its pros and cons pretty well.</p><p>Recently, the topic of the compatibility of GraphQL and single-table design in DynamoDB came up again, sparked by a tweet from Rick Houlihan that led to a follow-up tweet indicating that I should update my post about single-table design where I mentioned that GraphQL might be an area where you don't want to use single-table design with DynamoDB.</p><p>Twitter is a bad medium for nuance, and I think the question of whether to use single-table design with GraphQL is a nuanced one that depends a lot on why you are choosing to use GraphQL.</p>
<p>If you want the TL;DR Twitter version of this post, it is: You absolutely can use single-table design with GraphQL, and I know some very smart people who are doing so. However, I don't think it's wrong to chose not to, depending on your specific needs + preferences.</p><p>Further, I think Amplify made the right choice in opting for a table-per-model default.</p><p>Post-publishing note: if you want to see some good practical tips on how to use single-table design with AppSync + GraphQL, check out this video from Adam Elmore on his setup. He also has some good conceptual points on when he does (and doesn't) use single-table design principles in his application.</p><p>If you want the full details, check out the post below. We will cover:</p><p>Let's goooooo.</p>
<h2>What are the benefits of GraphQL?​</h2><p>I want to start off by understanding the potential benefits of GraphQL in order to understand why people are choosing to use GraphQL.</p><p>Note that I refer to these as potential benefits. People may adopt GraphQL for all of these reasons, or there may be one or two reasons that are more important. Further, they may use GraphQL in a way that eschews one or more of the other potential benefits. I don't think these decisions are necessarily wrong -- it depends a lot on context and the particular needs of a team and product.</p><p>Finally, there are a few benefits of GraphQL that I don't mention here, such as the ability to specify the exact fields you want to save on network bandwidth. It doesn't mean these aren't important for some situations. Rather, they're not that relevant to the single-table vs. multi-table debate.</p>
<h3>API type safety​</h3><p>The first reason people like GraphQL is the type safety from the API.</p><p>Single-page applications make requests to a backend server to retrieve data to display in the application. On a shopping application, the requested data could be a list of products in a category or the contents of a customer's shopping cart. On GitHub, the requested data could be a GitHub repository or a list of issues for the repository.</p><p>In a standard JSON-based REST API, it's likely there is a schema for this data, at least theoretically. The problem comes in reality -- many frontend developers find that the returned data shape is unreliable, and developers need to implement lots of defensive coding to ensure they have the data properties they are expecting.</p>
<p>GraphQL helps with this problem by having a defined, typed schema that is available from the backend. Further, most GraphQL implementations will ensure that a response from the backend adheres to this schema before sending to the frontend. This makes it significantly easier as a frontend client of a GraphQL-based service as you have greater confidence over the returned data.</p>
<h3>Fewer network requests from the frontend​</h3><p>A second reason that people like GraphQL is to reduce the number of network requests from the frontend to the backend.</p><p>Let's return to our previous examples with a single-page application. If I'm implementing a shopping cart for my single-page application with a typical REST backend, I might need to make a number of calls to render the shopping cart page:</p>
<p>This is the dreaded N + 1 problem. The N + 1 problem is commonly used as an argument against using ORMs as evidence they can make suboptimal queries that add additional load on your database. However, we're often making this same problem on the frontend! By using single-page applications with a RESTful backend implementation, our frontend needs to make a number of requests to render the page. Further, there may be a sequential nature to this, as I can't make the N calls to fetch the item details until my first request to retrieve the shopping cart contents. Now I have a waterfall of sequential requests that make my application seem sluggish.</p><p>GraphQL reduces this problem by allowing a frontend client to make a single request that retrieves a graph of data. In a single GraphQL query, I can retrieve both my shopping cart and all of the items in the cart. As soon as this request finishes, I can paint the entire page without waiting for subsequent requests.</p><p>Notice that this doesn't necessarily eliminate the N + 1 problem entirely. As we'll see below, it could move the N + 1 problem to the backend. The only thing GraphQL does here is eliminate the N + 1 problem from the frontend application and developer.</p>
<h3>Flexible querying patterns​</h3><p>The third and final benefit of GraphQL for the point of this post is that GraphQL provides more flexible querying options for clients.</p><p>We saw above that GraphQL can eliminate the N + 1 problem from the frontend. Another alternative to reduce calls from the frontend to the backend is the Backends for Frontends (BFF) architecture pattern. With this pattern, backend APIs are implemented specifically for the frontends that will be calling them. For example, we may have an endpoint specifically for rendering our shopping cart page that assembles both the cart contents and updated details about the specific items on the backend before returning a complete response to the frontend.</p><p>Once fully implemented, the BFF pattern looks a lot like GraphQL. I can get the full graph of data I need in a single request rather than by making multiple calls to my backend.</p><p>The main difference between BFFs and GraphQL is in flexibility.</p>
<p>BFFs are designed specifically for my frontend experiences, and thus changing my frontend experience requires changing my backend as well. This can mean pulling in additional teams, dealing with backward compatibility issues, and other factors. As a result, BFFs can be slower to evolve.</p><p>In contrast, GraphQL has provides significantly more flexibility. With GraphQL, you publish a schema that describes the shape of your data. A client can make any request against that schema they need, and the GraphQL backend implementation will do the work to assemble the data. Thus, changing requirements on the frontend (usually) do not require backend changes as well. It's as simple as rewriting the GraphQL query for a page to change the desired data.</p><p>Generally, GraphQL backends use isolated, focused resolvers that retrieve specific pieces of data. This can result in N + 1 queries on the backend. For example, imagine our shopping cart query below:</p>
<p>The cart resolver would resolve details about the shopping cart overall, whereas details about the items would be handled by a separate resolver. On the backend, these would be handled sequentially -- after the cart resolver finishes, the items resolver would be called to fetch the items for the cart.</p>
<h3>Key takeaways​</h3><p>If you look closely at the benefits above, you'll notice that GraphQL makes life quite a bit easier for frontend developers. They don't have to be as defensive about the data they get back from an API. They don't have to make a waterfall of network requests and deal with partial errors. And they can iterate on the frontend without requiring backend assistance. Because the resolvers are implemented generically, independent of the originating query, it's easy to reshape a response simply by changing the GraphQL query input.</p>
<p>Because of this, I've generally seen more GraphQL adoption in engineering teams where frontend developers have more sway. There is nothing inherently wrong with this. Most engineering teams have some inherent bias toward a particular type of developer and optimize their systems accordingly. I think REST APIs make life a lot easier for backend developers -- by simplifyng the amount of data on any particular request -- by pushing a lot of that complexity to clients (often the frontend developers).</p>
<h3>Background on DynamoDB​</h3><p>I'm not going to go deep on the details of DynamoDB, but I do want to discuss some high-level points in order to help the discussion later.</p><p>DynamoDB is a NoSQL database from AWS. It optimizes for extreme predictability and consistency of performance. It wants to provide the exact same performance for a query when you have 1 MB of data as when you have 1 GB, 1 TB, or even 1 PB of data.</p><p>To do that, DynamoDB intentionally restricts how you can use it. It forces you to use a primary key for most of your access patterns. This primary key is a combination of a hash key and a B-tree to allow for fast, predictable performance when operating on an individual item or when retrieving a range of contiguous items. Because of this primary key structure, you should see single-digit millisecond response times on individual items regardless of your database size.</p>
<p>Additionally, DynamoDB removes certain features provided by other databases. You can't do JOINs in DynamoDB. You can't do aggregations across multiple records in DynamoDB. Both of these operations can operate on an unbounded number of items with unpredictable performance characteristics. DynamoDB will not let you write a bad query and will not provide features that allow you to do so.</p>
<p>Finally, DynamoDB has clear, strict limits in certain areas. It won't let you create an item over 400KB as a way to force you into smaller, more targeted items. More importantly, it has a limit on the number of concurrent reads and writes against a subset of your data. If you exceed that limit for an individual second, your request will be throttled and you'll need to try again. Again, this is in pursuit of consistency and predictability. It turns performance into a binary decision -- single-digit response that either succeeded or failed -- rather than having slowly degrading application performance under load.</p><p>Because of the choices made by the DynamoDB team in order to provide that consistency, you need to model your data differently. Most importantly, you have to think about how you will actually use your data.</p>
<p>In a relational database, you design your data in an abstract, normalized way. Then you implement the queries and potentially add indexes to assist in performance.</p><p>With DynamoDB, this pattern is flipped. You consider your access patterns first, then design your table to handle those access patterns. Your data probably won't be normalized as it would in a relational database. It's going to be handcrafted for your specific requirements.</p><p>Let's think back to our GitHub repository example before. Because DynamoDB does not have joins, fetching a repository and the ten most recent issues in the repository would take at least two requests to the database if you had two different tables. Depending on how you modeled the primary keys for the tables, it could be two sequential requests, which will increase latency. We're back to the old N + 1 problem we discussed before (or at least the 1 + 1 problem :)).</p>
<p>This is where single-table design comes in. If you know you have an access pattern where you need the repository and the ten most recent issues, you can assemble the items next to each other in a single DynamoDB table. Then, you can use a Query operation to fetch both the repository and the issue items in a single request. You've turned your N + 1 query into a single query, and you've done it in a way that doesn't burn through CPU like you may with a SQL join operation. (For more on single-table design in the context of one-to-many relationships, check out this post on how to model one-to-many relationships in DynamoDB).</p>
<p>When I talk about single-table design, I'm primarily using it to mean using DynamoDB in a way that uses the Query operation to retrieve multiple, heterogenous items in a single request. You can also use a single table with DynamoDB in a way that never retrieves heterogenous items. I often do! However, in that case, the difference between using one table or multiple tables is less stark.</p>
<h3>The case for (and against) single-table design with GraphQL​</h3><p>Now that we know about the benefits of GraphQL and of single-table design, can we merge these two in a happy marriage?</p><p>It really depends on how invested you are in flexibility as a key benefit of using GraphQL. Remember that DynamoDB relies on designing for known access patterns, while one of GraphQL's benefits is the ability to quickly iterate on access patterns. By using single-table design, you are combining multiple entities in a single table to handle patterns where you need nested, related data in a single access pattern.</p><p>Earlier, we talked about how most GraphQL implementations use isolated, focused resolvers to fetch specific pieces of data. If a complex query with nested data is requested, the GraphQL will pass it to sequential resolvers that each focus on fetching their own data.</p>
<p>However, this keeps the N + 1 problem we discussed in an earlier section. We moved this problem to the backend rather than the frontend, but we haven't eliminated it entirely. And even moving this to the backend can be a performance win, as the backend is closer to the database, so multiple queries can be faster there.</p><p>We can remove or at least significantly reduce the N + 1 problem if we want. Rather than writing focused GraphQL resolvers, we can write broader, more complex resolvers. A top-level resolver could "lookahead" at other bits of the query to see if nested data has been requested. If it has, the resolver could fetch that nested data as part of its request to the database. In a relational database, that probably means a JOIN in your query. In DynamoDB, that means utilizing the tenets of single-table design to fetch multiple, heterogenous items in a single request.</p>
<p>However, lookaheads in your GraphQL resolvers are controversial, and some would even go to the point of calling them an anti-pattern.</p><p>Marc-Andre Giroux, the author of Production-Ready GraphQL and the implementor of large, public GraphQL APIs at GitHub and Shopify, has the following to say about lookaheads:</p><p></p><p>Note his recommendation to avoid lookahead queries and ensure that your resolvers are focused and narrow. If you follow this advice, most of the advantages of single-table design in DynamoDB are gone. You should still think about how your data is accessed and design for that, but it will be on a much narrower scope. You will think about how specific entities will be accessed, but you won't think about collections of heterogenous entities.</p>
<h3>Making a choice​</h3><p>Given the discussion above, does that mean you should or should not use single-table design with GraphQL?</p><p>It depends! Software is about tradeoffs, and you need to find the right tradeoffs for your application.</p><p>From my very unscientific research, I'd say the majority of the GraphQL community agrees with Marc-Andre Giroux's advice. You should use isolated, focused resolvers that only fetch a specific piece of data. This gives you the benefit of GraphQL's query flexibility while taking the downside of reduced performance.</p><p>My guess is that Marc-Andre's advice is partly based on his specific experience. He helped design public-facing GraphQL APIs for GitHub and Shopify. Because these are to be used by the public rather than primarily by internal applications, they need significantly more flexibility.</p>
<p>Think of the resolver complexity in the GitHub GraphQL API if you implemented lookahead functionality with full query flexibility. I looked at the GitHub GraphQL schema, and the Repository object alone is immensely complex. I wanted to count all the different relations on the Repository object, but I gave up when I was at 11 and had only made it to discussionCategories.</p><p>Handling the variety of potential nested queries against a Repository would be near impossible. Even if your GraphQL API is smaller than the GitHub one, it can still be a lot of complexity to take on all the permutations of data.</p>
<p>Additionally, note that your problem at this point is not with GraphQL and DynamoDB. Your problem is with GraphQL, period. Marc-Andre's advice is not for a specific DynamoDB implementation. It's for all GraphQL APIs, regardless of the database used. Whether you're using DynamoDB, Postgres, or Neo4J, most GraphQL APIs are making multiple, sequential requests to the database to handle a nested query.</p><p>That said, I have seen multiple smart people advocate for single-table design principles with GraphQL. Adam Elmore has done so on Twitter, and Rich Buggy has written a nice post about single-table design with GraphQL.</p><p>In these situations, I'm guessing most of the GraphQL usage comes from clients that they control. The access patterns are known and can be planned for. They're using GraphQL for the API type safety and the fewer network requests from the frontend, but they're less concerned about infinitely flexible APIs.</p>
<p>In this same vein, I saw a tweet recently describing BBC's usage of GraphQL from a QCon talk. The talk noted that GraphQL queries need to be registered in advance before they go live. This sounds like known, specific access patterns to me! I don't know whether the BBC is optimizing their resolvers in conjunction with that, but it can be a nice way to get some of the benefits of GraphQL without going all-in on flexible queries.</p><p>In sum, you need to decide what's more important to you -- system performance or flexibility. I can't tell you what's more important because it depends a lot on your context, team, and product.</p>
<h2>Did the Amplify team make a mistake in its implementation?​</h2><p>Now that I've (hopefully) convinced you that single-table and multi-table are both acceptable choices with GraphQL, I do want to discuss a final question that spurred this whole discussion.</p><p>AWS Amplify is a toolkit on top of AWS AppSync that makes it easy to turn a GraphQL schema into a fully managed GraphQL server, complete with databases and even the resolver code (or code-like configuration). As part of its implementation, it creates a separate DynamoDB table for each object (denoted by an @model directive) in the GraphQL schema.</p><p>Given that single-table and multi-table are both acceptable, was it right for the Amplify team to push the defaults toward multi-table? I believe yes, for two reasons.</p>
<p>First, as mentioned above, isolated, focused resolvers appear to be the preferred option in the GraphQL community. It's not unanimous -- you can find examples of using lookaheads to enhance performance in GraphQL -- but that's the vibe I get.</p><p>Second, single-table design is playing on God-mode, and Amplify is designed to be a developer-friendly experience. Many people using Amplify are not building Amazon-scale services that require extreme performance. They're probably building MVPs and trying to iterate quickly to get traction. There may be a time for optimization later, but it's not on day 1.</p>
<p>Further, nothing precludes you from using single-table design with AppSync. As discussed, Rick has shown how to write a single-table resolver, and Rich Buggy has written a great post on this. There's a bit of asymmetry here -- someone that knows single-table design is knowledgeable to opt out of the defaults, whereas someone completely new to DynamoDB would be baffled by single-table design and would likely drop without knowing that multi-table is an option.</p><p>None of this is to excuse complaints of the difficulty of learning single-table design or new things generally. But given that I believe multi-table is a valid option for AppSync, it's a defensible choice to default to that mode.</p><p>Finally, if we're picking nits, my complaint about Amplify is that it makes it too easy to accidentally implement patterns using an inefficient DynamoDB Scan operation, which can blow up once an application goes to production.</p>
<h2>Conclusion​</h2><p>I hope this post has convinced you that both single-table and multi-table design are acceptable with GraphQL and DynamoDB. More broadly, I hope it's shown that GraphQL itself has a number of benefits and some downsides, and you need to pick what is important to you in your implementation.</p><p>If you have questions or comments on this piece, feel free to blast me on Twitter, leave a note below, or email me directly.</p>
<h1>How you should think about DynamoDB costs</h1><p>Last week, someone emailed me to ask about a potential cost optimization mechanism in DynamoDB. More on the specifics of that situation below, but the basic point is they were thinking about adding some additional application and architectural complexity because they were concerned about high DynamoDB costs for a particular use case.</p><p>I responded the way I always respond for these requests -- "have you done the math?"</p><p>One of my favorite things about DynamoDB is that you can easily do the math when considering how much it will cost you. I use this all the time in a few different ways, from getting a rough guess at how much DynamoDB will cost for my application to deciding between different approaches to solving a specific access pattern.</p><p>You can and should think about DynamoDB costs as you're designing your model to understand feasibility and weigh tradeoffs.</p>
<p>In this post, I'll walk through my approach to reasoning about DynamoDB costs. I'll start with an overview about how DynamoDB pricing works -- feel free to skip this if you're already familiar. From there, I'll walk through a few examples of how I use this to make decisions about DynamoDB costs.</p><p>This post isn't comprehensive on DynamoDB costs -- it's more about how to approach this and make cost modeling part of your design process. If you want some other tips on DynamoDB costs, check out Khawaja Sham's epic thread on patterns for saving costs with DynamoDB. It has a ton of wide-ranging thoughts on DynamoDB costs.</p><p>There also was a nice Reddit thread on DynamoDB costs yesterday. Some correct stuff, some incorrect stuff, but I was happy to see a few people mention you can just do the math.</p>
<h2>How DynamoDB pricing works​</h2><p>Let's start with a basic overview of how DynamoDB pricing works. You can get bogged down in the different pricing modes (on-demand vs. provisioned?) or table storage classes (Standard or Standard-IA?), but that is secondary to understanding the core dimensions on which DynamoDB charges.</p><p>Basically, DynamoDB is charging for three things:</p><p>Read consumption: Measured in 4KB increments (rounded up!) for each read operation. This is the amount of data that is read from the table or index. I'll refer to each increment as an 'RCU' (read capacity unit) for this post, even though it's sometimes called a 'read request unit'. Thus, if you read a single 10KB item in DynamoDB, you will consume 3 RCUs (10 / 4 == 2.5, rounded up to 3).</p>
<p>Write consumption: Measured in 1KB increments (also rounded up!) for each write operation. This is the size of the item you're writing / updating / deleting during a write operation. I'll refer to each write increment as a 'WCU' (write capacity unit). Thus, if you write a single 7.5KB item in DynamoDB, you will consume 8 WCUs (7.5 / 1 == 7.5, rounded up to 8).</p><p>Storage: Measured in GB-months. This is the amount of data stored in the table or index, multiplied by the number of hours in the month. I almost never think about this during modeling as read and write consumption are usually the dominant cost factors.</p><p>The great thing about this is its predictability! You don't have to wonder about how many concurrent operations a db.m6gd.large instance can handle. You don't have to worry about how much data you can store in a 1TB gp3 storage volume. You just have to worry about how much data you're reading, writing, and storing.</p>
<p>Not only this, but you can easily do a rough calculation of your costs as you're designing your data model. I'll walk through this further in the Doin' the Math sections below, but this is pretty straightforward math you can do with an Excel spreadsheet.</p><p>A few other quick notes / quirks about DynamoDB pricing:</p><p>Notice that RCUs and WCUs are step functions -- you're charged for each increment, even if you don't use the full increment. For example, if you read 1.5KB of data, you'll be charged for 1 RCU. If you write 0.5KB of data, you'll be charged for 1 WCU.</p><p>For reads, you have the option between a strongly consistent read request and an eventually consistent read request. An eventually consistent read request consumes half as many RCUs as a strongly consistent read request. IMO, you should almost always use an eventually consistent read request. If you think otherwise, let's fight :)</p>
<p>A DynamoDB Query allows you to read multiple items in a single request. When doing a Query operation, the items will be added together first before dividing by 4KB to determine RCU usage. Thus, using a Query to read 100 items of 100 bytes each will cost you 3 RCUs (10KB / 4 == 2.5, rounded up to 3). If you had read each of those 100 items separately, it would have cost you 100 RCUs!</p><p>For reads, you're charged for the data that's actually read, not the data returned to the client. Thus, DynamoDB Filter Expressions don't help the way you think they might. You should almost always use your key attributes to filter your results.</p>
<p>When doing a write operation, you get charged the larger of the size of item as it was before the operation or as it is after the operation. For example, if you have an existing item that is 9.5KB and you increment a single counter integer on it, you will pay the full 10 WCUs for the operation. Likewise, if you delete that 9.5KB item, you will pay 10 WCUs.</p><p>When you have a condition expression that fails, you'll still be charged WCUs for it. Further, it will be based on the size of the matching item (if any), with a minimum consumption of 1 WCU. (This one kind of makes me grumpy.)</p><p>Using DynamoDB Transactions doubles your WCU usage. It's using two-phase commit under the hood, so basically think of being charged for each phase.</p>
<h3>Choosing between provisioned and on-demand​</h3><p>In the examples below, we'll do some calculations to see how you can think about DynamoDB pricing. For my examples, I always calculate using DynamoDB On-Demand billing mode. On-Demand billing has a fixed price for reads and writes, so it's easy to calculate the cost of a particular access pattern. I don't have to think about things like utilization over time or providing a buffer to handle unexpected bursts of traffic. This greatly simplifies the math and gives you a directionally accurate look at your potential costs.</p><p>However, this doesn't mean you should never think about your billing mode! Once you've run the initial numbers, then you can fine-tune your calculations by looking at the difference between provisioned and on-demand.</p>
<p>At a first cut, if the on-demand numbers are negligible, you should probably stick with that. On-demand is going to be a hands-off solution that (almost completely) avoids throttling. If the potential bill won't break the bank, take the operational simplicity and move on.</p><p>However, when you're talking about real money, then you can consider whether provisioned capacity would be a worthwhile optimization. In general, on-demand billing is 6.94x as expensive as fully-utilized provisioned capacity.</p><p>This seems like a lot, but note the key qualifier there -- fully-utilized. You're not going to get anywhere near full utilization, so saving 85% is not a realistic goal. However, it's not unrealistic to think you could get to 50% utilization, which would save you 42% on your bill. For a high-traffic DynamoDB workload, that's a pretty significant savings!</p>
<p>Just remember what you're taking on. You're responsible for scaling up and down to account for traffic patterns. If traffic grows over time, you're responsible to account for the new highs. If you have a spike in traffic, you're responsible for scaling up to handle it. If you have a spike in traffic that you don't expect, you're responsible for handling it. If you don't do this, you'll get throttled and your users will be unhappy.</p>
<h2>Doin' the math pt. 1: Tracking view counts​</h2><p>Alright, enough background chatter. Let's get into some examples.</p><p>The first example is what spurred this blog post. Someone wrote me an email about potential DynamoDB costs in tracking view counts. Every time a user visits one of his pages, he wants to increment a counter for that page.</p><p>He was worried about the cost of this and asked what I thought about batching page views in Redis for the short term, then occasionally writing them back to DynamoDB. This adds some cost (Redis instance!) and some complexity (multiple sources of data! syncing back to DynamoDB!), but he was thinking about doing it out of fear of a large DynamoDB bill.</p><p>The first thing I told him -- "Do the math!" Look at how big your item sizes are, then try to roughly estimate your traffic. I love spreadsheets, so I usually throw this in a spreadsheet and fiddle with the numbers to see what I'm dealing with.</p>
<p>In this situation, imagine his page record was pretty small -- under 1KB. If so, each write will be 1 WCU. Thus, he can estimate that his DynamoDB write costs will be $1.25 per million page views (on-demand billing is $1.25 per million WCUs).</p><p>This pricing may not be acceptable to you, but it gives you a sense of what you're dealing with. For him, it helped him realize that it wasn't something to optimize yet. He should spend his time on other things.</p><p>PS: If you are thinking 'how do I know my item size?', generate an example record in your application and then paste it into Zac Charles' DynamoDB Item Size Calculator. I love this thing.</p>
<p>Note that I discussed an enhanced version of this during my 2022 talk at AWS re:Invent. I talked about 'vertical sharding', which can be a nice use of single-table design principles to split a single entity into multiple records to save on costs. For this example, imagine our underlying record was much larger -- 10KB instead of 1KB. Now, each million page views will cost us $12.50 even though we're just incrementing a counter.</p><p>Instead, we could separate our record into two items -- the page view counter and the actual item data. We can place them in the same item collection to fetch them in a single Query request when needed, but now a page view increment is only acting on the small, focused item. This would save 90% of our write costs for this example.</p>
<h2>Doin' the math pt. 2: Do I really need that secondary index?​</h2><p>Alright, that's an easy example. Let's look at another example that involves some tradeoffs between different approaches to an access pattern within DynamoDB.</p><p>We saw earlier that each write to DynamoDB is charged 1 WCU per KB of data, rounded up. However, this applies to each target to which the write will be applied. I use the word 'target' to refer to not just the base table but any secondary indexes to which the item will be written.</p><p>It's not uncommon to have 4-5 secondary indexes on a DynamoDB table. When this happens, your write costs can quickly add up. Think of our view count example above. If we had 5 secondary indexes, we'd be paying 6 WCU per page view instead of 1 WCU!</p>
<p>There are lots of ways to reduce costs for GSIs that you truly need (check out Khawaja's tweet about saving money on GSIs during his awesome thread on DynamoDB savings generally), but sometimes you may be better off avoiding a GSI altogether.</p><p>Let's see an example.</p><p>In DynamoDB, I tell people that you almost always want to filter and sort the exact records you need via the key attributes. The key word here is 'almost'. There are certain times when it may be better to over-read your data.</p><p>Let's work with a simple example. Imagine you're selling a SaaS service that is purchased by teams. By nature of the industry you're in, it's natural that teams will be small. It would be extremely rare that over 100 people would belong to a single team. In the vast majority of cases, teams will be fewer than 10 users.</p>
<p>Each User record within the team is pretty small -- 1KB of data. However, you also have a few access patterns on the User -- maybe a point lookup to fetch a User by username, and two range queries -- one to fetch all the Users with a given role, and one to fetch all Users on the team ordered by the date added to the team.</p><p>You could set up a GSI for each of these secondary access patterns. However, you could also just handle the range queries by fetching all the Users on the team and filtering them in your application code. Even in the worst case scenario, you'll be fetching 100KB of data. That's 12.5 RCUs (100KB / 4 = 25 * 0.5 for eventually consistent reads = 12.5 RCUs). For most cases, it will be much smaller than that -- likely less than a single RCU. Further, RCUs are not only 4 times bigger than WCUs (4KB vs. 1KB), they're also 5 times cheaper ($0.25 per million as opposed to $1.25 per million for WCUs). </p>
<p>Based on some back-of-the-napkin math, it might be better to skip the GSI and over-read data to handle long-tail access patterns. This is particularly true when the overall result set is pretty small.</p><p>Again, you should still focus on using your key attributes as much as possible. However, there are times when you can relax that default assumption to save on write costs.</p>
<h2>Doin' the math pt. 3: Complex filtering​</h2><p>One of the harder patterns in DynamoDB is what I call "complex filtering". This is when you may need to filter a dataset by a number of potential variables, all of which can be optional. DynamoDB wants to work with known access patterns, and this includes knowing the attributes you'll be filtering on. This kind of dynamism can be tricky for DynamoDB.</p><p>If this is a pattern you really need to support, you can again do some math to see if it's a feasible situation.</p><p>Another example. Imagine you have a CRM system that tracks customers and their orders. You want users to be able to filter their customers by a number of different attributes -- name, email, city, and industry segment. You also want to be able to filter by the date they were added to the system. Each of these attributes is optional in your filter.</p>
<p>These customer records can be pretty big -- let's say 4KB. Because of this, it's infeasible to add all the permutations of access patterns as GSIs. This could easily be 5 GSIs. This would make each update to your User cost 24 WCUs (4KB * 6 = 24 WCUs). This is a lot of write capacity for a single record!</p><p>But we also can't use the simple over-reading approach that we used in the previous example. Because each User is 4KB, we'll be paying half an RCU per customer on an eventually consistent read. A user might have a thousand customers in their system, so we'd be paying 500 RCUs per query to fetch them all. That's a lot of RCUs!</p><p>What are our options? Well, notice that while our application allows filtering on a few different attributes, it's still a very small portion of the record on which we filter. We only need about 100 bytes of the record, not the full 4KB. </p>
<p>We could use a secondary index and take advantage of index projections (more detail on secondary index projects from the wonderful Pete Naylor here). With an index projection, you can choose which attributes are included in the secondary index. As Pete notes, this helps us reduce writes by avoiding writes when only non-projected attributes are changing.</p><p>But it also helps in another way -- reducing our item size for complex filtering. If our item sizes are a mere 100 bytes, now we can fetch 40 customers per RCU unit. If we want to fetch 1000 customers, it only costs 40 RCUs (or 20 RCUs for eventually consistent reads). That's 4% of our original cost!</p>
<p>Note that if you need to fetch the full records, you'll need to do a follow-up, BatchGetItem operation to hydrate them. This is an additional cost to consider, both in terms of read costs and in response latency . However, if you're doing complex filtering, you may not be fetching the full record anyway. You're probably just showing a list of customers that match the filter, and then allowing the user to click on a customer to see the full record.</p><p>This pattern can also help you see when use cases are infeasible. If a user might have 100,000 customers in their CRM, now you're looking at 4000 RCUs to fetch and filter them all. If this is going to be a common pattern for you, you'll be burning through RCUs like crazy. At this point, I usually recommend integrating something like (e.g. Rockset or Elasticsearch) to augment your core DynamoDB patterns with some complex filtering. This adds cost and complexity, but it's often the right tradeoff for your use case.</p>
<h2>Conclusion​</h2><p>In this post, we learned how to think about DynamoDB costs. We walked through the background of how DynamoDB does billing and then worked through some examples of how to think about DynamoDB costs in practice.</p><p>The strongest point I want to emphasize is that you should use the predictability and legibility of DynamoDB to your advantage. Use it to understand whether an access pattern will be cost effective. Use it to weigh the tradeoffs of different approaches to DynamoDB. And, yes, use it to understand when DynamoDB might not be the right fit for a particular use case.</p>
<h1>Event-Driven Architectures vs. Event-Based Compute in Serverless Applications</h1><p>I recently delivered serverless training to some engineers, and there was confusion between two concepts that come up in discussons of serverless architectures.</p><p>On one hand, I describe AWS Lambda as event-based compute, which has significant implications for how you write the code and design the architecture in your serverless applications.</p><p>On the other hand, many serverless applications use an event-driven architecture that relies on decoupled, asynchronous processing of events across your application.</p><p>These two concepts -- event-driven architectures and event-based compute -- sound similar and are often used together in serverless applications on AWS, but they're not the same thing. Further, the patterns you use for one will not necessarily apply if you're not using the other.</p>
<p>The event-based nature of Lambda compute is what fundamentally distinguishes Lambda from other computing paradigms and is what leads to the unique constraints and demands of serverless application developers.</p><p>In this post, we'll look at both event-driven architecture and event-based compute. We'll examine the key characteristics of each as well as the implications for you in your applications.</p>
<h2>What is an event-driven architecture​</h2><p>Event-driven architectures are all the rage, so we'll start there.</p><p>Event-driven architectures are characterized by services that communicate (1) asynchronously (2) via events. These two elements distinguish them from other architecture patterns.</p>
<h4>Event-driven architectures communicate asynchronously​</h4><p>If you've worked with a frontend client calling a backend GraphQL API or a backend service calling another service via a REST API or RPC, you've used the request-response pattern. This is a common pattern for communication from one client to a service. It is a synchronous flow where the client will wait for a full response from the service indicating the result of the request. A synchronous flow is simple as you get direct feedback on what happened from your request.</p>
<p>However, relying on synchronous communiation has costs as well. It can reduce the overall performance of your application, particularly when working with slower tasks like sending emails or generating reports. Further, the availability of your services goes down. If Service A relies on synchronous responses from Service B to operate, the uptime for Service A cannot be higher than that of Service B. Service B's downtime becomes Service A's downtime.</p><p>With asynchronous patterns, these problems are reduced. Services can still communicate, but there's no expectation of an immediate response. The downstream service can process the communication on its own schedule without tying up the upstream service. This adds its own challenges around debugging, eventual consistency, and more, but it does reduce the downsides of synchronous communication.</p><p>For more on request-response vs. event-driven, check out Former AWS Developer Advocate Talia Nassi's post on the benefits of moving to event-driven architectures.</p>
<h4>Event-driven architectures communicate via events​</h4><p>Unsurprisingly, the "events" bit is the more unique part of event-driven architectures. In event-driven architectures, services will broadcast events that will be consumed by and reacted upon by other services.</p><p>Thus, events require two types of parties:</p><p>Event producers, which publish events describing something that happened within the service (a User was created, an Order was delivered, or a Login Attempt failed).</p><p>Event consumers, which subscribe to published events and react to them (updating local state, incrementing aggregates, triggering workflows, etc.).</p><p>A key feature of true event-driven architectures is that the producers and consumers are completely decoupled -- a producer shouldn't know or care who is consuming its events and how the consumers use those events in their service..</p>
<p>An event producer is like a broadcaster on the nightly news. The broadcaster will say the news that happened regardless of whether anyone is tuned in.</p><p>Contrast this with a more traditional message-driven architecture in which one component in a system may insert a message to a message queue (like SQS or RabbitMQ) for processing. A message-driven pattern is also asynchronous, like an event-driven pattern. However, the producer of the message is purposefully sending the work to a specific customer. The use of the queue helps with resiliency and with faster response times from the initial component, but it's not truly "event-driven" in the normal sense of the term given the tight connection between the producer of the message and the consumer.</p>
<p>If event-driven is like a TV broadcast, message-driven is more like your boss sending you an email with a request to create a report. Not only is there a specific output she wants (the report), but there's a specific person she wants to do it (you).</p><p>Some resources out there will consider message-driven workflows to be a part of an event-driven architecture. I sort of disagree but mostly don't care too much. I do agree that some implications and reasons for using message-driven patterns are similar to event-driven patterns and thus some of the lessons are similar.</p><p>The main benefits of event-driven architectures are in their flexibility and resiliency. If I want to add a new consumer of a given event, I don't have to coordinate with the producer of the event. I can start processing the events as they are published and use them in my new service.</p>
<p>Event-driven architectures have been around for a while. If you talk to a developer that spent time at an enterprise shop from the 90s and 2000s, you'll likely hear complaints about the enterprise service bus. More recently, the rise of Apache Kafka and the very effective evangelism from Jay Kreps (one of the original Kafka creators) about the effectiveness of logs & streams has breathed new life into event-driven architectures.</p><p>There are a lot of flavors of event-driven architectures out there, including purist patterns like event sourcing and CQRS. I generally recommend avoiding those -- they're fun to think about and imagine all the possibilities, but I've seen them turn into a maintenance and debugging headache.</p><p>This was only a cursory review of event-driven architectures. If you want more on this, AWS Developer Advocate David Boyne is the person to follow on all things event-driven. Lots of great stuff on event-driven architectures, including a great set of visuals on Serverless Land.</p>
<h2>What is event-based compute​</h2><p>Now that we understand a bit about event-driven architectures, let's turn to event-based compute to see how it differs.</p><p>There are two key characteristics of event-based compute:</p><p>First, the existence of a compute instance is intimately tied to the occurence of an event to be processed.</p><p>Second, the compute acts on a single event at a time.</p><p>That's sort of abstract, so let's make it more concrete.</p><p>Imagine you create a Lambda function. You write a bit of code, create a ZIP file, upload it to AWS, and set up the configuration in the Lambda service. Configuring this doesn't actually start your code, like it might with an EC2 instance or a Fargate container. By default, there will be no actual instances of your Lambda compute running. Your Lambda function has potential, but it hasn't realized it yet.</p>
<p>To make your function a reality, you need to hook it up to an event source. There are a ton of services that integrate with Lambda. The most popular sources are probably API Gateway (HTTP requests), SQS (queue processing), EventBridge (event bus), and Kinesis / DynamoDB Streams (stream processing).</p><p>Note that, on the Lambda docs page linked above, it uses the term 'event-driven' for a lot of event sources, including many that I would not describe as event driven! More on this below.</p><p>Once you've configured your event source and an event has flowed through the configured service, then your function will spring to life. The Lambda service will create an instance of your function and pass the triggering event to it for processing by your function. Your function will process the event as desired and return a response back to the event trigger.</p>
<p>For optimization purposes, the Lambda service may keep your function instance running for a bit to serve other events that occur in a short time period. However, the specifics of this is (mostly) out of your control.</p><p>Importantly, the purpose of an instance of running compute is to handle a single, specific event. This distinguishes Lambda from traditional instances or containers which are created to be available to handle requests as they arrive or to poll for messages from a queue or stream. It even distinguishes Lambda from something like creating a Fargate task on a schedule. While the creation of the task is based on an event, the task doesn't naturally have awareness of the event that created it while executing.</p>
<h3>Implications of event-based compute​</h3><p>We now know how AWS Lambda is event-based computing -- so what? How does this actually affect our applications?</p><p>In my mind, this is the most crucial shift about AWS Lambda. It leads to some of the following preferences from engineers using Lambda:</p><p>Note that none of these preferences is universal -- there's a lot of diversity across Lambda-based architectures. However, they are preferences that you tend to see, particularly in comparison to architectures that use other methods of compute.</p><p>There are two main implications from Lambda's event-based nature that you should keep front of mind.</p><p>First, you need to think about statelessness and rapid initialization more than usual. In an instance-based or container-based application, you can initialize your application, establish database connections, build local caches, and perform other initialization work before making your application available to handle requests.</p>
<p>This is not the case with Lambda compute. Remember, you may be initializing your compute in response to an event. There might be an active HTTP request with a real live user on the other end of it, waiting to see the latest Tweets or to buy some Taylor Swift tickets. You need to make sure your code can load and execute quickly without a bunch of setup.</p><p>Specifically, this means keeping your function code small and shallow. Try to avoid loading and initializing nested dependencies across many files. Depending on your tolerance for punishment, you can look at using esbuild or other tools to bundle your code into a single file and reduce init-time disk reads.</p>
<p>Also, consider tips to improve performance on subsequent requests. There are kinds of work that you must do on your cold-start initialization, like establishing network connections to databases or other resources or retrieving dynamic config. However, you should understand how to reuse those resources across multiple requests to avoid making every request to your compute perform like a cold start.</p><p>Second, scaling is about more instances of compute than concurrency within a single instance. Within a Lambda function instance, you will be processing one and only one event at a time. If multiple events occur at the same time, the Lambda service will spin up more function instances to handle the events. However, it will always be a single event at a time.</p>
<p>This will change not only how you write your application code but potentially even the language you choose. The inimitable Ben Kehoe once said that Node is the wrong runtime for serverless because of its async-first approach. This approach is helpful for a traditional backend application where you can handle multiple web requests or operate on batches of queue messages in parallel. However, because Lambda works on a single event at a time, you usually aren't doing a lot of asynchronous work. You're processing a single event in order and thus can benefit from a more straightforward procedural style.</p><p>Ben's post is nearly 6 years old at this point, and some of the changes in Node.js since then (specifically async / await) help to sand off the rough edges of Node's model in a Lambda world. But regardless of the language you choose, you should consider how the single-event model affects your application.</p>
<p>Because Lambda scales horizontally across function instances rather than vertically within function instances, you can't share resources across concurrent requests within your compute layer. The easy example here is a pool of database connections, which a traditional web server would use to share across many requests. This leads to the preference noted above where Lambda users prefer databases without connection limits (like DynamoDB) or to implement database pooling outside of their compute (such as via Amazon RDS Proxy).</p>
<p>Further, because your compute is working on a single event at a time and is billed as long as it is active, you want to limit the time your application is doing nothing. No more using setTimeout() in your web server to handle a background job. More commonly, you'll want to avoid regular long-polling within your function. If you're waiting on something to be done before moving forward, see if you can turn it into an event itself. If you can't do that, try to implement the polling trigger outside of your compute.</p>
<h2>Confusion over "event"​</h2><p>Now we know about event-driven architecture and event-based compute, how do they interact? Do you have to use event-driven architectures with AWS Lambda? Can you use event-based compute with Kubernetes?</p><p>As mentioned above, I think the confusion in this area is not helped by the Lambda documentation that describes services to interact with Lambda. That document describes a number of sources as "event-driven" that do not fall under the traditional definition of event-driven, such as HTTP requests from API Gateway. Further, integration with most of the services that do utilize event-driven patterns are not defined as event-driven, such as DynamoDB Streams, Kinesis Streams, and Apache Kafka.</p><p>Luckily for you, I've made a handy Venn diagram to show how these patterns overlap.</p><p></p><p>In general, the rules are:</p><p>AWS Lambda functions are always event-based compute</p>
<p>You may use event-driven patterns with your AWS Lambda functions</p><p>You can use event-driven or non-event-driven patterns with compute other than AWS Lambda</p><p>I'm not trying to cover all the cases here. There are other tools (OpenFaaS, Knative) that allow for event-based compute. I'm less familiar with them, but many of the same principles are likely to apply.</p>
<h3>Lambda + AWS services: which are event-driven and why?​</h3><p>After publishing this post, Emily Shea had a great suggestion to elaborate on why certain AWS services are or are not event-driven when connected with Lambda. Below is a quick overview. There are caveats here, but I try to outline the general shape of how a service is used with Lambda.</p><p>API Gateway + Lambda: Generally not event-driven. This is a request-response pattern as the client will receive a response from your Lambda function indicating the result of the request. If you're using API Gateway in a traditional REST API sense (e.g. "Get User", "Add to Cart", etc), it's not event-driven. It is synchronous and coupled.</p>
<p>That said, you can use API Gateway as an entrypoint to an event-driven system. For example, a service or a frontend client could publish an event to an API Gateway endpoint, which would send it through to a Kinesis Stream or EventBridge bus. The big tell here will be the response code or body. If you get a response code of 202 Accepted or a response body that only includes an eventId or messageId property, it could indicate the API is a frontend to an event-driven system.</p><p>SQS + Lambda: Generally not event-driven. It is asynchronous, which differs from the synchronous request-response pattern above. However, a queue-based pattern usually has a specific task that the producer desires (e.g. "Send a welcome email after user creation."). Because of this, it's a "message-driven" pattern. May also be called "point-to-point".</p>
<p>There are caveats here as well. SQS might be used as a buffering / throttling mechanism as part of an event-driven system. For example, a consumer to SNS or EventBridge (discussed below) may push messages to SQS and process from there. SQS can provide greater control and visibility over throttling and retry logic.</p><p>SNS + Lambda: Event-driven! 🎉 At least in theory. An SNS topic can have many consumers, and a publisher to an SNS topic usually doesn't have a specific outcome in mind. It is an asynchronous, decoupled pattern.</p><p>EventBridge + Lambda: Also event-driven! Basically the same points as SNS here. In practice, EventBridge is probably even more likely to be event-driven than SNS given its singular focus on an event bus with many different event types.</p>
<p>Kinesis + Lambda: Often event-driven! Similar to SNS and EventBridge, but a stream-based solution like Kinesis allows for batch processing instead of individual events as a time. My hunch is that Kinesis is somewhat less likely to be used for 'pure' event-driven architectures as the asynchronous batch processing it enables is good for situations even when the producer and consumer know about each other.</p><p>Step Functions + Lambda: Not event-driven! A Step Function state machine executes a specific, defined, multi-step workflow. It may have some asynchronous elements, but there's no de-coupling of producers and consumers here! That said, you may want to introduce some event publishing in between steps of your state machine execution. Yan Cui's post on Choreography vs. Orchestration covers this topic and is one of my favorite serverless posts.</p><p>Thanks to Emily Shea for the suggestion and to Maik Wiesmüller for notes on the original writeup. 🙏</p>
<h2>Conclusion​</h2><p>In this post, we reviewed the difference between event-driven architectures and event-based compute. We saw that architectures with all types of compute can benefit from event-driven architectures (in the right situations!). Further we saw that all use of Lambda is relying on event-based compute. We also saw some of the implications of event-based compute and the ways in which it affects the design of your application.</p><p>If you have any questions or corrections for this post, please leave a note below or email me directly!</p>
<h1>Why I (Still) Like the Serverless Framework over the CDK</h1><p>Over the past year or two, I've seen the AWS CDK turn a lot of my friends into converts. And in the very recent past, a few of these people have written up their (mostly positive) experiences with the CDK. See Maciej Radzikowski on why he stopped being a CDK skeptic here or Corey Quinn's list of the CDK's hard edges which, ultimately, is still favorable. For a more skeptical view of the CDK, check out Mike Roberts' excellent thoughts here.</p><p>Some of these articles describe similar concerns I have, but none of them quite nails my thoughts on the matter. I thought I'd throw my hat in the ring and describe why I still prefer the Serverless Framework over the CDK.</p><p>In this post, I'll start off with my biases and background to help frame my thinking. Next, I'll discuss four reasons why I think the Serverless Framework is a better abstraction for building serverless applications on AWS. Finally, I'll describe some areas that the CDK does do well.</p>
<h2>My Background and Preferences​</h2><p>Let's start with some facts that factor into my preferences. Your situation might not overlap with mine and, thus, some of my points may not be applicable to you.</p><p>First and foremost, I worked for Serverless, Inc., creators of the Serverless Framework, for about two and a half years. This is relevant! I learned a ton about AWS + serverless while working there, and I built a lot of knowledge around the Serverless Framework in particular. I don't have any financial interest in Serverless, Inc., but I do have friends that I like and respect there. In this case, familiarity breeds affinity.</p>
<p>Second, I am primarily building "little-s" serverless applications on AWS. The term "serverless" has gotten pretty slippery now that the marketers have descended, but I generally take this to mean "applications which use primarily managed services glued together with AWS Lambda functions." Yes, it's true that Lambda != serverless, or that you can do "functionless" in your serverless application. I'm more cautious around functionless, but if that's what you're about, have at it! That said, the vast majority of serverless applications on AWS include Lambda at the core.</p>
<p>More importantly for the discussion below is what my applications don't have. I very rarely have a VPC, which requires a lot of complicated, boilerplate-y configuration that is easy to screw up if you don't know what you're doing. I don't have security groups, or autoscaling pools, or ECS service definitions that might make using the CDK more pressing. I also don't have a need to create a lot of resources that are similar but slightly different, another area where the CDK could help.</p>
<p>Finally, I have a pretty strong bias for boring in my programming. This claim might seem a little rich for some people, given that I advocate using AWS Lambda and DynamoDB for most applications. You might think something ought to be more commonplace before we call it boring. But I'd actually argue, in solidarity with Brian LeRoux, that Lambda and DynamoDB are fairly boring at this point. They may be newer than some technologies, but it's actually easier to wrap your head around their operating model and, more importantly, their failure modes than those of other technologies.</p><p>Another aspect of this boring bias is my preference against cleverness. My first programming language was Python, where explicit over implicit is written into the language itself. It took me a while to learn this lesson, but now I dislike too much indirection in programming. Be kind to your future self -- keep it simple.</p><p>These are my biases, and they may be very different from your own. If that's the case, feel free to take the rest of this post with a grain of salt.</p>
<h3>A standard structure for every application​</h3><p>We're already a few hundred words into this post, and I haven't even described how the Serverless Framework and the AWS CDK differ.</p><p>Both tools are basically pre-processors for AWS CloudFormation. Their core difference is in the process they use to get to CloudFormation.</p><p>With the Serverless Framework, you're writing a single, declarative YAML configuration file called serverless.yml that will describe your application. It will look something like the following:</p><p>It has a number of top-level blocks, including functions for creating Lambda functions and resources for provisioning other AWS resources.</p>
<p>The key abstraction is really in the functions block, as it focuses on simplifying the process around creating Lambda functions and hooking up event sources to these functions. This is the core of most serverless applications, and simplifying this process is a big win. We'll talk about this further in the next section.</p><p>The CDK takes a different approach. Rather than a declarative configuration file, the CDK has you write imperative code in a general-purpose programming language (TypeScript, Python, Golang, etc.) to describe the infrastructure you want. With this structure, you can use things like functions to abstract common operations or loops to create multiple resources with similar configuration. Further, you can have abstracted modules from other files or even other NPM packages that encapsulate resource creation logic.</p>
<p>I like the Serverless Framework because I always know where I am. If I come to an existing project that I've never seen before, I can look for the serverless.yml file and understand the structure of the application -- what functions there are, where they're located in the repository, how they're triggered, and what supporting resources they use.</p><p>If you're from the Rails ecosystem, you might think of the Serverless Framework's approach as 'convention over configuration'. Mohamed Said recently made the same point about Laravel applications:</p><p>The biggest reason why following the Laravel convention is so important is that I can look into a team's work after weeks and I know exactly where to look to find things.</p>
<p>In contrast, every CDK repo I've looked at turns into a murder mystery. I'm digging through application code that is mixed with infrastructure code, trying to find my function handlers, my service code, anything that will help me solve The Case of the Missing IAM Permission.</p><p>Some people have discussed the long-term issues around CDK maintainability -- what if people aren't updating your constructs or staying on top of CDK version updates?</p><p>Those issues are real, but I'm way more worried about how you maintain developer knowledge around a specific CDK application. There are real hurdles to onboarding a new developer into your custom structure. They don't just need to know the CDK, they need to know your team's specific implementation of the CDK.</p><p>The knowledge of your application that is built up within your team is hidden, and it will result in slower ramp-up times for new developers.</p>
<h3>The Serverless Framework abstracts the right things​</h3><p>I mentioned in the last section that both the Serverless Framework and the CDK are abstractions over CloudFormation. In my opinion, the Serverless Framework does a better job of abstracting the right things to make it easier to work with serverless applications.</p><p>There are two hard things about deploying serverless applications:</p><p>The Serverless Framework has rightly identified that functions and events are core to serverless applications and has made them the core abstraction. Let's look at another example functions block in a serverless.yml file:</p>
<p>When I run serverless deploy to deploy my stack, it's going to build and configure a function on AWS for each function in my functions block (here, there are three: createUser, processQueue, and fanout). For each function, it may install your dependencies, run a build process, zip up the directory, upload it to S3, and register the function and its basic configuration with the Lambda service. You don't really need to know much about that packaging process, unless you have specific needs.</p><p>To be fair to the CDK, some of their function abstractions, like the NodeJsFunction construct do most of this too. Anecdotally from friends, I've heard that the function build part of CDK is under-loved compared to the core infrastructure deployment part, but it's functional.</p><p>One example of this is from Roman here:</p><p>That’s good defaults I think. How’s your overall experience with these lambda constructs? I find it still problematic to build, test and publish ts lambdas</p>
<p>The bigger difference between the two is in the second hard part of serverless applications -- configuring events to trigger your functions.</p><p>In the serverless.yml example above, there's a consistent format for potential trigger. Each function has an events property where you can configure an array of events (though it's usually one per function). Each configured event has a type and any required properties for the event.</p><p>Thus, configuring a Lambda function to connect to API Gateway to handle HTTP requests feels very similar to configuring a function to consume from an SQS queue or to handle SNS notifications.</p><p>And sometimes these configurations are quite complex! Creating a Lambda function to handle an API Gateway endpoint requires creating 6-8 resources, including two sub-resources on an API Gateway instance (a Resource and a Method), an IAM role for your function, plus a resource-based permission on your function so API Gateway can invoke it.</p>
<p>That's a lot! With the Serverless Framework, that's abstracted away from you. Further, it's not abstracted in a way that harms you (as we'll discuss in the next section).</p><p>If you compare configuring these three events types in the CDK, you'll see there are three completely different ways to manage it.</p><p>With API Gateway, you need to create the API Gateway REST API instance. Then, you need to create a resource on the API (or multiple resources, if this is a nested route). Finally, you need to add a method on your created resource that integrates with your created Lambda function. Check an example of this from Borislav Hadzhiev here.</p><p>If you're doing an SQS integration, the format is completely different. Under the hood, certain Lambda integrations use event source mappings, and the CDK exposes that directly to you. Thus, connecting a Lambda function to an SQS queue looks like this:</p><p>Source: CDK docs</p>
<p>Notice that you create a Lambda function, create an SQS Queue, and then create a third thing to connect them together. Notice that this third thing, the event source mapping, is attached to the Lambda function.</p><p>If we go to the third type of event source, connecting to an SNS topic, we find a third pattern:</p><p>Source: CDK docs</p><p>Similar to SQS, we create a Lambda function and the related resource (an SNS topic). However, to connect them, we attach the function to the resource (rather than the resource to the function, like in SQS).</p><p>In each of these three examples, CDK more accurately reflects the resources required to create and configure these integrations. However, I think it retains some complexity that can easily be abstracted away. For the most part, I don't need to know about event source mappings vs. API Gateway Lambda integrations (though I do need to know about the various types of error handling across different services!).</p>
<p>I think this leads to a confusing mental model of how serverless applications on AWS should be built. You should think of it as a Lambda-centric model (or perhaps "compute-centric", if you're a functionless fanatic) where you're using lots of managed services but tying them together via Lambda. With the Serverless Framework, Lambda functions and their events are the core, and the supplemental resources are the leaf nodes. With CDK, it puts all resources on the same footing, making it harder to organize how your application works.</p>
<h3>The CDK abstracts the wrong things​</h3><p>On the flip side of the previous section, I also think that the CDK abstracts the wrong things.</p><p>But first, I have a confession. In the previous section, I made the Serverless Framework event configuration look slightly easier than it is. For both the SQS and SNS integrations, we're referring to additional infrastructure in our application -- an SQS queue and an SNS topic.</p><p>While the Serverless Framework looks at functions and events as the core of your serverless applications, it also realizes you will need additional supporting infrastructure to do anything meaningful. That's where the resources block of your serverless.yml comes in.</p><p>In the resources block, you can configure any additional AWS resource you want via straight CloudFormation. For our SQS- and SNS-enabled application above, our resources block would look as follows:</p>
<p>The part I love about this is that you're actually writing CloudFormation. You're learning the tool that you'll be using under the hood.</p><p>There are a lot of complaints about CloudFormation, but they mostly center on the verbosity of it -- there's just so much I have to write. And it's boilerplate!</p><p>But as we saw in the last section, most of the boilerplate CloudFormation has been abstracted from you in functions and events. The resources you create in the resources block are pretty flat and direct. There's not a lot you can abstract away in SQS queues, SNS topics, or DynamoDB tables. (For the latter, AWS SAM tried with the SimpleTable resource, but it turns out the other properties of DynamoDB tables are pretty useful!)</p><p>Additionally, your Lambda functions will need to have the requisite IAM permissions to interact with your resources, and you'll need to specify those in IAM-policy format using the iam block in serverless.yml.</p>
<p>I think part of the attraction of CDK is that it abstracts both raw CloudFormation and IAM. Rather than writing declarative YAML, you're invoking a method with some properties. And rather than detailing the specific IAM statements you need, you're doing something like myTable.grantReadWriteData(myFunction) to allow your function to read and write to your DynamoDB table.</p><p>But at some point, you need to learn those fundamentals. If your deployment fails, it's going to fail in a CloudFormation-specific way. If your permissions don't work, it's going to tell you the specific resource and IAM action combination that was disallowed. If that is completely foreign to you, you're not going to be able to debug effectively.</p><p>AWS is a massive ecosystem -- thousands of services with intricate connections, complex auth requirements, and loads of pitfalls. You can do some amazing things, but it's truly daunting to newcomers.</p>
<p>I was able to massively upskill my AWS ability through the Serverless Framework, and I've seen the same for others. With the Serverless Framework, you can get started with a simple function and event connection quickly, without learning the specifics of CloudFormation and IAM. Then, you can gradually layer in new pieces -- as you want persistent storage, you add a DynamoDB table resource and the needed PutItem and GetItem permissions. It's all incremental, so you're learning as you go, rather than tossing you into the deep end once something inevitably goes wrong.</p>
<h3>The CDK enables our bad impulses​</h3><p>The previous section was about how the Serverless Framework helps beginners to AWS. But you, dear reader, may not be a beginner. Maybe you think you don't need the training wheels that the Serverless Framework provides. I'm here to tell you that you may need the structure that the Serverless Framework provides more than anyone.</p><p>CDK proponents love to mention that you can write code for your infrastructure. But one of my biggest problems with the CDK is that you can write code for your infrastructure. I don't mean to turn into a jaded old man, but I've seen some stuff in CDK applications. Stuff that would give you nightmares.</p><p>As engineers, we can be too clever for our own good. We can fall in love with abstraction, with DRY-ing up our code, or with beautiful code. But too often, these elegant edifices are impenetrable to outsiders or difficult to maintain over time.</p>
<p>There are two specific problems I've seen with the CDK, though I'm sure there are others.</p><p>The first is that it's too easy to create a mess of resources without understanding the underlying purpose or intent. The most common place I see this is with an inexplicable explosion of CloudFormation Stacks. One CDK application I saw used seven different stacks for a pretty simple service (~8 HTTP endpoints, an S3 bucket, a DynamoDB table, and a few state machine definitions). Note that these weren't CloudFormation Nested Stacks, which are inherently tied to one another, but were separate, independent stacks, which made deployment (and failures in deployment) much more complicated.</p>
<p>The second is in over-emphasizing abstractions that can be reused across teams. This is a core concept in CDK -- there are different levels of 'constructs' from low-level CloudFormation resources ("L1 constructs") to highly specific patterns ("L3 constructs"). People get fired up about this because they think they can write this One Resource to Rule Them All to solve all their problems, but the reality is a lot trickier.</p><p>I think the average software developer (and I include myself in this group!) is actually not that great at writing reusable, library code. This is doubly true for the average CDK-adjacent developer (again, I'm in this group!) who is either a DevOps-y shaped engineer that's doing some product work or a full-stack-y engineer that's learning just enough AWS to be dangerous. Few developers boast a deep knowledge of cloud infrastructure combined with the ability to create useful abstractions.</p>
<p>Taylor Otwell, creator of Laravel, shared a screenshot of an HN discussion recently that hit on similar points:</p><p>Suspect this is also very true for Laravel projects. 😅 pic.twitter.com/reyAWYkH6U</p><p>Good, solid abstractions are hard to make. But they're sooo tempting to try. In most cases, you're better off relying on proven, tested abstractions rather than trying to write your own.</p>
<h2>What the CDK does well​</h2><p>I've spent a lot of time talking about what I don't love about the CDK, but it can't be all bad. The CDK is getting a lot of adoption in the AWS circles in which I travel, and many people whom I respect are loving the CDK. I do want to outline some of the benefits. And because I refuse to have any fun, I'll also point out the downside of each one.</p><p>Given that, what is the CDK doing well that is accounting for its popularity? There are three core areas I see.</p><p>First and foremost, the CDK removes some of the drudgery of infrastructure-as-code. As much as I like the Serverless Framework, writing YAML can be boring. Moving from declarative template-land into imperative coding-land can be a lot more fun. Plus, there are some benefits around auto-complete, and the L2 constructs can help with the low-level bits of gluing these things together.</p>
<p>I'm cautious about this benefit, because fun does not necessarily equal good. Further, as mentioned above, I think the Serverless Framework abstracts a lot of the true drugery of little-s serverless applications. However, we have to acknowledge this impetus as a real factor. And if you're a more experienced AWS engineer that understands IAM, CloudFormation, and the IaC process, you're not losing as much by going to the CDK.</p><p>A second benefit of the CDK is an easier way to create lots of similar resources. While on the excellent aws.fm podcast, Matthew Bonig noted he had a situation where a team was making hundreds of similar RDS instances. The copy-pasta required to create all of these or to encode best practices can be difficult, and the CDK makes this much easier than a more declarative format.</p>
<p>That said, I've only had this situation come up one time in my ~6 years of doing serverless development, and even that was for a resource that had 6-7 similar instances. The slight copy-pasta required to maintain it wasn't a burden. But if this applies to you, go for it!</p><p>A third, more common, benefit is the ability to abstract truly boilerplate stuff. If you're creating a VPC or certain other AWS solutions that require a ton of configured resources, being able to abstract that in a single construct that has sane defaults is a big benefits.</p><p>The main downside to this is that you still are responsible for owning that infrastructure! The CDK can make it easier on Day 1, but that infra is still yours on Day 2.</p><p>It reminds me of this tweet from Gwen Shapira:</p>
<p>All AWS EKS "get started" docs tell me to run CloudFormations. I ended up with 32 resources - mostly network related. I have no idea why they all exist and what each does. How will I debug things if they break? or help someone else if my stuff doesn't work for them?</p><p>Gwen is talking about a standard CloudFormation template, but the point is the same. If you're using something off the shelf to avoid learning about it, it's a ticking time bomb as to when you'll actually have to learn it. And at that point, the pressure will be higher.</p><p>Finally, I like the experimentation that the CDK is doing. Tech and software moves in fits and starts, and it's hard to predict what will work. There have been lots of tech trends that I've been skeptical of or downright hostile to. Sometimes I was wrong!</p><p>Infrastructure as code is not a solved problem yet. Let a thousand flowers bloom.</p>
<h2>Conclusion​</h2><p>In this post, I described why I still prefer the Serverless Framework for building serverless applications on AWS. I prefer the way it constrains our impulses, its convention over configuration, and the way it incrementally teaches the cloud to users. I also did a quick run through of things the CDK does well.</p><p>I realize opinions will differ on this, and that's OK! I'm here to describe the tradeoffs as I see them and in light of my situation and preferences. If yours differ, then your conclusion might as well. Hit me up and let me know why I'm wrong :)</p><p>Thanks to Matt Bonig for his comments on this post. Be sure to check out The CDK Book, an excellent resource he wrote with three other smart people. All mistakes are mine.</p><p>If you have any questions or corrections for this post, please leave a note below or email me directly!</p>
<h1>Key Takeaways from the DynamoDB Paper</h1><p>In 2007, a group of engineers from Amazon published The Dynamo Paper, which described an internal database used by Amazon to handle the enormous scale of its retail operation. This paper helped launch the NoSQL movement and led to the creation of NoSQL databases like Apache Cassandra, MongoDB, and, of course, AWS's own fully managed service, DynamoDB.</p><p>Fifteen years later, the folks at Amazon have released a new paper about DynamoDB. Most of the names have changed (except for AWS VP Swami Sivasubramanian, who appears on both!), but it's a fascinating look at how the core concepts from Dynamo were updated and altered to provide a fully managed, highly scalable, multi-tenant cloud database service.</p><p>In this post, I want to discuss my key takeaways from the new DynamoDB Paper.</p><p>There are two main areas I found interesting from my review:</p><p>This post will be at a higher-level than the paper, though I strongly recommend reading the paper itself. It's really quite approachable. Additionally, Marc Brooker has written a nice review post that includes some interesting systems-level thoughts on the paper.</p>
<h2>Product-level takeaways from the DynamoDB Paper​</h2><p>Both the Dynamo paper and the DynamoDB paper describe some incredible technical concepts, but I'm equally impressed by the discussion of user needs. In both papers, there is a deep review of existing practices to see what is important and what should be re-thought around core user needs.</p><p>In the Dynamo paper, we saw this in the articulation that much of the higher-level querying functionality provided by an RDBMS is unused by Amazon's services. Werner Vogels expanded on this later as he mentioned that 70% of database operations were single-record lookups using a primary key, and another 20% read a set of rows but only hit a single table.</p>
<p>The Dynamo paper also noted that the traditional guarantee of strong consistency, while critical in some circumstances, was not necessary for all applications. In many cases, the enhanced availability and reduced write latency achieved by relaxing consistency requirements was well worth the tradeoff.</p><p>Just as the Dynamo paper re-examined some shibboleths from traditional database systems, the DynamoDB paper explores user needs around what was needed to make the Dynamo learnings applicable more broadly. In doing so, DynamoDB was able to institute a set of clear product priorities that distinguish DynamoDB from many other database offerings.</p><p>There are three important notes on user needs that I took from the paper:</p>
<h3>The importance of consistent performance​</h3><p>One point that the DynamoDB paper hammers over and over is that, for many users, "consistent performance at any scale is often more important than median request service times." Stated differently, it's better to have a narrower range between median and tail latency than it is to reduce median (or even p90 or p95) latency.</p><p>This is a surprising point to some people who think "DynamoDB is super fast." The story is more nuanced than that.</p><p>In my DynamoDB talks, I often show the following:</p><p></p><p>In this chart, we see that RDBMS latency will get worse as the amount of data in your database increases, whereas DynamoDB will provide consistent performance as your data increases. This same relationship holds as the number of concurrent requests increases.</p>
<p>The chart above is a rough sketch, but the overall point stands. At certain levels of data and transaction volume, an RDBMS will have faster response times than DynamoDB. Conceptually, this is easy to understand. A request to DynamoDB will pass through a number of systems -- a request router, a metadata system, an authentication system -- before making it to the underlying physical storage node that holds the requested data. While these systems are highly optimized, each one of them adds latency to the overall request. Conversely, a single-node RDBMS can skip a lot of that work and operate directly on local data.</p><p>So MySQL might beat DynamoDB at the median, but that's not the full story. There are two reasons you should also consider the full spectrum of your database's performance.</p>
<p>First, tail latencies are important, particularly in architectures with lots of components and sub-components. If a single call to your backend results in a lot of calls to underlying services, then each request is much more likely to experience the tail latency from some service, resulting in a slower response.</p><p>Second, the consistency and predictability of DynamoDB's performance profile leads to less long-term maintenance burden on your service. You don't have to come back to investigate, tune, and refactor as your performance inevitably declines. You know you'll get the same performance in your test environment as you will five years after launch, allowing you to focus on more value-generating features for your users.</p>
<h3>Fully managed over self-managed​</h3><p>If you're reading this blog, you're probably drinking the cloud Kool-Aid and may even be fully into the serverless world. In the serverless world, we're as focused as possible building the key differentiators of our business while offloading the undifferentiated heavy lifting to others.</p><p>But the internal experience of Amazon retail and the creators of Dynamo (not DynamoDB) really drives this home.</p><p>Recall that the Dynamo paper was hugely influential in the industry, and Amazon's internal Dynamo system was a big improvement in terms of availability and scalability for the enormous scale at which Amazon was operating.</p><p>Despite this improvement, many internal engineers chose to eschew running Dynamo themselves in favor of using Amazon SimpleDB, which was AWS's first foray into the hosted NoSQL database market.</p>
<p>If you've never heard of Amazon SimpleDB, you're not alone. DynamoDB is essentially a successor to SimpleDB and is superior in nearly every aspect. AWS rarely markets it anymore, and it's mostly a mascot for how AWS will never deprecate a service, even when better options exist.</p><p>SimpleDB has some truly surprising downsides, such as the fact that the entire database cannot exceed 10GB in size. This is a huge limitation for most applications but particularly for a company who has applications so large that they had to design a completely new database. Yet engineers were choosing to use batches of multiple SimpleDB tables to handle their needs, likely sharding at the application layer to keep each database under 10GB.</p><p>This had to add significant complexity to application logic. Despite this, engineers still chose to use SimpleDB over operating their own Dynamo instance.</p><p>This revealed preference by Amazon engineers helped spur the development of Amazon DynamoDB, a database that combined the scalability of Dynamo with the fully managed nature of SimpleDB.</p>
<h3>User data isn't as evenly distributed as you want​</h3><p>The final user takeaway is that you have to work with the users you're given, not the users you want. In an ideal world, users would have steady, predictable traffic that spread data access evenly across a table's keyspace. The reality is much different.</p><p>The original Dynamo paper used the concept of consistent hashing to distribute your data across independent partitions of roughly 10GB in size. (Partitions are discussed in more depth below). It uses the partition key of your items to place data across the partitions, which allows for predictable performance and linear horizontal scaling.</p><p>Further, unlike the original Dynamo system, DynamoDB is a multi-tenant system. Your partitions are co-located with partitions from tables of other DynamoDB users.</p>
<p>Originally, the DynamoDB team built a system to avoid noisy neighbor issues where high traffic to one partition results in a reduced experience for unrelated partitions on the same storage node. However, as the system developed, they realized that the initial system to manage this led to a subpar experience for those with spiky, unbalanced workloads.</p><p>As the AWS team built out DynamoDB, they realized they needed to evolve the access control system that managed whether a partition was allowed to service a request. We look more into the technical aspects of this evolution below.</p>
<h2>Technical takeaways from the DynamoDB Paper​</h2><p>The product-level learnings are fascinating, but this is ultimately a technical paper. The work the DynamoDB team is doing at massive scale is impressive, and many of the technical learnings apply even to those without DynamoDB's scale.</p><p>I had three technical takeaways that were most interesting from the paper:</p>
<h3>Using log replicas to improve durability and availability​</h3><p>One of the more interesting points was how DynamoDB uses something called log replicas to assist during periods of instance failure. To understand log replicas, we first need some background on the underlying architecture of DynamoDB's storage.</p><p>Start of DynamoDB storage background section</p><p>Under the hood, DynamoDB is splitting your data into partitions, which are independent storage segments of roughly 10GB in size. DynamoDB uses the partition key to assign your items to a given partition, which allows DynamoDB to scale horizontally as your database grows while still keeping related items together. DynamoDB is running a massive fleet of storage nodes which are handling partitions from many different user tables.</p><p></p>
<p>An individual partition is actually a set of three partition instances in different availability zones which form a replication group. One of the instances is the leader for a given partition and is responsible for handling all writes. When a write comes in, the leader writes it locally and ensures it is commited to at least one additional replica before returning to the client. This increases durability in the event of failure, as the loss of one node will not result in loss of data.</p><p></p>
<p>On each storage partition are two data structures -- the B-tree that contains the indexed data on the partition along with a write-ahead log (WAL) that contains an ordered list of updates applied to that partition. A write-ahead log is a commonly used tactic in databases to enhance the durability and latency of write operations. Updating the B-tree is slower as it involves random I/O and may include re-writing multiple pages on disk, whereas updating the write-ahead log is an append-only operation that is much faster (P.S. the write-ahead log is basically the concept behind Kafka and related systems!).</p>
<p>Note that in addition to the performance difference of an individual operation against these structures, there's also a vast difference in the size of these two structures. The B-tree can be 10+ GB in size (accounting for the 10GB size of a partition along with the index overhead), whereas the write-ahead log is only a few hundred megabytes (the full history of the write-ahead log is periodically synced to S3, which is used to power point-in-time restore and other features).</p><p>End of DynamoDB storage background section</p>
<p>When you're running a system as large as DynamoDB, instances are failing all the time. When a storage node fails, you now have potentially thousands of partition replicas that you need to relocate to a non-failing node. During this failure period, every replication group that had a replica on that node is now down to two replicas. Because two replicas are required to acknowledge a given write, you're now increasing the latency distribution of your writes as well as the probability of an availability event if another replica fails.</p><p>To reduce the period where only two replicas are live, DynamoDB uses log replicas. A log replica is a member of a replication group that contains only the write-ahead log. By skipping the B-tree for the partition, the DynamoDB subsystem can quickly spin up a log replica by copying over the last few hundred MB of the log. This log replica can be used to acknowledge writes but not to serve reads.</p>
<p>While this log replica is helping the replication group, the DynamoDB subsystem can work in the background to bring up a full replica member to replace the failed one.</p><p>It's so interesting to see the incremental tweaks the team has made to continually push the envelope on durability, availability, and latency. Most of these aren't altering the core promises that the system makes, such as making a different choice on the CAP theorem. Rather, they're steady improvements to the reliability and performance of a database.</p><p>First, DynamoDB uses the standard combination of a write-ahead log with the indexed storage to improve durability and reduce latency on write requests.</p>
<p>Second, DynamoDB discards the typical single-node setup of a RDBMS and moves to a partitioned system. This reduces recovery time (and hence availability) as recovering a 10GB partition is much faster than recovering a 200GB table. Further, this replication is across three different availability zones (AZs) so that an entire AZ can go down without affecting availability of the system.</p><p>Then, DynamoDB relaxes the consistency requirements to require that only two of the three nodes in a replication group acknowledge the write. At the cost of some occasionally stale data, DynamoDB is able to enhance the availability and reduce the latency of writes.</p><p>Finally, DynamoDB uses log replicas to improve availability during periods of node failure.</p>
<h3>Decoupling partitions from throughput​</h3><p>In the previous section, we discussed how partitions are used to segment data within a table and allow for horizontal scaling. Additionally, we saw how DynamoDB co-locates partitions from different customers on the same storage nodes to allow for greater efficiency of the DynamoDB service.</p><p>A second interesting technical takeaway is the slow, steady improvements to the "admission control" system for these partitions. Admission control refers to the process in which DynamoDB determines whether a request can succeed based on the amount of capacity available. In determining this, DynamoDB is checking capacity across two axes:</p><p>The first one is a cost decision, as DynamoDB wants to make sure you're paying for the service they're delivering. The second one is a performance decision, as they want to avoid noisy neighbor issues from co-located partitions.</p>
<p>The first iteration of admission control was purely at a partition level. DynamoDB would divide the total provisioned throughput by the number of partitions and allocate that amount to each partition evenly. This was the easiest system, as you didn't have to coordinate across lots of partitions on a second-by-second basis. However, it led to issues with unbalanced workloads and the "throughput dilution" problem. This could lead to situations where requests to hot partitions were being throttled even though the table wasn't using anywhere near its provisioned capacity.</p><p>To fix this problem, DynamoDB wanted to decouple admission control from partitions but realized this would be a big lift. To handle this, they moved in stages.</p>
<p>First, they improved the partition-level admission control system. While each partition was limited to prevent over-consumption of resources on an individual node, they also realized that storage nodes were often running under full capacity. To help with temporary spikes in traffic to individual partitions, DynamoDB added short-term bursting that would let a partition use additional throughput if it was available for the given storage node. This improvement was mostly focused on the second axis of access control -- protecting against noisy neighbors.</p>
<p>A second initial improvement helped with the other axis of access control -- the provisioned throughput for an individual table. As mentioned, a table with skewed access patterns might consume all the throughput for one partition while still being below the total provisioned throughput for the table. To help with this, DynamoDB added adaptive capacity, where throughput from sparsely used partitions could be shifted to highly used partitions.</p><p>These two changes, while still maintaining the general partition-based access control scheme, alleviated a significant amount of pain based on uneven access patterns of data.</p>
<p>Later, DynamoDB moved to a global access control system which decoupled throughput from partitions entirely. This changed adaptive capacity from a slower, 'best efforts' system to a nearly instant system to spread your throughput across your partitions. This flexibility led to amazing other improvements, including the ability to separate particularly hot items onto their own partitions, to provide DynamoDB On-Demand billing, and to 'overload' storage nodes based on predicted workloads of the underlying partitions.</p><p>All of this is recounted in more detail in Section 4 of the paper, and it is well worth your own read to understand the details.</p>
<h3>The use of asynchronous caches​</h3><p>The last technical takeaway was in DynamoDB's use of asynchronous caches. By "asynchronous cache", I'm meaning a system that caches data locally but then rehydrates the cache behind the scenes, asynchronously, to ensure it stays up to date.</p><p>We all know caches as a way to reduce latency by storing the results of an expensive call. In both cases mentioned in the paper, individual request router instances are storing the results of external calls locally to avoid a slow network request. But there are two more subtle points that are pretty interesting. In reviewing these, we should note how DynamoDB treats "external" systems (also called "dependencies") from "internal" systems.</p>
<p>DynamoDB uses other AWS services, such as IAM to authenticate requests or KMS to encrypt and decrypt data. Both of these services are external dependencies as they're not under the control of the DynamoDB team. Here, DynamoDB will cache the results of calls to these services as a way to increase availability. These results are periodically refreshed asynchronously to ensure freshness. This allows DynamoDB to keep working (somewhat) even if these external services are having issues themselves. Without this, DynamoDB's availability would necessarily be lower than those of IAM and KMS.</p><p>DynamoDB also uses asynchronous caches for 'internal' systems. DynamoDB has a metadata system that tracks table information and locations for each DynamoDB partition. When a request comes to a DynamoDB request router, it needs to find the relevant partition for the given item to forward the request to the storage node.</p>
<p>This metadata information doesn't change frequently, so the request routers heavily cache this data. The paper notes that the cache hit rate is 99.75% (!!), which is quite good. However, a high cache hit rate can also lead to problems where slight decreases in traffic can result in significantly more load to the underlying service. Decreasing the metadata cache hit rate from 99.75% to a still-amazing 99.5% results in twice as many requests to the underlying metadata service.</p><p>The DynamoDB team found that the metadata service had to scale in line with the request router service, as new request routers had empty caches that resulted in a lot of calls to the metadata service. This led to instability in the overall system.</p>
<p>To increase resiliency of its internal systems, DynamoDB uses asynchronous cache refreshing to provide constant load to the underlying metadata system. While the request routers would cache locally with a high hit rate, each hit results in an associated request to the metadata service to refresh the cached data.</p><p>By pairing a local cache hit with an asynchronous request to the metadata service, it ensures a more consistent rate of traffic to the metadata service. Both a cache hit and a cache miss result in a request to the metadata service, so increasing the number of request routers with cold caches doesn't result in a burst of new traffic to the metadata service.</p>
<p>There's a lot of other really interesting information about the metadata caching system that I won't cover here, but I thought these two uses of asynchronous caches were interesting. Both used local, instance-based caching to reduce latency but also coupled with asynchronous refreshing to decouple availability from external dependencies and to increase the resiliency of internal services.</p>
<h2>Conclusion​</h2><p>Once again, Amazon has helped to push forward our understanding of deep technical topics. Just as the Dynamo paper was revoluationary in designing new database architectures, the DynamoDB paper is a masterful lesson in running and evolving large-scale managed systems.</p><p>In this post, we looked at the core learnings from a user-needs perspective in the DynamoDB paper. Then, we looked at three technical learnings from the paper.</p><p>The paper has a number of other interesting points that we didn't cover, such as how DynamoDB monitors client-side availability by instrumenting internal Amazon services, the strategies used to deploy new versions of DynamoDB against an enormous fleet of instances, and the mechanisms used to protect against data errors, both in flight and at rest. If you write up an examination of those points, let me know and I'll link to them here!</p><p>If you have any questions or corrections for this post, please leave a note below or email me directly!</p>