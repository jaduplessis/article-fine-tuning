
<Chunk>

# How to secure CI/CD roles without burning production to the ground

By now, most of us have moved away from using IAM users for CI/CD pipelines. Instead, we’d use dedicated CI/CD roles, one for each pipeline. This forces us to consider who can assume this role. Identity federation is widely supported by 3rd-party providers such as GitHub Actions [1]. So, no more putting IAM credentials in CI/CD tools and worry that they might be compromised in a security breach [2].

However, attackers can still compromise the pipeline through supply chain attacks. For example, by compromising a Docker image we depend on in our CI/CD pipeline. Or by compromising static analysis tools such as eslint [3]. So, the question of “ How best to limit CI/CD role’s permissions?” has come up many times during my Production-Ready Serverless [4] workshop. Your instinct might be to lock down the CI/CD role to just what it needs. Because we all want to follow the principle of least privilege. There are different ways to achieve this. Here are two common approaches:

1. Start with a blank slate and add permissions to the role until it’s able to deploy your application.
2. Start with the AdministratorAccess policy and allow the role to do everything. Use CloudTrail to track the actions performed by the role over some time. Then use the CloudTrail data to create a tailored set of IAM permissions.

The first approach is very time-consuming and laborious and is best avoided. But in truth, both approaches have significant drawbacks in the long run.

<Chunk>
 Your instinct might be to lock down the CI/CD role to just what it needs. Because we all want to follow the principle of least privilege. There are different ways to achieve this. Here are two common approaches:

1. Start with a blank slate and add permissions to the role until it’s able to deploy your application.
2. Start with the AdministratorAccess policy and allow the role to do everything. Use CloudTrail to track the actions performed by the role over some time. Then use the CloudTrail data to create a tailored set of IAM permissions.

The first approach is very time-consuming and laborious and is best avoided. But in truth, both approaches have significant drawbacks in the long run.
## Rollback surprises

For most people, CloudFormation rollbacks don’t happen often. Especially for those of us who don’t write CloudFormation templates by hand. So when a deployment fails for the first time, you find out that the CI/CD role is missing a whole bunch of permissions! This can leave you in an awkward position where your stack is stuck in the ROLLBACK_FAILED state. And you are stuck until you resolve the missing permissions. In essence, crafting a least privileged CI/CD role takes twice as much work as you think. Because you also need to factor in all the permissions for rollbacks.

<Chunk>
## Rollback surprises

For most people, CloudFormation rollbacks don’t happen often. Especially for those of us who don’t write CloudFormation templates by hand. So when a deployment fails for the first time, you find out that the CI/CD role is missing a whole bunch of permissions! This can leave you in an awkward position where your stack is stuck in the ROLLBACK_FAILED state. And you are stuck until you resolve the missing permissions. In essence, crafting a least privileged CI/CD role takes twice as much work as you think. Because you also need to factor in all the permissions for rollbacks.
## A tax on productivity

Your architecture is not static. It needs to evolve with your application and your business needs. Every time you introduce a new service, you also need to revise the CI/CD role. Want to start using OpenSearch? Not before you update the CI/CD role! Want to start using Bedrock? Not before you … you get the point. In organizations where the application team doesn’t own the CI/CD role, this productivity tax can be steep. It requires many back-and-forth between teams and hits productivity hard. And you likely have to do these for every service because every service has its own CI/CD pipeline. This friction is a constant tax on your productivity. It hampers innovation and delays the adoption of new services into the tech stack. Because introducing a new service means new CI/CD permissions. Who in their right mind wants to deal with all that bureaucracy?

<Chunk>
 Want to start using OpenSearch? Not before you update the CI/CD role! Want to start using Bedrock? Not before you … you get the point. In organizations where the application team doesn’t own the CI/CD role, this productivity tax can be steep. It requires many back-and-forth between teams and hits productivity hard. And you likely have to do these for every service because every service has its own CI/CD pipeline. This friction is a constant tax on your productivity. It hampers innovation and delays the adoption of new services into the tech stack. Because introducing a new service means new CI/CD permissions. Who in their right mind wants to deal with all that bureaucracy?
## They are not as safe as you think

The reason why we pursue least privileged CI/CD roles is to reduce the blast radius of a security breach. If an attacker compromises our CI/CD pipeline, we want to limit what the attacker can do. However, the CI/CD role likely needs to deploy IAM roles and Lambda functions. In the event of a compromise, a least privileged CI/CD role is not enough to stop the attacker. Because the attackers can use the CI/CD role to create confused deputies to act on their behalf. For example, attackers can use the CI/CD role to create IAM roles for them to assume. And they can also create Lambda functions to carry out malicious activities. We can mitigate these risks to some extent with permission boundaries. But that requires lots more work and it’s hard to get right. And again, you have to do it for every CI/CD role.

<Chunk>
 If an attacker compromises our CI/CD pipeline, we want to limit what the attacker can do. However, the CI/CD role likely needs to deploy IAM roles and Lambda functions. In the event of a compromise, a least privileged CI/CD role is not enough to stop the attacker. Because the attackers can use the CI/CD role to create confused deputies to act on their behalf. For example, attackers can use the CI/CD role to create IAM roles for them to assume. And they can also create Lambda functions to carry out malicious activities. We can mitigate these risks to some extent with permission boundaries. But that requires lots more work and it’s hard to get right. And again, you have to do it for every CI/CD role.
## Prefer a permissive role that is hard to abuse

It’s counterproductive to blindly pursue least privileged CI/CD roles. Beyond a certain point, you get diminished return-on-investments. That is, you have to work much harder to address security risks that are unlikely to happen. With that in mind, here’s my preferred approach to securing a CI/CD pipeline.

Use a separate AWS account for each environment

This is something everyone should do. It insulates environments from each other. If one environment is compromised, it’s contained by the account boundary. If an attacker compromises your dev environment, they won’t be able to access your production data.
For large organizations, you should go a step further. I recommend having at least one account per team per environment. That way, teams are insulated from other teams’ mistakes.
For business critical systems, they should have their own set of accounts too. That is one account per service per environment. It’s not always necessary. But it’s a good idea to protect these business critical systems from other systems.

<Chunk>
 It insulates environments from each other. If one environment is compromised, it’s contained by the account boundary. If an attacker compromises your dev environment, they won’t be able to access your production data.
For large organizations, you should go a step further. I recommend having at least one account per team per environment. That way, teams are insulated from other teams’ mistakes.
For business critical systems, they should have their own set of accounts too. That is one account per service per environment. It’s not always necessary. But it’s a good idea to protect these business critical systems from other systems.
## Service Control Policies (SCPs)

Use SCPs to deny access to unused resources. These broad strokes include access to unused AWS regions and unused services. This way, you can eliminate large parts of AWS as possible attack surfaces.
For example, if you’re not running any EC2 instances, then deny all EC2 activities. That’s one less target for crypto-jacking attacks. But hey, ECS is the new EC2 for crypto-jacking. If you can’t disable ECS as a whole, then at least deny access to the regions where you don’t have any containers.
## Attribute-Based Access Control (ABAC)

Support for ABAC is improving all the time, although it’s still lacklustre in its current form. But many services already support the `aws:ResourceTag/${TagKey}` and `aws:PrincipalTag/${TagKey}` conditions. These conditions can be used to stop attackers from accessing and creating resources. They can be added to the IAM permissions of the CI/CD role. Or they can be enforced through a permission boundary.
To make them more effective, the CI/CD role should be denied the ability to find out about itself. That way, the attacker can’t find out what tags the CI/CD role has and what tags they need to use to create new resources.
Doing so helps us stop the attacker from accessing our resources through the CI/CD role. But they might target the following services to execute malicious code or escalate their privilege:
- Lambda
- EC2
- ECS
- IAM
- CodePipeline
- CloudFormation
Luckily, all these services (and many others) support the `aws:ResourceTag/${TagKey}` condition. We can use this condition to stop the attacker from creating new resources.

<Chunk>
 Or they can be enforced through a permission boundary.
To make them more effective, the CI/CD role should be denied the ability to find out about itself. That way, the attacker can’t find out what tags the CI/CD role has and what tags they need to use to create new resources.
Doing so helps us stop the attacker from accessing our resources through the CI/CD role. But they might target the following services to execute malicious code or escalate their privilege:
- Lambda
- EC2
- ECS
- IAM
- CodePipeline
- CloudFormation
Luckily, all these services (and many others) support the `aws:ResourceTag/${TagKey}` condition. We can use this condition to stop the attacker from creating new resources.
## A permissive CI/CD role

After doing the above:
- We have removed the attack surface associated with unused regions and services.
- We have limited the blast radius of a compromise to individual teams and services.
- We have made it difficult for attackers to abuse the CI/CD role by requiring correct tags on resources. Which the attacker is not able to figure out easily because the CI/CD role is not allowed to describe itself.
Now we can allow ourselves to have a permissive CI/CD role that is not a pain in the ass to create and maintain! Because the role will be difficult to abuse, and there is a ceiling to what it can actually do.
By “permissive”, I don’t necessarily mean “AdministratorAccess”. But you can give broad read & write permissions to all the services you use. Instead of picking out just the actions you need with a fine comb!

<Chunk>

# When to use Step Functions vs. doing it all in a Lambda function

I’m a big fan of AWS Step Functions. I use it to orchestrate all sorts of workflows, from payment processing to map-reduce jobs. Why it’s yet another AWS service you need to learn and pay for. And it introduces additional complexities, such as:
- It’s hard to test.
- Your business logic is split between configuration and code.
- New decision points. Such as whether to use Express Workflows or Standard Workflows.

So it’s fair to ask “Why should we even bother with Step Functions?” when you can do all the orchestration in code, inside a Lambda function. Let’s break it down.

<Chunk>
# When to use Step Functions vs. doing it all in a Lambda function

I’m a big fan of AWS Step Functions. I use it to orchestrate all sorts of workflows, from payment processing to map-reduce jobs. Why it’s yet another AWS service you need to learn and pay for. And it introduces additional complexities, such as:
- It’s hard to test.
- Your business logic is split between configuration and code.
- New decision points. Such as whether to use Express Workflows or Standard Workflows.

So it’s fair to ask “Why should we even bother with Step Functions?” when you can do all the orchestration in code, inside a Lambda function. Let’s break it down.
## Lambda pros

1. Doing all the orchestration in code is simpler. It’s more familiar. Everything you can do in Step Functions, you can do with just a few lines of code. Case in point:

```javascript
module.exports.handler = async (event) => {
    // error handling
    try {
        await doX()
    } catch (err) {
        // handle errors
    }
    // branching logic
    if (condition === true) {
        await doSomething()
    } else {
        await doSomethingElse()
    }
    // parallelism
    const promises = event.input.map(x => doY())
    const results = await Promise.all(promises)
    return results
}
```

2. It’s likely cheaper. A Step Functions state machine would likely use Lambda for its Task states. In which case, you’d end up paying for both:
- The Lambda invocations. There are multiple invocations instead of just one. However, the total billable execution time should be similar.
- Step Functions state transitions. At $25 per million state transitions, it’s one of the more expensive services in AWS. Paying for two services is likely more expensive than paying for just one.
3. It’s likely more scalable. When you use both Step Functions and Lambda functions (for the Task states) you are constrained by the throughput limits of both services. Step Functions Standard Workflows have modest limits on the no. of state transitions and the no. of executions you can start per second. Both of these limits can be raised. So with proper planning, they wouldn’t be an issue. Without Step Functions, you are limited only by the concurrent executions limit on Lambda. Similarly, Lambda has default throughput limits on the no. of concurrent executions. Again, with proper planning, and given the recent scaling changes for Lambda, you will be OK. Both the cost and scalability arguments are situational and depend on several architectural choices.

<Chunk>
 When you use both Step Functions and Lambda functions (for the Task states) you are constrained by the throughput limits of both services. Step Functions Standard Workflows have modest limits on the no. of state transitions and the no. of executions you can start per second. Both of these limits can be raised. So with proper planning, they wouldn’t be an issue. Without Step Functions, you are limited only by the concurrent executions limit on Lambda. Similarly, Lambda has default throughput limits on the no. of concurrent executions. Again, with proper planning, and given the recent scaling changes for Lambda, you will be OK. Both the cost and scalability arguments are situational and depend on several architectural choices.
## Lambda cons

1. Cold starts.
2. 15 mins max duration.
3. Not good for waiting. Because you pay for execution time by the milliseconds. It’s a poor solution when it comes to waiting for something to happen because all that execution time (and money) is wasted.
## Step Functions pros

1. The visual workflows make it very easy to debug problems. This is true even for non-technical team members, such as product owners and customer support teams.
2. Step Functions has a built-in audit history of everything that happened, including:
- When did a state start?
- What were the input and output?
- Error message and stack trace.
3. Step Functions have direct service integration with almost every AWS service. So it’s possible to implement an entire workflow without needing any Lambda functions.
4. No Lambda, no cold starts. No cold starts = more predictable performance.
5. Long execution time. A Standard Workflow can run for up to a year.
6. Callback patterns are a great way to support human decisions (e.g. approve a deployment request) in a workflow.
7. Standard Workflows are arguably the most cost-efficient way to wait. Because you don’t pay for the duration, only the state transition.
8. You can implement more robust error handling. This is important for business-critical workflows.

<Chunk>

# The one mistake everyone makes when using Kinesis with Lambda

<Chunk>
# The one mistake everyone makes when using Kinesis with Lambda

## AWS Kinesis and Lambda are a great combo for processing large amounts of data in real-time. However, there’s a common oversight that many developers make when integrating these two services.
There are established best practices for configuring Lambda’s EventSourceMapping for Kinesis:
- Configure an OnFailure destination for failed records.
- Enable BisectBatchOnFunctionError.
- Override MaximumRetryAttempts. Choose a value that gives failed messages a reasonable number of retries before sending them to the OnFailure destination.

These are great advice for building a resilient data processing pipeline and guarding against poison messages. But one crucial detail is missing: the payload captured in the OnFailure destination does not contain the event payload.

This is what will be captured in the OnFailure destination.

```json
{
  "requestContext": {
    "requestId": "cf6fa2a6-48e2-49a6-bc23-7ed2b154b176",
    "functionArn": "arn:aws:lambda:us-east-1:xxx:function:kinesis-processor",
    "condition": "RetryAttemptsExhausted",
    "approximateInvokeCount": 4
  },
  "responseContext": {
    "statusCode": 200,
    "executedVersion": "$LATEST",
    "functionError": "Unhandled"
  },
  "version": "1.0",
  "timestamp": "2023-12-10T22:06:23.446Z",
  "KinesisBatchInfo": {
    "shardId": "shardId-000000000000",
    "startSequenceNumber": "4964723877401875844914379084747495239034204468389137",
    "endSequenceNumber": "496472387740187584491437909411123017485931487851477729",
    "approximateArrivalOfFirstRecord": "2023-12-10T22:05:59.280Z",
    "approximateArrivalOfLastRecord": "2023-12-10T22:06:19.196Z",
    "batchSize": 3,
    "streamArn": "arn:aws:kinesis:us-east-1:xxx:stream/MyKinesisStream"
  }
}
```

The KinesisBatchInfo contains the shard ID, sequence numbers and batch size. The actual data payload — the heart of the event that you need for processing and analysis — is not included.

This omission can lead to significant issues:
1. Difficulty in debugging: Without the actual event data, pinpointing the cause of the failure becomes a complex task.
2. Data loss: If you are not able to fetch the record before it expires from the stream, then the data is lost forever. Default retention for a Kinesis stream is 24 hours. If the error occurs on a Saturday, likely, your developers won’t see it until they are back in the office on Monday. By then, the record is no longer available in the stream.
3. Increased complexity: Developers need to implement additional mechanisms to retrieve and store the original event data.

Most people don’t realise this and many have been caught off-guard when they encounter a failed message for the first time!

<Chunk>

# When to use API Gateway vs. Lambda Function URLs

"In 2024, if you want to build a REST API using serverless technologies. Should you use Lambda Function URLs or API Gateway?"
Here are some trade-offs you should consider when making this decision.
## Function URL pros
1. It works naturally with Lambdaliths.
2. No latency overhead from API Gateway.
3. No API Gateway-related costs.
4. It supports response streaming. This is useful for returning large payloads in the HTTP response.
5. There are fewer things to configure.
6. Your API can run for up to 15 mins.
## Function URL cons
1. You have to use Lambdaliths and deal with all the shortcomings that come with that.
2. No per-endpoint metrics. It’s possible to make up for this by implementing custom metrics with embedded metric format (EMF).
3. No direct integration with WAF. Although this is possible through CloudFront, the original function URL would still be unprotected.
4. It only supports AWS_IAM auth.
5. You cannot configure different auth methods per endpoint.
### Where Function URL makes sense
If you want to (i.e. not forced into it by the choice to use Function URL) build a Lambdalith and you don’t need any of the additional features that API Gateway offers. Then Function URLs make sense - it’s cheaper, faster and has fewer moving parts.
Similarly, if you need to return a large payload (> 10MB) or to run for more than 29s, then Function URL can also make sense. That is if you can’t refactor the client-server interaction.
Given the limited support for authentication & authorization, it’s not suitable for user-facing APIs. These APIs often require a Cognito authorizer or a custom Lambda authorizer.
This leaves function URLs as best suited for public APIs or internal APIs inside a microservices architecture.
By “internal API”, I refer to APIs that are used by other services but not by the frontend application directly. These APIs usually require AWS_IAM auth because the caller is another AWS resource — a Lambda function, an EC2 instance, an ECS task, etc.

<Chunk>
 That is if you can’t refactor the client-server interaction.
Given the limited support for authentication & authorization, it’s not suitable for user-facing APIs. These APIs often require a Cognito authorizer or a custom Lambda authorizer.
This leaves function URLs as best suited for public APIs or internal APIs inside a microservices architecture.
By “internal API”, I refer to APIs that are used by other services but not by the frontend application directly. These APIs usually require AWS_IAM auth because the caller is another AWS resource — a Lambda function, an EC2 instance, an ECS task, etc.
## API Gateway pros
1. It’s more flexible. It works with both Lambdaliths and the one function per endpoint approach.
2. It has direct integration with most AWS services. So in many cases, you don’t even need a Lambda function.
3. It can proxy requests to any HTTP APIs. This is very useful for integrating with 3rd party APIs.
4. It offers lots more features, including (but limited to) the following:
   - cognito authorizer
   - usage plans (great for SAAS applications that offer tiered pricing)
   - built-in request validation with request models
   - detailed per-endpoint metrics
   - mock endpoints (useful for endpoints that return static data)
   - request and response transformation (also useful for integrating with 3rd party APIs)
   - WebSockets support
   - and lots more…

<Chunk>
 It works with both Lambdaliths and the one function per endpoint approach.
2. It has direct integration with most AWS services. So in many cases, you don’t even need a Lambda function.
3. It can proxy requests to any HTTP APIs. This is very useful for integrating with 3rd party APIs.
4. It offers lots more features, including (but limited to) the following:
   - cognito authorizer
   - usage plans (great for SAAS applications that offer tiered pricing)
   - built-in request validation with request models
   - detailed per-endpoint metrics
   - mock endpoints (useful for endpoints that return static data)
   - request and response transformation (also useful for integrating with 3rd party APIs)
   - WebSockets support
   - and lots more…
## API Gateway cons
1. Additional latency overhead.
2. Additional cost.
3. No response streaming.
4. 29s integration limit.
5. 10MB response limit.
## When API Gateway makes sense
Given the vast array of features that API Gateway offers, it makes sense in most cases if you’re OK with the additional cost that comes with the convenience. The 29s and 10MB response limits can be problematic in some cases. But they can be mitigated with patterns such as Decoupled Invocations and S3 presigned URLs. However, these workarounds require you to refactor the client-server interaction, so they are not always viable.

<Chunk>

# Empowering Enterprises with Serverless Generative AI: Amazon Bedrock
Matt Carey, aleios

As the number of Large Language Models (LLMs) continues to grow and enterprises seek to leverage their advantages, the practical difficulties of running multiple LLMs in production is becoming evident. Established hyper-scale cloud providers, such as AWS, are in a favourable position to facilitate the adoption of Generative AI, due to their existing computing infrastructure, established security measures, and modern cloud-native patterns like Serverless.

AWS’s introduction of Bedrock stands out as a poignant reaction to these trends and is well positioned through its platform centric, model agnostic and serverless operating model to be the facilitator for this next era of GenAI. Last week, Bedrock became Generally Available (GA), giving AWS customers their first look at the tools which allow them to integrate the GenAI into all aspects of their operations.
## Serverless Advantage
Infrastructure management of LLMs at high scale, especially for those not entrenched in the ML/AI domain, can be daunting. Managing compute, load balancers and exposed APIs requires platform teams and few businesses are willing to make that up front investment. Amazon Bedrock alleviates these concerns. As a serverless service, businesses only pay for the tokens consumed and generated by the LLM. These scaling challenges become things of the past. For instance, you are building a customer support chatbot: It has 100x the users on Black Friday compared to a Monday in March. You do not have to provision extra servers. Bedrock handles the scale out and back in to meet demand.

<Chunk>
## Serverless Advantage
Infrastructure management of LLMs at high scale, especially for those not entrenched in the ML/AI domain, can be daunting. Managing compute, load balancers and exposed APIs requires platform teams and few businesses are willing to make that up front investment. Amazon Bedrock alleviates these concerns. As a serverless service, businesses only pay for the tokens consumed and generated by the LLM. These scaling challenges become things of the past. For instance, you are building a customer support chatbot: It has 100x the users on Black Friday compared to a Monday in March. You do not have to provision extra servers. Bedrock handles the scale out and back in to meet demand.
## Data Security
With an increasing emphasis on ensuring good data governance and audit trails, Bedrock provides peace of mind for enterprises seeking to adopt GenAI. All data provided to the Bedrock LLMs is encrypted at both rest and in transit and customers are free to use their own keys. Amazon Bedrock has achieved HIPAA eligibility and GDPR compliance and provided data is never used to improve the base models or shared with third-party model providers. Enterprises can even use AWS PrivateLink with Amazon Bedrock to establish private connectivity between LLMs and their VPC’s to avoid exposing traffic to the public internet. This gives businesses the security to create tools using LLMs that can use their own sensitive data archives as context.

<Chunk>
 All data provided to the Bedrock LLMs is encrypted at both rest and in transit and customers are free to use their own keys. Amazon Bedrock has achieved HIPAA eligibility and GDPR compliance and provided data is never used to improve the base models or shared with third-party model providers. Enterprises can even use AWS PrivateLink with Amazon Bedrock to establish private connectivity between LLMs and their VPC’s to avoid exposing traffic to the public internet. This gives businesses the security to create tools using LLMs that can use their own sensitive data archives as context.
## Features
Amazon Bedrock’s offering designed to be a full suite of tools to empower builders:
### Easy Model Selection
Bedrock supports a selection of both proprietary and open-source models including Amazon’s new Titan models. Depending on the task at hand, whether it's long form text generation, quick summarisation, or back and forth conversation, you will be able to find a model which meets your use-case. Bedrock also offers a playground, allowing teams to try out various models and work out the best prompts for their chosen model. The confirmed addition of Meta’s Llama 2 model through a Serverless API is definitely a unique selling point of Bedrock. AWS recently partnered with Hugging Face and made a significant investment into their $235 million Series D funding round so it’s a safe bet to expect more open-source models to be included with Bedrock in the coming months.

<Chunk>
 Depending on the task at hand, whether it's long form text generation, quick summarisation, or back and forth conversation, you will be able to find a model which meets your use-case. Bedrock also offers a playground, allowing teams to try out various models and work out the best prompts for their chosen model. The confirmed addition of Meta’s Llama 2 model through a Serverless API is definitely a unique selling point of Bedrock. AWS recently partnered with Hugging Face and made a significant investment into their $235 million Series D funding round so it’s a safe bet to expect more open-source models to be included with Bedrock in the coming months.
### Agents
Autonomous agents for Bedrock are now in preview. Agents are capable of automating tasks normally done by humans; such as pitch deck creation, replying to emails or coding tasks. Companies like Canva and Adobe have already integrated GenAI to resize images, remove backgrounds and objects, and it won't be long before we can also incorporate external style guides as context for these creations. With just a selection of notes, these tools will be able to create slides, flyers and other materials. Code generation is also becoming easier with single shot accuracy possible for increasingly more complex use-cases. Progress in this area has been rocketing recently with AI code assistants, code generation through test driven development and autonomous code reviews becoming more common.


Although the output of agents may not be perfect, even at around 70% accuracy, it is a significant time saver for the human operator. The days of paying analysts substantial sums for mundane tasks like colour adjustments on slide decks may soon be seen as nostalgic.

<Chunk>
 With just a selection of notes, these tools will be able to create slides, flyers and other materials. Code generation is also becoming easier with single shot accuracy possible for increasingly more complex use-cases. Progress in this area has been rocketing recently with AI code assistants, code generation through test driven development and autonomous code reviews becoming more common.


Although the output of agents may not be perfect, even at around 70% accuracy, it is a significant time saver for the human operator. The days of paying analysts substantial sums for mundane tasks like colour adjustments on slide decks may soon be seen as nostalgic.
## Fine-Tuning
LLMs work well when they have been trained on the general context of the task and are able to rephrase it. If it has no knowledge of the underlying concept, for instance a particular part or process which only happens in your company, you may get better results from fine-tuning your own custom model. Users can fine-tune any of the Bedrock LLMs directly in the console, using data stored in S3.
## Bedrock over OpenAI?
Many of the arguments for using Bedrock are also applicable to OpenAI’s platform. Both have a choice of models, a Serverless cost per token pricing model and support fine-tuning in the console. However Bedrock supports models from a large variety of providers and has much clearer data governance policies. Bedrock also benefits from the huge AWS ecosystem, allowing for closer integrations with other services such as Lambda for compute, OpenSearch for vectors and S3 for object storage. Pricing is also in favour of Bedrock: 1k tokens using Claude 2 through Bedrock will cost $0.01102 for input and $0.03268 for output whereas the closest offering from OpenAI (GPT4 32k context) will cost $0.06 for input and $0.12 for output. There is a situation where an individual may opt for using OpenAI’s GPT models. If they are particularly invested in their prompts or are making use of function calling where the LLM returns strictly JSON then sticking with OpenAI could be a good option. Otherwise switching to Bedrock is straightforward, especially if your application uses a library like LangChain which has a drop-in replacement for Bedrock.

<Chunk>

# Serverless webhooks — Designing Lift

This article is part of a series on Lift, an open-source project that simplifies deploying serverless applications. As introduced in previous articles, Lift is a Serverless Framework plugin that simplifies deploying serverless applications. At some point, every application needs to interact with an ecosystem of 3rd party SaaS providers. Implementing a webhook HTTP endpoint in your application allows this ecosystem of external applications to notify you. Your application can then react to those notifications and perform tasks accordingly. As we approach the first beta planned in the next weeks, let’s deep dive about Lift’s 4th component, webhooks: How to deploy production-ready HTTP webhook endpoints. How we plan to simplify that with Lift.
## The naive approach
### Simple webhook endpoint architecture

API Gateway is the go-to solution to easily expose HTTP endpoints in your Serverless application. API Gateway is an AWS managed service handling authorization, routing and input validation. You can natively integrate Lambda with API Gateway. Doing so, your code will be triggered every time an HTTP request hits a specific path of your application. To deploy a webhook endpoint with the Serverless Framework, it’s as easy as a few lines of configuration:

```yaml
<Chunk>
### Simple webhook endpoint architecture

API Gateway is the go-to solution to easily expose HTTP endpoints in your Serverless application. API Gateway is an AWS managed service handling authorization, routing and input validation. You can natively integrate Lambda with API Gateway. Doing so, your code will be triggered every time an HTTP request hits a specific path of your application. To deploy a webhook endpoint with the Serverless Framework, it’s as easy as a few lines of configuration:

```yaml# serverless.yml# ...
functions:
  webhook:
    handler: webhook.handler
    events:
      # Bind our function to /webhook path for POST requests
      - httpApi: 'POST /webhook'
```

<Chunk>
# ...
functions:
  webhook:
    handler: webhook.handler
    events:
      # Bind our function to /webhook path for POST requests
      - httpApi: 'POST /webhook'
```
# serverless.yml### Simple webhook endpoint architecture

API Gateway is the go-to solution to easily expose HTTP endpoints in your Serverless application. API Gateway is an AWS managed service handling authorization, routing and input validation. You can natively integrate Lambda with API Gateway. Doing so, your code will be triggered every time an HTTP request hits a specific path of your application. To deploy a webhook endpoint with the Serverless Framework, it’s as easy as a few lines of configuration:

```yaml## The missing characteristics of a good webhook design

The naive approach, while working, lacks some important criterion to maximize its efficiency. 3rd party apps notifiers expect your webhook endpoints to be available, respond fast and acknowledge all notifications. In order to do so, you should keep in mind: Transport / Processing separation of concerns: Webhooks are an inter-system notification medium based on HTTP transport. Acknowledging messages (authenticating, validating and parsing) and processing them are two different tasks: they should be done separately. As long as the message is acknowledged, our API should always return a 200 response. The response code should not depend on the ability of the application to use the notification content to perform some actions. Asynchronous decoupling: Webhooks are by nature an asynchronous notification protocol. A good webhook design should not put excessive stress on your application when the quantity of notification increases. A decoupling service (like a queue or a bus) should be implemented between the acknowledging and the processing workloads. Cost optimized: Webhook endpoints absorbs a various range of notification types. Your application might not want to actually do something for all of those types. It is therefore a good practice to ensure all of your infrastructure is not invoked for all incoming webhooks. Doing so, you’ll reduce risk of Denial of Wallet and be able to implement easily application filtering to process only the notification types you want. Error management: Processing error should not bubble up back to the notifier thanks to Transport/Processing separation of concern. However, some mechanism should be in place to ensure unprocessable messages are not lost forever. Monitoring: You should be notified of excessive amount of notification you cannot authorize. This could either be a malicious attack on your endpoint or a wrong configuration of the secret used to check notification signature.

<Chunk>
 It is therefore a good practice to ensure all of your infrastructure is not invoked for all incoming webhooks. Doing so, you’ll reduce risk of Denial of Wallet and be able to implement easily application filtering to process only the notification types you want. Error management: Processing error should not bubble up back to the notifier thanks to Transport/Processing separation of concern. However, some mechanism should be in place to ensure unprocessable messages are not lost forever. Monitoring: You should be notified of excessive amount of notification you cannot authorize. This could either be a malicious attack on your endpoint or a wrong configuration of the secret used to check notification signature.
## A production-ready approach
### Production-ready serverless webhook endpoint architecture

Here is a preview of a minimal `serverless.yml` configuration that includes those best practices:

```yaml# serverless.yml# ...
provider:
  eventBridge:
    useCloudFormation: true# You can use native Serverless framework functionalities to define your processors

<Chunk>

# Building a Massively Scalable Serverless Chat Application with AWS AppSync
## Introduction
When demand strikes, developers are tasked with building features quickly. My team knew this all too well when we were set to build a highly scalable chat app in just 4 weeks, when demand for our client’s video conferencing service went through the roof at the beginning of the first Covid-19 lockdown. Google Trends: Searches for ‘Video Conferencing’ in the UK. There was a significant increase in demand in March 2020.

When building the chat application discussed above we had to take into account the following requirements:
- Release to production quickly — their systems were failing to cope with the increased traffic levels
- Scale to 250,000 chat participants per video event
- Build the capability in their existing development team to maintain and further iterate the chat system

The chat application needed to have functionality where users could talk to each other in real-time. We used websockets rather than long-polling due to their cost efficiency and performance at scale (read more about the pros and cons of long-polling vs websockets).

The answer to all of the above was to use AWS AppSync, a fully-managed Serverless GraphQL interface to a number of different data sources — it offers speed of delivery, scale and ease of use. API Gateway websockets were also considered, but due to our requirements for mass broadcast of messages, it was not a viable option (there is no way to broadcast messages to all connected clients with one API call — you need an API call per connection). In addition, the GraphQL interface of AppSync allows for rapid frontend development.

<Chunk>
 We used websockets rather than long-polling due to their cost efficiency and performance at scale (read more about the pros and cons of long-polling vs websockets).

The answer to all of the above was to use AWS AppSync, a fully-managed Serverless GraphQL interface to a number of different data sources — it offers speed of delivery, scale and ease of use. API Gateway websockets were also considered, but due to our requirements for mass broadcast of messages, it was not a viable option (there is no way to broadcast messages to all connected clients with one API call — you need an API call per connection). In addition, the GraphQL interface of AppSync allows for rapid frontend development.
## Architecture
This simple architecture enabled us to have a high speed of delivery for our chat application while also have a clear set of technologies that would need to be taught to the existing team.

The architecture is based around AppSync, which integrates with Cognito user pools for our authentication. DynamoDB is also leveraged as our data source (read about DynamoDB in the next section).

All of this hooks up to our React frontend which uses the Apollo Client to fire our requests to the GraphQL server from the frontend. Note, the Serverless Framework was used to manage the IaC (infrastructure as code), while amplify simply provided the SDK for frontend interaction with backend deployed services.

<Chunk>
## Architecture
This simple architecture enabled us to have a high speed of delivery for our chat application while also have a clear set of technologies that would need to be taught to the existing team.

The architecture is based around AppSync, which integrates with Cognito user pools for our authentication. DynamoDB is also leveraged as our data source (read about DynamoDB in the next section).

All of this hooks up to our React frontend which uses the Apollo Client to fire our requests to the GraphQL server from the frontend. Note, the Serverless Framework was used to manage the IaC (infrastructure as code), while amplify simply provided the SDK for frontend interaction with backend deployed services.
## DynamoDB
Being a NoSQL Serverless Database, DynamoDB provides a fully scalable solution to our needs without the need to manage servers. It was built for enormous, high velocity use cases and big companies such as AirBnb and Samsung use the service, so we’re in good company! In addition DynamoDB streams came in use later on for further analytics integrations.

The fact that AppSync integrates with DynamoDB is ideal as it means we need to do minimal set-up and therefore increase our speed of delivery. 

```
DynamoDB Schema for basic Serverless chat application
```

<Chunk>
## DynamoDB
Being a NoSQL Serverless Database, DynamoDB provides a fully scalable solution to our needs without the need to manage servers. It was built for enormous, high velocity use cases and big companies such as AirBnb and Samsung use the service, so we’re in good company! In addition DynamoDB streams came in use later on for further analytics integrations.

The fact that AppSync integrates with DynamoDB is ideal as it means we need to do minimal set-up and therefore increase our speed of delivery. 

```
DynamoDB Schema for basic Serverless chat application
```
## GraphQL Explained
With REST we need to call separate endpoints to get the information you require. With GraphQL we need only call the same GraphQL endpoint.

In a nutshell, GraphQL is an alternative way to connect your client applications to your backend. It was developed at Facebook, to improve bandwidth efficiency since mobile devices don’t always have a good internet connection. Most of us are more familiar with REST, so let’s compare it to GraphQL to get a better understanding.

When using GraphQL we simply call one endpoint rather than having separate defined endpoints as in REST.
## Queries
Queries in GraphQL are analogous to GET requests in REST. Queries are a way of fetching data (in our case we are fetching from DynamoDB as our database so we’ll use this as our example going forward).

```
See how we define a query above to get messages. We pass in a roomId as an argument which we can use in our query to only get messages for that specific chat room. We then specify that the return type is an array of messages, where ‘Message’ is a type.
```

<Chunk>
 We then specify that the return type is an array of messages, where ‘Message’ is a type.
```
## GraphQL Explained
With REST we need to call separate endpoints to get the information you require. With GraphQL we need only call the same GraphQL endpoint.

In a nutshell, GraphQL is an alternative way to connect your client applications to your backend. It was developed at Facebook, to improve bandwidth efficiency since mobile devices don’t always have a good internet connection. Most of us are more familiar with REST, so let’s compare it to GraphQL to get a better understanding.

When using GraphQL we simply call one endpoint rather than having separate defined endpoints as in REST.
## Mutations
Mutations in GraphQL are similar to POST requests in REST. Mutations do what they say, they are way of mutating the data. 

```
See above how we set up a sendMessage mutation — we pass in the roomId and message as arguments so that we can add it with that information as an item to our DynamoDB table. The return type is a Message.
```
## Subscriptions
AppSync subscriptions, which allow for real-time updates, are where the fun begins. For our chat application, we want to ‘subscribe’ to the mutation event ‘sendMessage’ so that the frontend application updates when a message is sent with the new message to all users. 

```
You can see here that we pass in the roomId which means that only users in that particular chat room will be listening for messages sent.
```

<Chunk>
 For our chat application, we want to ‘subscribe’ to the mutation event ‘sendMessage’ so that the frontend application updates when a message is sent with the new message to all users. 

```
You can see here that we pass in the roomId which means that only users in that particular chat room will be listening for messages sent.
```
## Mutations
Mutations in GraphQL are similar to POST requests in REST. Mutations do what they say, they are way of mutating the data. 

```
See above how we set up a sendMessage mutation — we pass in the roomId and message as arguments so that we can add it with that information as an item to our DynamoDB table. The return type is a Message.
```
## Resolvers
We use Apache VTL for our business logic. In order to communicate with our data source we need to use resolvers — this is the connection between GraphQL and our data source. The resolvers are written in Apache Velocity Template Language (VTL) which takes the request as an input and outputs a JSON document.
## Pipeline Resolvers
Sometimes we may need to perform multiple operations to resolve the GraphQL field — in this case we use a great feature of AppSync called pipeline resolvers. Pipeline resolvers consist of a ‘before’ mapping template, followed by a number of functions, finishing with an ‘after’ mapping template.

```
As an example we use a pipeline resolver for starting a thread on a question. To make use of single table design we have ‘MESSAGE’ rows and ‘THREAD’ rows. When a person replies in a thread, we need to add a THREAD row and mutate the MESSAGE row to add a ‘repliedAt’ timestamp. This requires 2 separate requests. 
```

<Chunk>
## Pipeline Resolvers
Sometimes we may need to perform multiple operations to resolve the GraphQL field — in this case we use a great feature of AppSync called pipeline resolvers. Pipeline resolvers consist of a ‘before’ mapping template, followed by a number of functions, finishing with an ‘after’ mapping template.

```
As an example we use a pipeline resolver for starting a thread on a question. To make use of single table design we have ‘MESSAGE’ rows and ‘THREAD’ rows. When a person replies in a thread, we need to add a THREAD row and mutate the MESSAGE row to add a ‘repliedAt’ timestamp. This requires 2 separate requests. 
```
## Frontend — Amplify & Apollo Client
To authenticate our users we use the AWS Amplify SDK with Cognito. Amplify provides pre-built UI components to cater for sign-in and sign-out. We configure the authentication by using ‘Auth’ from aws-amplify and using our user pool information (from the Cognito user pool deployed by the Serverless Framework). To make the requests from the frontend we use the Apollo Client- it is our favourite tool for interacting with our backend as it is compatible with TypeScript, providing a great developer experience, and it manages state in the frontend using a caching mechanism.

<Chunk>

# PDF Visual Regression Testing — 'The PuppetMaster' Approach

Test automation is the only way to ensure quality in a CI (continuous integration) environment. Testing backend logic and web frontends, although time consuming and difficult to ensure accuracy, is well documented and best practices exist. PDF generation a feature often involved in consumer facing products (e.g. booking confirmation, data analysis, personalised planning documents). Yet, although often a mission critical feature, testing the generated PDFs is generally overlooked. We experimented with a new approach to PDF testing by combining Puppeteer (a way to manipulate headless Chrome), the jest JavaScript testing framework and a visual regression jest extension from American Express. The basic premise of visual regression testing is comparing known good snapshots of an interface with ones generated after a change to the underlying code. With builds failing if a difference of pixels is detected above some threshold.

To do this we need:
1. A way to convert a PDF page to an image snapshots that can be used by an image comparison tool. The PDF format is not good for comparison and does not work with existing image diff tools. Puppeteer will be able to generate a PNG of each page of the PDF.
2. A test running framework (jest).
3. A tool to compare the generated snapshots to new snapshots. (jest-image-snapshot).

<Chunk>
 The basic premise of visual regression testing is comparing known good snapshots of an interface with ones generated after a change to the underlying code. With builds failing if a difference of pixels is detected above some threshold.

To do this we need:
1. A way to convert a PDF page to an image snapshots that can be used by an image comparison tool. The PDF format is not good for comparison and does not work with existing image diff tools. Puppeteer will be able to generate a PNG of each page of the PDF.
2. A test running framework (jest).
3. A tool to compare the generated snapshots to new snapshots. (jest-image-snapshot).
## Generating snapshot images from a PDF

Puppeteer is a headless instance of chrome. It can be used to generate visual snapshots of websites to use for image comparison via a simple `screenshot` function.

```javascript
// Example of using Puppeteer's screenshot function
```

Chrome has a built in PDF viewer, and Puppeteer can manipulate chrome to open a url or a file. So Puppeteer can open a PDF file and then be used to generate a screenshot of the page. From the example above we can change line 6 to use the file protocol to open a local PDF file.

One issue with this approach is the scrollbar on the right hand side of the viewer. Due to the way chrome loads its scrollbar it is good to remove this from the snapshot as it can add flakiness and timing issues to the tests. Luckily Puppeteer can take a `clip` parameter to its screenshot function to crop the screenshot. For instance:

```javascript
// Example of using Puppeteer's clip parameter
```

<Chunk>
 So Puppeteer can open a PDF file and then be used to generate a screenshot of the page. From the example above we can change line 6 to use the file protocol to open a local PDF file.

One issue with this approach is the scrollbar on the right hand side of the viewer. Due to the way chrome loads its scrollbar it is good to remove this from the snapshot as it can add flakiness and timing issues to the tests. Luckily Puppeteer can take a `clip` parameter to its screenshot function to crop the screenshot. For instance:

```javascript
// Example of using Puppeteer's clip parameter
```
## In jest

Jest is a delightful JavaScript Testing Framework with a focus on simplicity. Adding jest to an existing JS project is as simple as:

```bash
// Commands to add jest
```

Our test template will then be as follows:

```javascript
describe('pdf suite', () => {
   it('works', async () => {
      expect(1).toBe(1);
   });
});
```

American express have a great extension to jest providing image comparison for visual regression testing. It can be added as follows:

```bash
yarn add -D jest-image-snapshot
npm i --save-dev jest-image-snapshot
```

Jest’s inbuilt expect object can then be expanded as follows:

```javascript
const { toMatchImageSnapshot } = require('jest-image-snapshot');
expect.extend({ toMatchImageSnapshot });
```

<Chunk>

# Enabling the Optimal Serverless Platform Team — CDK and Team Topologies

Serverless, and related technologies, have enabled teams to move faster, reduce total cost of ownership and overall empowered developers to have greater ownership of the systems they build. However, Serverless is not a silver bullet — there is an organisational side that’s key to unlock the full benefits of Cloud.
When it comes to team structure as we adopt more Cloud-Native technologies, like Serverless, there are 2 key areas that need to be rethought:
- The top-down approach to security bottlenecking developers
- Overwhelming cognitive load

As discussed above, the solution is not purely technical — but CDK can enable the socio-technical change needed.
## Rethinking Security

In many large organisations that move to adopt Serverless, security is typically seen as a top-down bottleneck that slows development and innovation. At the same time security incidents have been rising for years, and a strong cyber security posture requires organisations to work together, not just throwing things over the wall to InfoSec.
### Security as “Top Down” Blocker

As companies have started to adopt Cloud and DevOps developers have been brought closer to security, yet a wall still exists in this collaboration. With Serverless the application code and the cloud are inextricably linked, meaning developers have moved further into the realm of the security. This typically creates some turf wars in the initial stages of a transformation to adopt Serverless, but also provides the opportunity for us to rethink the traditional security approach.

Ideally Security is seen as an enabling function, with tools, approaches and teams enabling a secure by default paradigm. This requires us to turn our view of security on its head and have development teams be enabled by security rather than held hostage from deploying or innovating.

This requires a change in engineering team structure, as well as a technical solution to enable a bottom-up secure by design paradigm. The combination of “Team Topologies” and CDK can be leveraged to achieve this change.

<Chunk>
 This typically creates some turf wars in the initial stages of a transformation to adopt Serverless, but also provides the opportunity for us to rethink the traditional security approach.

Ideally Security is seen as an enabling function, with tools, approaches and teams enabling a secure by default paradigm. This requires us to turn our view of security on its head and have development teams be enabled by security rather than held hostage from deploying or innovating.

This requires a change in engineering team structure, as well as a technical solution to enable a bottom-up secure by design paradigm. The combination of “Team Topologies” and CDK can be leveraged to achieve this change.
## Cognitive Load

Cognitive load is a form of friction on teams — the greater the cognitive load the slower their velocity (and often the lower their satisfaction). One key way to reduce cognitive load is to design our architecture and systems on the underlying domain and building services based on independent subsections of the domain to enable independence of teams and services. This allows teams to restrict their scope to a subset of the overall domain complexity.

<Chunk>
 This requires us to turn our view of security on its head and have development teams be enabled by security rather than held hostage from deploying or innovating.

This requires a change in engineering team structure, as well as a technical solution to enable a bottom-up secure by design paradigm. The combination of “Team Topologies” and CDK can be leveraged to achieve this change.
### Cognitive Load: The amount of working memory being used

As cloud and application code move closer together we’re leveraging smaller cross-functional teams to a greater and greater degree. This has many advantages, with smaller teams building at a higher velocity and with complete autonomy. At the same time, if we don’t design our architectures and organisational structures correctly it can create such a large cognitive load on the teams involved that they struggle to deliver and it becomes extremely difficult to onboard new developers.

Inside team topologies there are 4 types of team:
1. Stream-Aligned Team: Ideally split into segments of the Business Domain (e.g. Bounded Contexts) and delivering a flow of work.
2. Enabling Team: Assist Stream-Aligned teams, remove obstacles and detect lacking capabilities.
3. Complicated Subsystem team: Domain specific complex expertise (e.g. algorithmic, machine learning)
4. Platform Team: Accelerate the Stream-Aligned teams via a product based approach while maintaining their autonomy.

In combination with Domain-Driven Design techniques Stream-Aligned Teams can work on independent areas of the domain with services architected accordingly (see EventBridge Storming).

Platform Teams can greatly reduce the cognitive load placed on Stream-Aligned teams by providing foundational services that no longer need to be created by the Stream-Aligned teams. These Platform teams should see themselves as service providers, with the Stream-Aligned teams as their “customers”.
## CDK — The Enabling Technology

Key to enable the platform team is the need for:
- Abstraction
- Encapsulation
- Composition

These components would enable the Platform team to hide complexity via abstraction, provide protected security defaults via encapsulation and allow Stream-Aligned Teams to self-compose components into their sub-domain specific use cases.

<Chunk>
 Complicated Subsystem team: Domain specific complex expertise (e.g. algorithmic, machine learning)
4. Platform Team: Accelerate the Stream-Aligned teams via a product based approach while maintaining their autonomy.

In combination with Domain-Driven Design techniques Stream-Aligned Teams can work on independent areas of the domain with services architected accordingly (see EventBridge Storming).

Platform Teams can greatly reduce the cognitive load placed on Stream-Aligned teams by providing foundational services that no longer need to be created by the Stream-Aligned teams. These Platform teams should see themselves as service providers, with the Stream-Aligned teams as their “customers”.
### What is CDK?

The AWS Cloud Development Kit (AWS CDK) is an open-source software development framework to define your cloud application resources using familiar programming languages.

AWS CDK allows developers to use their programming language of choice, e.g. TypeScript, to define their architecture. It allows developers to use the powerful expressive characteristics of modern programming languages, including concepts like Abstraction, Encapsulation & Composition.

Under the hood CDK synthesises familiar CloudFormation, providing a repeatable deployment and compatibility with existing deployment tooling and practices.

<Chunk>
### What is CDK?

The AWS Cloud Development Kit (AWS CDK) is an open-source software development framework to define your cloud application resources using familiar programming languages.

AWS CDK allows developers to use their programming language of choice, e.g. TypeScript, to define their architecture. It allows developers to use the powerful expressive characteristics of modern programming languages, including concepts like Abstraction, Encapsulation & Composition.

Under the hood CDK synthesises familiar CloudFormation, providing a repeatable deployment and compatibility with existing deployment tooling and practices.
#### The Construct Hierarchy

CDK Constructs represent the basic building blocks of AWS architectures. They contain everything needed to generate the CloudFormation needed to deploy a part of the architecture.

There are 3 levels of construct, each building on each other:
- Level 1: AWS CloudFormation-Only
- Level 2: Curated
- Level 3: Patterns

```plaintext
L1 constructs are generated directly from CloudFormation, and therefore represent a feature complete interface to generate resources at the lowest level of abstraction.
const bucket = new s3.CfnBucket(this, "MyBucket", {
  bucketName: "MyBucket"
});

L2 constructs build on L1, adding some default values, boilerplate and glue.
import * as s3 from 'aws-cdk-lib/aws-s3';
// "this" is HelloCdkStack
new s3.Bucket(this, 'MyFirstBucket', {
  versioned: true
});
```

And finally L3 constructs are patterns for common use cases, e.g. a REST API, often composing multiple types of resources into an end-to-end pattern.

<Chunk>

# Building a Robust Serverless Messaging Service with Amazon EventBridge Pipes and CDK

EventBridge Pipes is a powerful new tool from Amazon Web Services (AWS) that makes it easy to move data between sources and targets while filtering, enriching and even transforming the data en route. EventBridge used to be a singular product, the EventBridge Bus. However, in the last few months, AWS has expanded it into a complete product suite for building event-driven applications, including; EventBuses, Pipes and Schedulers.

High-scale messaging systems are traditionally complex to build. They must be able to handle a high volume of messages concurrently whilst ensuring that messages are not lost due to failures. In addition, they often require complex features such as routing, filtering, and transformation of messages, which can be challenging to implement and maintain. Pipes solves these problems by providing industry-leading horizontal scaling, redundancy with dead letter queues and inbuilt transformations, filtering and enrichment capabilities.

Using the Cloud Development Kit (CDK), we can build and deploy our messaging service without leaving our chosen development environment (in Typescript, too! Bye-bye AWS console and bye-bye writing YAML).

<Chunk>
 They must be able to handle a high volume of messages concurrently whilst ensuring that messages are not lost due to failures. In addition, they often require complex features such as routing, filtering, and transformation of messages, which can be challenging to implement and maintain. Pipes solves these problems by providing industry-leading horizontal scaling, redundancy with dead letter queues and inbuilt transformations, filtering and enrichment capabilities.

Using the Cloud Development Kit (CDK), we can build and deploy our messaging service without leaving our chosen development environment (in Typescript, too! Bye-bye AWS console and bye-bye writing YAML).
## Example Application

We are building a web app with a fully serverless backend. We need a microservice that sends an email to the user whenever the user changes something security-related on their account, such as an address. This account data is stored in a DynamoDB table. The messaging service should construct and send an email to the user with identifying information about them and the change that has occurred.

```
ToAddresses: [userEmail]
Subject: `Notice: ${modifiedAttribute} Change Successful`
Body: `Dear ${firstName} ${lastName},

This is confirmation that your ${modifiedAttribute} for your account associated 
If this is a change you made, we're good to go!
 
If you did not personally request this change, please reset your password or con
```

The catch is that the DynamoDB table the user has modified does not store names or email addresses, only a Cognito sub (user id). The Cognito User Pool is used as a single source of truth for security reasons and GDPR compliance. Therefore, we must query the rest of the information, such as name and email address, from Cognito directly.

<Chunk>
 The Cognito User Pool is used as a single source of truth for security reasons and GDPR compliance. Therefore, we must query the rest of the information, such as name and email address, from Cognito directly.
## What are Pipes?

AWS released Pipes on 1st December 2022. It makes life easier for developers to “create point-to-point integrations between event producers and consumers” while reducing the need for writing integration code. Essentially a Pipe automatically routes events from source to target, reducing the amount of extra code to write when building event-driven applications.
## How are EventBridge Pipes different from EventBridge Buses?

Event buses are like post offices for AWS services. Different resources can send and receive messages or “events” to each other through this central service. Think of it as a middleman using forwarding rules to direct messages to the target services. 

EventBridge Pipes, on the other hand, passes events from one AWS service directly to another. They allow you to connect an event source to a target service with no extra glue code needed. In addition, you can connect a Lambda function or other AWS service to manipulate or “enrich” the events before they reach the target.
## Why use Pipes?

The flexibility of Pipes makes it perfect for our messaging service. We have an event source needing to send an event to another AWS service. The event needs extra information fetching and adding somewhere along the flow. With Pipes, we can plug together each piece of infrastructure with minimal extra code. 

<Chunk>
 With Pipes, we can plug together each piece of infrastructure with minimal extra code. 
## How are EventBridge Pipes different from EventBridge Buses?

Event buses are like post offices for AWS services. Different resources can send and receive messages or “events” to each other through this central service. Think of it as a middleman using forwarding rules to direct messages to the target services. 

EventBridge Pipes, on the other hand, passes events from one AWS service directly to another. They allow you to connect an event source to a target service with no extra glue code needed. In addition, you can connect a Lambda function or other AWS service to manipulate or “enrich” the events before they reach the target.
## Building the Architecture

To build the Pipe, we must first choose a source producing the event. In our case, this is the DynamoDB stream of the table.
### DynamoDB Streams

DynamoDB Streams is a feature of Amazon DynamoDB that allows you to capture changes to the data in a DynamoDB table in near real-time. 
## Infrastructure as Code (IaC)

For this project, we are using CDK for IaC. CDK allows us to write IaC in imperative programming languages such as TypeScript or Python rather than declarative ones such as YAML or JSON. 

<Chunk>
## Infrastructure as Code (IaC)

For this project, we are using CDK for IaC. CDK allows us to write IaC in imperative programming languages such as TypeScript or Python rather than declarative ones such as YAML or JSON. 
### DynamoDB Streams

DynamoDB Streams is a feature of Amazon DynamoDB that allows you to capture changes to the data in a DynamoDB table in near real-time. 
## Building the Architecture

To build the Pipe, we must first choose a source producing the event. In our case, this is the DynamoDB stream of the table.
### DynamoDB

The table is created as shown below. 

```typescript
const sourceTable = new Table(this, 'example-table-id', {
  tableName: 'example-table',
  partitionKey: { name: 'PK', type: AttributeType.STRING },
  sortKey: { name: 'SK', type: AttributeType.STRING },
  stream: StreamViewType.NEW_AND_OLD_IMAGES,
  billingMode: BillingMode.PAY_PER_REQUEST,
  removalPolicy: RemovalPolicy.DESTROY,
  pointInTimeRecovery: true,
});
```
### Lambdas

The code for the enrichment Lambda handler function is found below.

```typescript
export const handler = async (event: DynamoDBStreamEvent): Promise<string> => {
  const record: DynamoDBRecord = event.Records[0];
  if (record.dynamodb?.NewImage == null && record.dynamodb?.OldImage == null) {
    throw new Error('No NewImage or OldImage found');
  }
  const modifiedAttributes: string[] = [];
  for (const key in record.dynamodb.NewImage) {
    if (record.dynamodb.NewImage[key].S !== record.dynamodb.OldImage?.[key].S) {
      modifiedAttributes.push(key);
    }
  }
  if (modifiedAttributes.length === 0) {
    throw new Error('No changed parameters found');
  }
  const userId = record.dynamodb.NewImage?.userId.S;
  if (userId == null) {
    throw new Error('No userId found');
  }
  const user = await getUser(userId);
  return JSON.stringify({
    ...user,
    modifiedAttributes,
  } as MessageBody);
};
```

<Chunk>

# Managing GenAI Risk: The Supply Chain

<Chunk>
# Managing GenAI Risk: The Supply Chain

## Introduction

Generative AI (GenAI) has captured the imagination of millions in just a few months since the release of high-profile applications like ChatGPT, DALL-E and Midjourney. After being stunned by the seemingly magical interaction with these early applications, many of us started to see the real-world uses in our work. My entire engineering team leverages Copilot (the AI pair programmer from GitHub powered by a GPT model), and we've built several productivity tools on top of our existing knowledge banks. Our clients have also been quick to react and ask for help in leveraging this technology. Right now, many use cases are for obvious customer support and information retrieval, but creative/knowledge work is already in the sight of many.

Almost as heavily publicised as the benefits of such technologies are the regulatory and privacy concerns. Even OpenAI, the company behind ChatGPT, has received attention from regulators across the globe. Other companies, including Goldman Sachs and JP Morgan, have also been quick to ban ChatGPT due to data security and intellectual property concerns. The US Federal Trade Commission (FTC) is now demanding disclosure on the data used to train ChatGPT’s underlying models in reaction to false accusations made by ChatGPT and alleged copyright infringement.

The core technology behind GenAI, Large Language Models (LLMs), are not inherently risky but require such a large volume of data and learning that can lead to issues when not handled well. When LLMs are trained on petabytes of public data, it's not inconceivable for intellectual property to be picked up, leading to complex questions about the ownership of generated materials. Moreover, these models are further trained on their interactions, which means that the content of "prompts" (questions asked by the user) can be "learned" by the model, potentially leading to leaks. For example, if I were working at a marketing firm and asked a free public GPT application for advice on how to handle the secret launch of a new soda by a large beverage provider, it's possible that an executive at a direct competitor could be asking the same application for new product ideas. As people begin to use GenAI applications to automatically take notes during all their business meetings, the situation becomes concerning.

Most of the work I’ve been involved in with GenAI has been in helping clients understand and mitigate risks. Simply put, risks exist throughout the supply chain of the GenAI application, from model training and foundational model selection through to prompt engineering, context, hosting, fine tuning, and integration. By understanding, mitigating, and documenting these risks, companies can leverage the power of GenAI while avoiding legal, societal, and reputational risks.

<Chunk>
 As people begin to use GenAI applications to automatically take notes during all their business meetings, the situation becomes concerning.

Most of the work I’ve been involved in with GenAI has been in helping clients understand and mitigate risks. Simply put, risks exist throughout the supply chain of the GenAI application, from model training and foundational model selection through to prompt engineering, context, hosting, fine tuning, and integration. By understanding, mitigating, and documenting these risks, companies can leverage the power of GenAI while avoiding legal, societal, and reputational risks.
## Foundational Model Training / Selection

LLMs are trained using deep learning techniques on extremely large datasets, much larger than those typically used to train previous deep learning models. The source of the data, as well as the licensing and guarantees on this data, are crucial in avoiding unintentional infringement of IP protections. For most companies, the use of GenAI will involve the use of a third-party Foundational Models (FMs). Due to the size and scale of the parameters involved in these large FMs, it is almost impossible to reference back to sources, leading to a grey area when it comes to defending against IP claims. To reuse the simple "garbage in ⇒ garbage out" analogy from the ML world, "IP in ⇒ IP risk out". The best way to mitigate this risk is to conduct due diligence on third-party FMs in use, or indeed, to establish a rigorous process for data validation if you are building a specialized FMs.

Apart from the IP risks, when training your own FM there is also a data security risk. For example, if you are a large financial institution that leverages an internal knowledge bank, you may decide to train a GPT-powered assistant to make your staff more efficient. The large corpus of data from your knowledge bank and CRM could be used as part of the training data. Then, an analyst may ask a query from a client and get access to data from or about another client, with potentially sensitive information. This would bypass the traditional "role-based access controls" that your other systems and tools have in place. Knowing the data's provenance, purpose, and permissions required to access it in the current organization is crucial to ensuring that a leaky model, even if only used internally, is not created.

<Chunk>
 For example, if you are a large financial institution that leverages an internal knowledge bank, you may decide to train a GPT-powered assistant to make your staff more efficient. The large corpus of data from your knowledge bank and CRM could be used as part of the training data. Then, an analyst may ask a query from a client and get access to data from or about another client, with potentially sensitive information. This would bypass the traditional "role-based access controls" that your other systems and tools have in place. Knowing the data's provenance, purpose, and permissions required to access it in the current organization is crucial to ensuring that a leaky model, even if only used internally, is not created.
## Fine Tuning

LLMs can be customized for a particular use case through a process called "Fine Tuning". In this approach, a copy of a trained model with known weights is taken and then trained again with a use-case-specific data set, starting from the initial weights. This process can be made more efficient by training a subset of weights in an approach called "Parameter Efficient Tuning". However, this process presents an opportunity for the model to be contaminated with IP-infringing data sets or sensitive information. Fortunately, the volume of data used for fine tuning is typically orders of magnitude lower than that used to train the FM. This means that auditing is simpler, and mistakes are less likely. Nonetheless, it is essential to have strong processes in place regarding the provenance, purpose, and permission level of data used for fine tuning, as well as to educate teams and clearly document any training for audit and accountability purposes.

<Chunk>
 This process can be made more efficient by training a subset of weights in an approach called "Parameter Efficient Tuning". However, this process presents an opportunity for the model to be contaminated with IP-infringing data sets or sensitive information. Fortunately, the volume of data used for fine tuning is typically orders of magnitude lower than that used to train the FM. This means that auditing is simpler, and mistakes are less likely. Nonetheless, it is essential to have strong processes in place regarding the provenance, purpose, and permission level of data used for fine tuning, as well as to educate teams and clearly document any training for audit and accountability purposes.
## Hosting

The location where a model resides during training and inference is crucial, especially when companies customize foundational models using high-value intellectual property. Without secure hosting, the intellectual property and sensitive information used to train the model could be at risk. Additionally, the prompts and outputs may contain sensitive information, and the roundtrip from the application layer to the model and back needs to be secure and compliant with data privacy laws. It is also important to have control over where models are trained and hosted to ensure compliance with data sovereignty laws. Mapping out the end-to-end flow of application prompts to outputs, the locations and security responsibilities of any outsourced components is key to understand your holistic hosting picture. Cloud providers have been quick to make hosting options available with regional segregation and security controls.

<Chunk>
 Additionally, the prompts and outputs may contain sensitive information, and the roundtrip from the application layer to the model and back needs to be secure and compliant with data privacy laws. It is also important to have control over where models are trained and hosted to ensure compliance with data sovereignty laws. Mapping out the end-to-end flow of application prompts to outputs, the locations and security responsibilities of any outsourced components is key to understand your holistic hosting picture. Cloud providers have been quick to make hosting options available with regional segregation and security controls.
## Application

When providing users with access to the outputs of LLMs, it is crucial to manage the risk of hallucination. An LLM may confidently and plausibly respond with an answer, even if it is false, leading to defamation lawsuits if individuals are falsely accused of crimes with fake links to non-existent news articles. This creates a risk of defamation and of false information impacting critical business processes. While researchers are working on creating new FM that are less susceptible to hallucinations, it is important to educate end users and incorporate UI features that remind users of the potential for false information. In addition, safeguards should be implemented to protect against malicious use of public-facing applications, such as the creation of fake news or inappropriate content (profanity, violence, hate speech, etc.). New FM with built-in protection, such as Amazon's Titan FMs, are also emerging.
## Context Embeddings

Vector embeddings enable LLMs to search for similar data. Vector embeddings are generated from application data and stored as vectors inside a database. A user's query can then be converted to an embedding, and a similarity search can be performed, providing a prompt to the LLM with formatting and further information.

<Chunk>

# Finally, the end of YAML? AWS CDK for Serverless
## The AWS Cloud Development Kit (CDK) 
The AWS Cloud Development Kit (CDK) is a framework that lets you define your Cloud Application resources using familiar programming languages such as TypeScript or Python. This allows developers familiar with these languages to apply their existing knowledge and get to grips with building Cloud infrastructure rapidly. By using provided high-level components, built with best practices as default, we can abstract much of the complexity away from the developer, and by encapsulating resources into constructs we can continue this practice as our resources scale in size and complexity.
## What is Infrastructure as Code?
Infrastructure as Code (IaC) is a discrete representation of all the various resources that our cloud application will use, in machine readable definition files. The main benefits this offers are:
- Version control via git
- Single source of truth
- Reusability — e.g. easy to deploy several production-like environments
- Typing — allowing suggestions and autocomplete
- Testing of the IaC
## Why CDK?
IaC is not a new concept, and there are many options to choose from, including Serverless Framework, Terraform, CloudFormation and AWS SAM. One of the unique features of CDK compared to these others is that it allows us to write IaC in imperative programming languages such as TypeScript and Python, rather than declarative ones such as YAML or JSON. The expressive power of these languages and their ubiquitous support by code editors, formatters, syntax checkers and linters is leagues ahead of support for any YAML-based IaC. This makes the development experience more approachable and rapid, as many errors can be caught by static checks performed by the editor. The code is also more easily readable and comprehensible to developers written in these familiar languages than in YAML.

This CDK code is transpiled into CloudFormation templates, AWS’s YAML or JSON IaC format. During this ‘synth’ process, further errors in the IaC can be caught — I like to think of these like compile-time errors in compiled languages like C or Rust. This means we don’t have to wait for a failed deployment to catch such errors, saving the developers time.

<Chunk>
 This makes the development experience more approachable and rapid, as many errors can be caught by static checks performed by the editor. The code is also more easily readable and comprehensible to developers written in these familiar languages than in YAML.

This CDK code is transpiled into CloudFormation templates, AWS’s YAML or JSON IaC format. During this ‘synth’ process, further errors in the IaC can be caught — I like to think of these like compile-time errors in compiled languages like C or Rust. This means we don’t have to wait for a failed deployment to catch such errors, saving the developers time.
## CDK deploy process
This is in contrast to Terraform’s API-based approach, where infrastructure is provisioned in a cloud provider agnostic language, and then plugins are used to interact with Cloud and SaaS providers. The benefit of this approach is migration between cloud providers does not require a complete rewrite of the IaC. However, with serverless architectures we’re likely already buying into cloud-provider-specific constructs. Terraform have also released ‘CDK for Terraform’, which is an AWS CDK equivalent that transpiles to Terraform rather than CloudFormation. However, this is currently less mature than AWS CDK.

CDK additionally provides libraries to write assertion and validation tests with familiar testing tools such as jest, further moving the point at which errors are caught to earlier in the development cycle. CDK also adopts the paradigm of encapsulation with its ‘constructs’. This allows you to wrap specific resource provisioning into a simple, reusable package that can be used again elsewhere.

<Chunk>
 However, with serverless architectures we’re likely already buying into cloud-provider-specific constructs. Terraform have also released ‘CDK for Terraform’, which is an AWS CDK equivalent that transpiles to Terraform rather than CloudFormation. However, this is currently less mature than AWS CDK.

CDK additionally provides libraries to write assertion and validation tests with familiar testing tools such as jest, further moving the point at which errors are caught to earlier in the development cycle. CDK also adopts the paradigm of encapsulation with its ‘constructs’. This allows you to wrap specific resource provisioning into a simple, reusable package that can be used again elsewhere.
## Installation
A pre-requisite is to have the AWS CLI installed and configured with an appropriate account. Once you’ve done this, you can install CDK using `npm i -g cdk` - this will install it globally on your machine. We’re going to start a new project from scratch using CDK’s TypeScript template. Create a new `cdk-demo` directory and execute `cdk init app --language typescript` inside it.
## The basics
There are a couple main files to go over here. Firstly, `lib/cdk-demo-stack.ts` creates our first CDK stack. A stack corresponds to a CloudFormation template, which provisions the resources needed for our applications and services.

Next, we have `bin/cdk-demo.ts`, which defines our CDK app. An app can contain multiple stacks, and modularising our services into stacks decouples their deployments. This means if we make changes to only one stack we only need to worry about redeploying this stack.

```typescript
env: { region: 'eu-west-1' }
```

<Chunk>
## The basics
There are a couple main files to go over here. Firstly, `lib/cdk-demo-stack.ts` creates our first CDK stack. A stack corresponds to a CloudFormation template, which provisions the resources needed for our applications and services.

Next, we have `bin/cdk-demo.ts`, which defines our CDK app. An app can contain multiple stacks, and modularising our services into stacks decouples their deployments. This means if we make changes to only one stack we only need to worry about redeploying this stack.

```typescript
env: { region: 'eu-west-1' }
```
## Creating a Lambda
Let's first create a `lambdas` directory to store our code. Inside this, we'll create `getLunchSpots.ts` containing the following:

```typescript
// This creates a simple handler that returns a 200 response with the serialised lunchSpots object
```
## Adding to the Stack
Now we’ll create a Lambda resource in our stack directly from our TypeScript handler function. Add the following code to the stack file:

```typescript
// This creates a Node.js Lambda function with the name ‘getLunchSpots’, using the handler function found in lambdas/getLunchSpots.ts
```
## Deployment
To deploy our changes, we can simply run `cdk deploy`. You may get a prompt asking to enter 'y' to confirm deployment of the changes. Near the end of the deployment script's terminal output you should see an 'Outputs' section with an entry 'CdkDemoStack.sohoLunchSpotsEndpoint...'. This is the endpoint of our API Gateway API. If we make a GET request to this url + /lunch-spots, we'll see the JSON object we specified in our Lambda function returned.

<Chunk>

# Run AWS on Your Laptop: Introduction to LocalStack
## What is LocalStack

LocalStack is a cloud service emulator that runs AWS services solely on your laptop without connecting to a remote cloud provider.
### How can it help us as a developer?

Whether you are,

1. A beginner who is looking to practice your AWS skills but don’t have access to a credit card which is required upon AWS registration.
2. A student, who wants to gain hands-on experience with AWS services without spending any money.
3. A professional who wants to troubleshoot or test your infrastructure configurations offline on your machine, without setting up a separate cloud environment for testing, and then seamlessly transition to your main AWS production environment once everything is ready and optimized.

LocalStack has got you covered. Because the last thing you want to do is set up an AWS environment to improve your skills, only to accidentally shoot yourself in the foot with a ridiculous amount of money because you didn’t know about cost plans or didn’t setup budget alerts.
## AWS services offered by LocalStack

There are currently more than 60 emulated AWS cloud services (and most of them free) provided by LocalStack. As an introduction we will create an S3 bucket and emulate the deployment of a static website and witness that it works just as if it were deployed on AWS S3.

<Chunk>
 A student, who wants to gain hands-on experience with AWS services without spending any money.
3. A professional who wants to troubleshoot or test your infrastructure configurations offline on your machine, without setting up a separate cloud environment for testing, and then seamlessly transition to your main AWS production environment once everything is ready and optimized.

LocalStack has got you covered. Because the last thing you want to do is set up an AWS environment to improve your skills, only to accidentally shoot yourself in the foot with a ridiculous amount of money because you didn’t know about cost plans or didn’t setup budget alerts.
### Installation

There are two ways to run LocalStack on your pc:

1. **Using PIP**

```shell
python -m virtualenv venv
venv/Scripts/activate
pip install localstack
```

2. **Starting LocalStack with Docker**

```shell
docker run --rm -it -p 4566:4566 -p 4510-4559:4510-4559 localstack/localstack
```

For this article, I will use the first choice and install localstack using pip. We also need to install AWS Command Line Interface so that we can interact with the underlying emulated services.

To download the AWS Command Line Interface go to AWS CLI Docs and find AWS CLI MSI installer for Windows, then run the setup. After the installation is complete, then let's proceed and configure the CLI using the commands below.

```shell
aws configure 
AWS Access Key ID: test
AWS Secret Access Key: test
Default region name: us-east-1
Default output format: [None]
```

The next step is to run LocalStack. So open a terminal and run the following:

```shell
localstack start -d
```

This will start LocalStack on localhost port 4566.

When interacting with LocalStack to emulate AWS services it’s important to configure your AWS CLI or SDK to point to the LocalStack endpoint URL. This allows you to interact with LocalStack easily without having to specify the `--endpoint-url` option every time you run a command.

Another option is installing a tool called “awslocal” which is a wrapper around the AWS CLI for LocalStack. It automatically configures the CLI to use the LocalStack endpoint URL, saving you from the manual step of specifying the `--endpoint-url` option.

<Chunk>
 So open a terminal and run the following:

```shell
localstack start -d
```

This will start LocalStack on localhost port 4566.

When interacting with LocalStack to emulate AWS services it’s important to configure your AWS CLI or SDK to point to the LocalStack endpoint URL. This allows you to interact with LocalStack easily without having to specify the `--endpoint-url` option every time you run a command.

Another option is installing a tool called “awslocal” which is a wrapper around the AWS CLI for LocalStack. It automatically configures the CLI to use the LocalStack endpoint URL, saving you from the manual step of specifying the `--endpoint-url` option.
### Example 1 - Creating S3 bucket

```shell
aws --endpoint-url=http://localhost:4566 s3api create-bucket --bucket testbucket
```
### Example 2 - Host a static website

In order to host a static website inside the S3 bucket we just created above, we need to prepare two html files: `index.html` and `error.html` and put them in a folder called `website`.

The `index.html` file serves as the main entry point for your website, representing the content and structure of your homepage. The `error.html` file is used to create custom error pages that get displayed during HTTP errors like 404 (Not Found) or 403 (Forbidden).

We also can attach a bucket policy and allow public access to all of its contents. So create a file named `bucket_policy.json` in the root directory and add the following code:

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "PublicReadGetObject",
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::testbucket/*"
        }
    ]
}
```

Sync the website directory to the S3 bucket using the command below:

```shell
aws --endpoint-url=http://localhost:4566 s3 sync .\website\ s3://testbucket
```

Finally, we enable static website hosting on the bucket and configure the index and error documents.

Our website is now hosted on the emulated S3 bucket. So let's browse to `http://testbucket.s3website.localhost.localstack.cloud:4566/` and see what it looks like.

<Chunk>

# How to create Private DynamoDB tables accessible only within a VPC

DynamoDB is a fully managed NoSQL database service known for its low latency and high scalability. Like most other AWS services, it’s protected by AWS IAM. So, despite being a publically accessible service, your data is secure. Conversely, zero-trust networking tells us we should always authenticate the caller and shouldn’t trust someone just because they’re in a trusted network perimeter. All this is to say that you don’t need network security to keep your DynamoDB data safe. However, adding network security on top of IAM authentication and authorization is not a bad thing. Sometimes it’s even necessary to meet regulatory requirements or to keep your CSO happy! This has come up numerous times in my consulting work. In this post, let me show you how to make a DynamoDB table accessible only from within a VPC.
## Can’t you just use a VPC endpoint?

You can create a VPC endpoint for DynamoDB to create a tunnel between your VPC and DynamoDB. This allows traffic to go from your VPC to DynamoDB without needing a public IP address. This is often perceived to be more secure “because it doesn’t go out to the public internet”. But as far as I know, traffic from a VPC to DynamoDB (through a NAT Gateway) would never traverse over public internet infrastructure anyway. It’s just that you need a public IP address so DynamoDB knows where to send the response. More importantly, a VPC endpoint doesn’t stop other people from accessing the DynamoDB table from outside your VPC.

<Chunk>
## Can’t you just use a VPC endpoint?

You can create a VPC endpoint for DynamoDB to create a tunnel between your VPC and DynamoDB. This allows traffic to go from your VPC to DynamoDB without needing a public IP address. This is often perceived to be more secure “because it doesn’t go out to the public internet”. But as far as I know, traffic from a VPC to DynamoDB (through a NAT Gateway) would never traverse over public internet infrastructure anyway. It’s just that you need a public IP address so DynamoDB knows where to send the response. More importantly, a VPC endpoint doesn’t stop other people from accessing the DynamoDB table from outside your VPC.
## No resource policy

Usually, this can be done using resource policies. Unfortunately, DynamoDB doesn’t support resource policies. Instead, you can use the aws:SourceVpc or aws:SourceVpce conditions in your IAM policies. You can use these conditions to ensure the IAM user/role can only access DynamoDB from a VPC, or through a VPC endpoint. But how we can ensure that everyone follows this requirement? As a member of the platform team, I don’t want to be a gatekeeper. I want the feature teams to create their own IAM roles for their Lambda functions or containers. But how can I make sure they don’t violate our compliance requirements? There are a few ways to do this. The easiest and most reliable is to use Service Control Policies (SCPs) with AWS Organizations.

<Chunk>
 Unfortunately, DynamoDB doesn’t support resource policies. Instead, you can use the aws:SourceVpc or aws:SourceVpce conditions in your IAM policies. You can use these conditions to ensure the IAM user/role can only access DynamoDB from a VPC, or through a VPC endpoint. But how we can ensure that everyone follows this requirement? As a member of the platform team, I don’t want to be a gatekeeper. I want the feature teams to create their own IAM roles for their Lambda functions or containers. But how can I make sure they don’t violate our compliance requirements? There are a few ways to do this. The easiest and most reliable is to use Service Control Policies (SCPs) with AWS Organizations.
## Service Control Policies (SCPs)

SCPs let you apply broad strokes to deny certain actions in the member accounts (but NOT the master account) of an AWS organization. For example, you can use SCPs to: Stop any actions in regions other than us-east-1 or eu-west-1, where your application resides. Stop anyone from creating EC2 instances. Both are common SCPs people use to protect against cryptojacking. In our case, we can use a SCP like this to reject any requests to DynamoDB, unless they originate from within the specified VPC.

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "DenyDynamoDBOutsideVPC",
      "Effect": "Deny",
      "Action": "dynamodb:*",
      "Resource": "*",
      "Condition": {
        "StringNotEquals": {
          "aws:SourceVpc": "vpc-xxxxxxxxx"
        }
      }
    }
  ]
}
```

Every AWS account would have its own VPC(s), so you will likely need an account-specific SCP. This SCP can also create significant friction to development. Every action against DynamoDB tables, including the initial deployment, has to be performed from within the designated VPC. And you won’t even be able to use the DynamoDB console!

A good compromise is to leave the SCP off the development accounts. That way, developers are not hamstrung by this strict SCP and you can still enforce compliance in production.

<Chunk>

# Serverless queues and workers — Designing Lift

This article is part of a series on Lift, an open-source project that simplifies deploying serverless applications.

As introduced in previous articles, Lift is a Serverless Framework plugin that simplifies deploying serverless applications.

As we approach the first beta planned for May, let’s talk about queues and workers:
- How to deploy production-ready queues and workers on AWS today.
- How we plan to simplify that with Lift.
## The naive approach

When going serverless, SQS + AWS Lambda is an iconic duo:
- SQS is a queue service that is entirely managed by AWS, scales automatically, and bills by the usage.
- AWS Lambda runs our code and is entirely managed by AWS, scales automatically, and bills by the usage.

The best part: Lambda integrates with SQS natively. We can write “queue workers” on Lambda without having to poll messages from SQS: instead, our code (the worker) is automatically invoked when a new message (aka job) is pushed into SQS.

To deploy queues and workers with the Serverless Framework, we need a bit of CloudFormation:

```yaml# Example configuration here
```
### Problems with the naive approach

The biggest missing piece here is error handling.

By default, SQS retries failed jobs indefinitely. Imagine your code has a bug (let’s pretend that can happen): the job will be retried over and over for days, possibly costing a lot of money and wasting resources.

To deal with errors, we need to set up a “dead letter queue” (a queue that stores failed messages) and limit the number of retries.

On top of that, there are many details that need to be configured, for example:
- We should configure the dead letter queue to store failed messages for 14 days (the maximum).
- We should set up an alarm that sends our team an email whenever there are failed messages in the dead letter queue.
- Many other settings need to be fine-tuned: the SQS “visibility timeout”, the Lambda max concurrency, message batching, etc.

<Chunk>
 Imagine your code has a bug (let’s pretend that can happen): the job will be retried over and over for days, possibly costing a lot of money and wasting resources.

To deal with errors, we need to set up a “dead letter queue” (a queue that stores failed messages) and limit the number of retries.

On top of that, there are many details that need to be configured, for example:
- We should configure the dead letter queue to store failed messages for 14 days (the maximum).
- We should set up an alarm that sends our team an email whenever there are failed messages in the dead letter queue.
- Many other settings need to be fine-tuned: the SQS “visibility timeout”, the Lambda max concurrency, message batching, etc.
## A production-ready approach

Here is a preview of a `serverless.yml` configuration that includes those best practices:

```yaml# serverless.yml
functions:
  worker:
    handler: worker.handler
    timeout: 10 # seconds
    reservedConcurrency: 10
    events:
      - sqs:
          arn: !GetAtt ReportGenerationQueue.Arn
          batchSize: 1
resources:
  Resources:
    ReportGenerationQueue:
      Type: AWS::SQS::Queue
      Properties:
        QueueName: !Sub '${AWS::StackName}-myqueue'
        VisibilityTimeout: 60
        MessageRetentionPeriod: 172800
        RedrivePolicy:
          maxReceiveCount: 5
          deadLetterTargetArn: !GetAtt DeadLetterQueue.Arn
    DeadLetterQueue:
      Type: AWS::SQS::Queue
      Properties:
        MessageRetentionPeriod: 1209600
```

If we wanted to add an alarm for failed messages, we would need 30 more lines of YAML.

<Chunk>

# Integration Testing Step Functions: Using sls-test-tools
## The states which make up a Step Function
A Step Function is a state machine, where states have types, namely:
- Task state: for performing an operation
- Choice state: for choosing which branch of the state machine to execute next
- Fail/Succeed state: for ending the execution of a state machine
- Pass state: for passing its input to output, or injecting fixed data
- Wait state: for imposing a delay into the execution of the state machine
- Parallel state: for creating parallel branches of execution in the state machine
- Map state: for performing the same state action for each item in a list
## Should we test Step Functions?
As with any AWS infrastructure, we need to be able to test it, so that we know we’ve configured it correctly and so that it can't be broken in the future. It's important to test the logic in the individual states, as well as the overall state machine. Testing on the real infrastructure is recommended to best simulate the real production environment.
## Unit vs Integration Testing
Unit tests focus on testing individual pieces of code, whereas integration tests check that various pieces of our system are interacting correctly with each other. It is important to have both! Testing individual states is relatively simple, while integration tests are slightly more complex but crucial.

<Chunk>
## Unit vs Integration Testing
Unit tests focus on testing individual pieces of code, whereas integration tests check that various pieces of our system are interacting correctly with each other. It is important to have both! Testing individual states is relatively simple, while integration tests are slightly more complex but crucial.
## Should we test Step Functions?
As with any AWS infrastructure, we need to be able to test it, so that we know we’ve configured it correctly and so that it can't be broken in the future. It's important to test the logic in the individual states, as well as the overall state machine. Testing on the real infrastructure is recommended to best simulate the real production environment.
## Introducing Step Functions assertions with sls-test-tools
The below code samples will demonstrate how to test a step function called 'TestAddAndDouble' which contains two task states, one which sums two numbers, and another which doubles the number.
### Assert that the state machine has the correct execution status
First let’s test that we get to a Succeeded status when we pass a valid input. To do this we can use the new `toHaveCompletedExecutionWithStatus()` assertion. This allows us to type in the name of the state machine we’ve executed, as well as the expected status for the execution. It then checks that the most recent execution of the state machine has the expected status.

<Chunk>
### Assert that the state machine has the correct execution status
First let’s test that we get to a Succeeded status when we pass a valid input. To do this we can use the new `toHaveCompletedExecutionWithStatus()` assertion. This allows us to type in the name of the state machine we’ve executed, as well as the expected status for the execution. It then checks that the most recent execution of the state machine has the expected status.
## Introducing Step Functions assertions with sls-test-tools
The below code samples will demonstrate how to test a step function called 'TestAddAndDouble' which contains two task states, one which sums two numbers, and another which doubles the number.
### Now let’s check we get the right output given a valid input
For example, if we enter {3, 6} we expect to get 18 as the output. We can once again use a new assertion, the `toMatchStateMachineOutput` assertion. This allows us to type in the name of the state machine we’ve executed, as well as the expected output for this execution, and checks that the most recent execution of the state machine produced the expected output. Here is an example of the usage of this assertion:

```javascript
// Helper: Execute the Step Function until its completed
```

You’ll have noticed above that we had to rely on the step function having been executed. Rather than require a manual step or complex call to the API we’ve also bundled in a helper function to simplify execution and wait for execution to complete. Here is an example of the call to the helper:

<Chunk>

# Building a Serverless Anti-Corruption Layer with CDK

When building large distributed systems, interacting successfully with external systems can introduce large amounts of complexity into your system design. Consider a company’s software system that interacts with a third-party provider. A plan is made to move to a new provider which is faster and cheaper. An issue is identified that the protocol to interact with the old third-party provider is not compatible with the new one. The company will need to modify multiple sections of its software to interact with this new provider. They want to implement a design principle to account for this and allow for easier transitions in future.

A common solution is the Anti-Corruption Layer (ACL). An ACL implements the principles of Domain Driven Design (DDD). DDD is a software development approach that focuses on modelling software based on the real-world business domain it aims to serve. It emphasizes collaboration between technical and domain experts to ensure the software accurately reflects and evolves with the domain. DDD advocates for a ubiquitous language for all stakeholders and organizes the system into bounded contexts to manage complexity. These bounded contexts are called domains but are often referred to as separate services in the context of a distributed system.

ACL follows the principles of DDD by providing a separation of these services, allowing you to structure your business domains most effectively and ensuring that the design and language of your system are protected from 3rd party influence. In addition, when executed correctly an ACL can also prevent vendor lock-in, and provide easier visibility on your system’s architecture.

<Chunk>
 DDD advocates for a ubiquitous language for all stakeholders and organizes the system into bounded contexts to manage complexity. These bounded contexts are called domains but are often referred to as separate services in the context of a distributed system.

ACL follows the principles of DDD by providing a separation of these services, allowing you to structure your business domains most effectively and ensuring that the design and language of your system are protected from 3rd party influence. In addition, when executed correctly an ACL can also prevent vendor lock-in, and provide easier visibility on your system’s architecture.
## What is an ACL?

An Anti-Corruption Layer is a design pattern that provides the functionality to transform or translate information passing from one system to another. It is not purely a mechanism for communicating messages between two systems, instead, it provides guardrails to ensure that misinterpreted data cannot enter into and corrupt your primary system. An ACL can be designed as a package to be reused across other services, or as a self-standing service. By acting as the communication layer between services, an ACL separates and increases decoupling allowing for independent scaling and greater portability of each of the services. Due to its low coupling with the rest of the software system, an external system, like a third-party service above, can be torn out when needed without large changes. This brings the added benefit that decoupled services can now be developed independently of each other increasing developer flexibility and build time.

<Chunk>
 An ACL can be designed as a package to be reused across other services, or as a self-standing service. By acting as the communication layer between services, an ACL separates and increases decoupling allowing for independent scaling and greater portability of each of the services. Due to its low coupling with the rest of the software system, an external system, like a third-party service above, can be torn out when needed without large changes. This brings the added benefit that decoupled services can now be developed independently of each other increasing developer flexibility and build time.
## ACL as a Service:

An ACL can be created as a package or a fully-fledged service, however, having it as a service will yield the following benefits:

- It encourages the abstraction of protocols that are required to communicate with external services. Building a whole new layer to handle the communication between services can become extremely complex. By encapsulating the ACL as a standalone service, an organisation is likely to have a dedicated team of experts building and maintaining this service. This protects developers from other domains needing knowledge of the intricacies of the ACL domain.
- Very often, logic used to communicate with external services can result in a large repository of code. Having this within a package can quickly become difficult to maintain, document and use by consumers. A service simplifies this complexity by allowing the creation of standalone features, such as dedicated infrastructure and organisation of logic, in a way that best fits the ACL rather than its consuming services.
- In addition, the ACL service itself can leverage internal packages to reuse code across the service and the ACL team will have context on these internal development tools. Ideally, having the ACL as a service, not a module or package within a service, will provide easier construction, understanding, and correct visibility of this tool at the levels required from those in and outside the ACL domain.

<Chunk>
 Having this within a package can quickly become difficult to maintain, document and use by consumers. A service simplifies this complexity by allowing the creation of standalone features, such as dedicated infrastructure and organisation of logic, in a way that best fits the ACL rather than its consuming services.
- In addition, the ACL service itself can leverage internal packages to reuse code across the service and the ACL team will have context on these internal development tools. Ideally, having the ACL as a service, not a module or package within a service, will provide easier construction, understanding, and correct visibility of this tool at the levels required from those in and outside the ACL domain.
## Building Blocks of an ACL:

When building an Anti-Corruption Layer, we can break down its core functionality into three mechanisms: Facades, Adapters, and Translators.

- A **Facade** provides a simplified interface on top of a complex set of underlying classes. In terms of the ACL, it simplifies access for the client by making the subsystem easier to use.
- An **Adapter** allows the client to use a different protocol than the one understood by the system they are trying to interact with.
- The final element is the **Translator**. The Translator’s job can be seen as a subset of the Adapter’s. The translator’s job is to translate the actual logic to be recognisable to the client’s system.

<Chunk>

# Service Ports: Finding a Loosely Coupled Utopia with Event-Driven Serverless

When we build Event-Driven Serverless architectures using Domain-Driven Design, we end up with a set of services clearly split by business function and communicating asynchronously over an event channel (e.g. Amazon EventBridge). These architectures bring many advantages: loose coupling, independent deployability, testability, and reduced complexity, to name a few. Yet, no matter how elegant our modelling of the domain, there are always some cross-cutting infrastructure dependencies (e.g. the event channel itself) that can’t be eliminated.

How we deal with them determines if we get a loosely-coupled utopia, or a sprawling tangle of semi-independent services. When you’ve put in 90% of the effort, don’t let this be the final stumbling block.

In this article we’ll see how clearly defined Service ports resolve the final 10% of infrastructure dependencies and bring loose coupling, independent deployment/testing and increase development velocity.

<Chunk>
. Amazon EventBridge). These architectures bring many advantages: loose coupling, independent deployability, testability, and reduced complexity, to name a few. Yet, no matter how elegant our modelling of the domain, there are always some cross-cutting infrastructure dependencies (e.g. the event channel itself) that can’t be eliminated.

How we deal with them determines if we get a loosely-coupled utopia, or a sprawling tangle of semi-independent services. When you’ve put in 90% of the effort, don’t let this be the final stumbling block.

In this article we’ll see how clearly defined Service ports resolve the final 10% of infrastructure dependencies and bring loose coupling, independent deployment/testing and increase development velocity.
## How the problem arises

We’ve discussed previously how Amazon EventBridge can be combined with strong Domain-Driven Design to create such architectures (see EventBridge Storming). When we build a collection of Services (or “microservices”) in such a way there are still some cross-cutting global dependencies. These dependencies get in the way of developing services independently, make integration testing difficult and can create complex code in our infrastructure definition. In addition, when such dependencies are not made explicit, the deployment order of Services can become a mystery at best, and a set of race conditions at worst.

To make this more tangible, let’s imagine we conducted an EventBridge Storming session to understand an example problem domain of an e-commerce website by gathering all the business events of the system into Bounded Contexts. From this session we would understand the business domain, the events that belong to it, and how they can be grouped together. From such a session we could derive the following services, grouping together the domain events and creating service boundaries with loose coupling.

A high level view of the architecture would look as follows:

Notice two classic shared infrastructure dependencies are the EventBus itself, and the Cognito User Pool. Reference to these in the infrastructure is simple, it’s an ARN. But requiring these dependencies to be deployed (with everything else in the stack they belong to) makes independent development and testing complex. In addition, these references can also be hidden in the weeds of the infrastructure as code files.

Such cross-cutting infrastructure dependencies need to be intentional, and to be designed for and made explicit to the developers. If not, this will lead to several problems:

“The Source of Existence Problem”. We could keep this definition of the dependencies in the Finance Service and reference it in the HR Service. When it’s 2 Services it’s fine, but what about 10 Services all needing this resource.

In addition, we’ve created some knock-on issues. The Finance Service now needs to be deployed before the HR service. In production this seems manageable, it will happen the first time; but what about Dev, Staging and UAT? Also, with Serverless, one amazing benefit is ephemeral stacks (see Serverless Flow) — how will this be achieved?

“The New Joiner Problem”: Independent development of the HR service is now no longer possible. Any new developer to the company will have had to have deployed their own version of the Finance Service before they can develop on the HR service — or they depend on the staging version and

<Chunk>
 When it’s 2 Services it’s fine, but what about 10 Services all needing this resource.

In addition, we’ve created some knock-on issues. The Finance Service now needs to be deployed before the HR service. In production this seems manageable, it will happen the first time; but what about Dev, Staging and UAT? Also, with Serverless, one amazing benefit is ephemeral stacks (see Serverless Flow) — how will this be achieved?

“The New Joiner Problem”: Independent development of the HR service is now no longer possible. Any new developer to the company will have had to have deployed their own version of the Finance Service before they can develop on the HR service — or they depend on the staging version and
## Hexagonal Architecture To The Rescue

Ports and Adapters, or hexagonal architecture, is an approach that tries to make dependencies interchangeable. The interchangeable aspect allows development and testing to be simplified. In addition, clear specification of ports ensures that dependencies are explicitly defined. Components list their connections as ports, with an API specification of the protocol they expect. One or more adapters can then implement this protocol.

A classic example is a UI port implemented as either a GUI for users or a CLI for development ease. These are typically at the application code level, but we can bring the concept up to higher level architecture dependencies.

If we look at our example above, the clear dependencies are an EventBus and a UserPool. We could take things further and make these more abstract dependencies, but for now let’s just expect an EventBridge EventBus (any EventBridge EventBus) and a Cognito UserPool (any Cognito UserPool).

A simple representation of these ports can be shown in a Serverless Framework IaC yaml file:

```yaml
Notice here that it’s environment dependent, and there is a clear switch case on the environment. This can be easily overwritten to support a temporary CI stack as per Serverless flow, and switched out manually during local development as needed.
```

Creating this explicit ports section makes the interfaces clearer and should be further explained in a consistent README section for search service of the architecture.

<Chunk>
 We could take things further and make these more abstract dependencies, but for now let’s just expect an EventBridge EventBus (any EventBridge EventBus) and a Cognito UserPool (any Cognito UserPool).

A simple representation of these ports can be shown in a Serverless Framework IaC yaml file:

```yaml
Notice here that it’s environment dependent, and there is a clear switch case on the environment. This can be easily overwritten to support a temporary CI stack as per Serverless flow, and switched out manually during local development as needed.
```

Creating this explicit ports section makes the interfaces clearer and should be further explained in a consistent README section for search service of the architecture.
## Where Should Cross-Cutting Infrastructure Dependencies Live?

Going back to our example of the Finance Service. This service relies on an EventBus and a Cognito User Pool. But where should these dependencies live?

If they live with a Service, e.g. the Finance Service, they will be deployed with it — but is that correct? Well no, just because there is a change to the Finance Service, it does not mean there should be a deployment of the EventBridge or Cognito User Pool.

Our Ports have mitigated some of the interdependence that would be created, but that does not mean we should arbitrarily pick a host Service. Instead we should see these as Services, very simple services due to the abstraction our Cloud provider is giving us over Event Bussing and User Authentication.

It’s tempting to create a “global” or “central” or “infra” repo and dump any cross cutting dependency into this. Avoid this temptation. It will create a dumping ground for anything needed by > 1 Service and will become a mess quickly. Instead create very small Services for each cross-cutting dependency. These will simply contain the Infrastructure as Code configuration, the CI/CD Pipeline and any integration tests.

<Chunk>

# Testing GenAI Applications: Addressing the Challenges of Non-deterministic Language Models
Fabien Zucchet, aleios

The recent emergence of Large Language Models (LLMs) has given rise to a new breed of applications powered by Generative AI (GenAI). These applications leverage third-party LLM services to process inputs in a more creative way than traditional applications. With GenAI applications, developers can easily harness the power of AI to generate content, write code, or solve problems in innovative ways. However, despite their different characteristics, GenAI applications should still be subject to best practice software development principles. Developers still value version control, code review, and rigorous testing for these applications. In this article, we will explore the challenges involved in testing GenAI applications, given the underlying nature of LLMs, and highlight our approach to address these complexities.
## The Emergence of Prompt-Centric Applications and the Need for Testing
Unlike traditional applications, where the core value lies in the code, the main value of GenAI applications comes from interacting with third-party LLMs to process user inputs. This paradigm shift has several implications, the most prominent being the non-deterministic nature of LLMs. Running the same model twice with the same prompt can return slightly different responses. Developers have no way to predict precisely how a model will respond to a given prompt (instructions given to an LLM). In addition, the response from the model is returned as loosely structured or unstructured plain text. Efficient prompt engineering relies on a quick feedback loop to evaluate the quality of LLM responses. Additionally, updating prompts in GenAI applications can introduce unintended consequences and may lead to regressions in the application's behavior. Having a robust testing mechanism is key to bridge the gap between proof of concepts and production applications.

<Chunk>
 Running the same model twice with the same prompt can return slightly different responses. Developers have no way to predict precisely how a model will respond to a given prompt (instructions given to an LLM). In addition, the response from the model is returned as loosely structured or unstructured plain text. Efficient prompt engineering relies on a quick feedback loop to evaluate the quality of LLM responses. Additionally, updating prompts in GenAI applications can introduce unintended consequences and may lead to regressions in the application's behavior. Having a robust testing mechanism is key to bridge the gap between proof of concepts and production applications.
## Defining Test Cases and Generating Test Prompts
Before testing an application, the first step is to identify what aspects require the most quality assurance. The answer to this question depends on the specific application, and there is no one-size-fits-all approach. In the case of Code Review GPT, our primary concern was the consistency of the tool's reviews. To measure this consistency, we focused on a list of common coding flaws that the tool should identify. Each item on this list became a test case, and we aimed to ensure that the tool consistently identified and commented on these patterns in the future. We ran these cases through the application by generating example code snippets based on plain text descriptions of the test cases.

<Chunk>
 The answer to this question depends on the specific application, and there is no one-size-fits-all approach. In the case of Code Review GPT, our primary concern was the consistency of the tool's reviews. To measure this consistency, we focused on a list of common coding flaws that the tool should identify. Each item on this list became a test case, and we aimed to ensure that the tool consistently identified and commented on these patterns in the future. We ran these cases through the application by generating example code snippets based on plain text descriptions of the test cases.
## Evaluating the Quality of AI Responses
The last step involves evaluating the quality of the review produced by the LLM. Using snapshots as reference points for evaluation, we can gauge the quality of the reviews accurately. By comparing the test results to these snapshots, we can determine the success of the test based on the semantic similarity of the responses. This approach reduces the complexity of evaluating the quality of AI responses down to comparing two plain text documents, making it easier to assess the consistency and performance of the GenAI application.
## Generalizing the Test Pipeline
The automated threshold testing pipeline, including generating the test prompts, feeding them to the model, and computing a similarity score with the embedded snapshots, is a generic concept that can be applied to many GenAI applications.


<Chunk>

# Serverless Image Object Detection at a Social Media Startup
## What is Image Object Detection?
Simply put, image object detection is the process of detecting and extracting information about entities in a given image. This involves detecting objects, activities, places, animals, products, etc. Image object detection has a wide array of use cases across a variety of industries. Major sectors such as banks, insurance, social media and dating apps, news agencies and FinTech use object detection in some form or another.
## Our Use Case
Recently, we were tasked with building an image object detection feature for a social media startup. The use case was simple — users should be able to select some of their favourite photos and submit them to be featured on one of the startup’s social media pages. The social media marketing team needed a way to search through the image submissions for certain themes — such as photos of the ocean, popular landmarks, animals, music concerts, etc.

<Chunk>
 The social media marketing team needed a way to search through the image submissions for certain themes — such as photos of the ocean, popular landmarks, animals, music concerts, etc.
## What is Image Object Detection?
Simply put, image object detection is the process of detecting and extracting information about entities in a given image. This involves detecting objects, activities, places, animals, products, etc. Image object detection has a wide array of use cases across a variety of industries. Major sectors such as banks, insurance, social media and dating apps, news agencies and FinTech use object detection in some form or another.
## Image Analysis using Deep Learning
Analysing images and classifying them based on scenery and objects within the image is no simple task. Human sight is nothing short of remarkable and building an application that’s able to replicate the brain’s ability to detect objects is immensely complex. There is an entire Computer Vision industry devoted to doing just that. Performing object detection from scratch is typically a multi-step process that involves: Collecting images and labelling the objects within Training the ML models Using the trained models to perform the analysis Performance tuning And, repeat… Our aim for this feature, like all others on the project, was to build it quickly and test its efficacy in a production environment as soon as possible. Furthermore, we didn’t want to devote development resources to building a solution from the ground up when we could leverage existing cloud services. Cue Serverless — the startup’s entire backend is fully Serverless and event-driven. With this architecture we’re able to have teams of developers that only need to focus on features that differentiated the social media app from others. Serverless also enables us to build highly-scalable services whilst also only paying for exactly what we use — an important consideration for a scaling startup. So to achieve this feature, we used Amazon Rekognition — a fully Serverless image and video analysis service. Using Rekognition, we were able to develop this complex and critical workflow in a matter of hours. Let’s dive into it.

<Chunk>
 Cue Serverless — the startup’s entire backend is fully Serverless and event-driven. With this architecture we’re able to have teams of developers that only need to focus on features that differentiated the social media app from others. Serverless also enables us to build highly-scalable services whilst also only paying for exactly what we use — an important consideration for a scaling startup. So to achieve this feature, we used Amazon Rekognition — a fully Serverless image and video analysis service. Using Rekognition, we were able to develop this complex and critical workflow in a matter of hours. Let’s dive into it.
## What is Amazon Rekognition?
Amazon Rekognition is an AWS Serverless offering that uses deep learning to perform image and video analysis. Being fully Serverless means that with Rekognition we don’t need to worry about the complexity of the underlying infrastructure, we pay only for what we use and it provides us with pre-written software for image and video analysis tasks. Rekognition offers a range of features, including image label detection, face detection, celebrity detection, content moderation and text detection. The best part? Rekognition abstracts away the heavy lifting of building, training and analysing deep learning models. Image and video analysis is quick and simple, with minimal set-up necessary. We didn’t need to worry about building and training our own datasets and provisioning server capacity so that our service would scale. All we needed to worry about was integrating.

<Chunk>
 Rekognition offers a range of features, including image label detection, face detection, celebrity detection, content moderation and text detection. The best part? Rekognition abstracts away the heavy lifting of building, training and analysing deep learning models. Image and video analysis is quick and simple, with minimal set-up necessary. We didn’t need to worry about building and training our own datasets and provisioning server capacity so that our service would scale. All we needed to worry about was integrating.
## Architecture
The architecture is straight-forward. Our mobile app uploads images from users’ phones into an S3 bucket. The upload to S3 then triggers a Lambda function which in turn calls the Rekognition API, and stores the results in DynamoDB for querying.

A simplified version of our Serverless framework Infrastructure as Code file looks like this:

```yaml
//serverless.yaml
functions:
  imageLabelDetection:
    handler: image-label-detection.handler
    events:
      - s3:
          bucket: my-image-bucket
          event: s3:ObjectCreated:*
          existing: true
    iamRoleStatements:
      - Effect: Allow
        Action: rekognition:DetectLabels
        Resource: "*"
      - Effect: Allow
        Action: s3:GetObject
        Resource: arn:aws:s3:::my-image-bucket
```

Our Lambda code simply calls the Rekognition API and stores the results in DynamoDB, but you can use whatever makes sense for your use case. We obtain the S3 bucket name and the image’s object name from the S3 event and pass those in to the detectLabels function of the Rekognition SDK. We also pass in two optional parameters (MaxLabels and MinConfidence) to specify the confidence level threshold and maximum number of labels that we want returned. In the example below, we will only get up to 20 labels in the response and all labels will have a confidence level of more than 80%.

```javascript
//image-label-detecion.js
const AWS = require("aws-sdk");
const rekognition = new AWS.Rekognition();
exports.handler = async (event) => {
  const imageDetails = event.Records[0].s3;
  const bucketName = imageDetails.bucket.name;
  const objectKey = imageDetails.object.key;
  const rekognitionResp = await rekognition
    .detectLabels({
      Image: {
        S3Object: {
          Bucket: bucketName,
          Name: objectKey,
        },
      },
      MaxLabels: 20,
      MinConfidence: 80,
    })
    .promise();
  // Send to data store, e.g. DynamoDB
  // ...
};
```

<Chunk>

# Building Serverless Observability Tools With Custom Metrics and Dashboards in CDK

Meaningful logging is a must for Serverless architectures, and of course, there are a lot of third-party tools that can help with observability. However, there is a lot of untapped potential in using Cloudwatch Dashboards and Alarms. This article will show you how to have powerful observability, natively, across your project.
## Current observability solutions in Serverless
Serverless (and almost all cloud) projects comprise multiple services carrying out different functions to form your product. This modularity is incredibly powerful, but by its very nature makes it hard to track any issues — there’s a tonne of moving parts and it’s often difficult to tell exactly where something went wrong.
This lack of observability is one of the largest gripes people have when adopting serverless, and companies have relied on a huge host of pre-built tools to help better understand their products. Until now, setting up robust monitoring using AWS services has been a painful process that takes up a lot of upfront work before getting any real benefit.
Now, The AWS Cloud Development Kit (AWS CDK) changes all of that.
CDK is an open-source framework to create Infrastructure as Code (IaC) and provision it through AWS CloudFormation. It supports the most common Object Oriented Programming Languages, and for Serverless that’s most importantly Typescript.
CDK utilises this Object Oriented paradigm, and as a result, is instinctually modular and extendable. This allows you to abstract dashboards to use across services and teams, being incredibly easy to create custom constructs compared to other Cloud Formation-creating IaC that use YAML syntax. This has changed the creation of observability tools from arduous to automatic, and now makes native the solution for project observability.
Existing tools still have their place and for off-the-shelf there is no better, but for highly customizable observation in environments where you can’t leverage external tools, CloudWatch IaC provides a powerful and flexible option if you’re willing to dedicate the time.

<Chunk>
 This allows you to abstract dashboards to use across services and teams, being incredibly easy to create custom constructs compared to other Cloud Formation-creating IaC that use YAML syntax. This has changed the creation of observability tools from arduous to automatic, and now makes native the solution for project observability.
Existing tools still have their place and for off-the-shelf there is no better, but for highly customizable observation in environments where you can’t leverage external tools, CloudWatch IaC provides a powerful and flexible option if you’re willing to dedicate the time.
## Fair Warning
CDK is a maturing product, and at times, the documentation can be both sparse and complicated — it’s easy to get lost down a rabbit hole, which I spent an embarrassingly long amount of time doing.
At the end of that process, however, I’ve now worked with the tool enough to be able to build incredibly flexible monitoring tools, alongside setting automated alarms should any issues arise. This article should help demystify Cloudwatch in CDK, and set you in the right direction to monitor your Serverless applications.

We will cover:
- The infrastructure for a Cloudwatch Dashboard
- How to publish your custom metrics
- How to create widgets and a dashboard to represent them
- Setting up an Alarm for a Metric
- Sending an email with SNS when an Alarm is triggered
- Storing an email list in SSM
- The pieces that make up a Cloudwatch Dashboard

A Cloudwatch Dashboard is made up of three parts:
- Metrics — the actual data you want to show in the dashboard
- Widgets — effectively a ‘graph’ in the dashboard
- Dashboards — the collection of widgets
A Dashboard can contain multiple Widgets, and a Widget can visualize multiple Metrics.
An example of multiple metrics in a Widget

<Chunk>
h in CDK, and set you in the right direction to monitor your Serverless applications.

We will cover:
- The infrastructure for a Cloudwatch Dashboard
- How to publish your custom metrics
- How to create widgets and a dashboard to represent them
- Setting up an Alarm for a Metric
- Sending an email with SNS when an Alarm is triggered
- Storing an email list in SSM
- The pieces that make up a Cloudwatch Dashboard

A Cloudwatch Dashboard is made up of three parts:
- Metrics — the actual data you want to show in the dashboard
- Widgets — effectively a ‘graph’ in the dashboard
- Dashboards — the collection of widgets
A Dashboard can contain multiple Widgets, and a Widget can visualize multiple Metrics.
An example of multiple metrics in a Widget
## Creating a Metric
The first step in creating a dashboard is to create a metric. There are two kinds, default and custom metrics.
### Default Metrics
By default, lambdas already come with some metrics you can record, such as Errors and the Number of Invocations. The whole list can be found here.
It’s straightforward to reference these metrics in CDK, and the official documentation for doing so is quite good!

Here’s how you reference a metric that shows the average number of errors per minute.

```typescript
declare const fn: lambda.Function;  
const minuteErrorRate = fn.metricErrors({ statistic: 'avg', period: Duration.minutes(1), label: 'Lambda failure rate' });
```

To instantiate a custom metric, you have to publish a data point. As publishing data during run-time isn’t IaC, you have to use the SDK, using the putMetricData function. This creates an instance of a datapoint, which you can then aggregate to create a metric.

Here’s an example call that would facilitate a similar Error Count metric, initially creating the datapoint and then creating a metric from that.

<Chunk>
 As publishing data during run-time isn’t IaC, you have to use the SDK, using the putMetricData function. This creates an instance of a datapoint, which you can then aggregate to create a metric.

Here’s an example call that would facilitate a similar Error Count metric, initially creating the datapoint and then creating a metric from that.
### Creating the Data Point:

```typescript
const params = {
  MetricData: [
    {
      MetricName: 'Errors Thrown',
      Dimensions: [
        {
          Name: 'Error Code',
          Value: request?.error?.code.toString() ?? '500',
        },
        {
          Name: 'Function Name',
          Value: request.context?.functionName ?? 'Unknown Function',
        },
      ],
      Unit: 'None',
      Value: 1,
      Timestamp: new Date(),
    },
  ],
  Namespace: `${process.env['STAGE']}`, //STAGE
};
await cloudwatch.putMetricData(params).promise();
```
### Creating the Metric:

To then reference the Metric you are publishing in IaC (for use in Dashboards and Alarms), you could then create the following!

```typescript
const errorRate = new Metric({
  namespace: stage,
  metricName: 'Errors Thrown',
  region: 'eu-west-1',
  dimensionsMap: {
    'Error Code': 500,
    'Function Name': functionName,
  },
  period: Duration.minutes(1),
  statistic: 'Average',
});
```

The key part to note is the metricName, dimensionsMap, region and namespace all have to be the same as the ones that you put.

<Chunk>
### Creating the Metric:

To then reference the Metric you are publishing in IaC (for use in Dashboards and Alarms), you could then create the following!

```typescript
const errorRate = new Metric({
  namespace: stage,
  metricName: 'Errors Thrown',
  region: 'eu-west-1',
  dimensionsMap: {
    'Error Code': 500,
    'Function Name': functionName,
  },
  period: Duration.minutes(1),
  statistic: 'Average',
});
```

The key part to note is the metricName, dimensionsMap, region and namespace all have to be the same as the ones that you put.
### Creating a Dashboard & Widget
Creating the metric is the difficult part, and the CDK documentation for creating a Widget and Dashboard is fairly good.

Firstly, to create a dashboard you could write the following:

```typescript
const lambdaDashboard = new Dashboard(this, 'MyDashboardId', {
  dashboardName: 'MyDashboardName',
});
```

And the following to add a Widget:

```typescript
lambdaDashboard.addWidgets(
  new GraphWidget({
    title: `${name} Errors`,
    width: 24,
    left: [
      myMetric
    ],
  }),
);
```

<Chunk>

# Why Serverless will enable the Edge Computing Revolution
## On the Edge

Edge Computing is a model in which computing and storage move closer to the end user. Currently ~10% of enterprise data is created and processed outside the traditional Data Centre/Cloud. By 2025 that’s predicted to reach 75%.

Content Distribution Networks (or CDNs) represent the first wave of Edge computing. With a CDN the data needed by users is stored in multiple edge locations closer to the users — reducing the transport time and improving performance. As CDNs have become more advanced the maturity of virtualisation technologies has allowed code as well as storage to move outside the bounds of the Cloud/Data Centre. This has enabled computing resources to emerge as a service in Edge locations.

The typical usage of Edge computing is for real-time and instant data processing. As opposed to the Cloud, where “Big Data” is the name of the game, there is much more of a focus on realtime “Instant Data” as we move to the Edge.

At its core Edge computing is all about reduction — reduction in latency, traffic, bandwidth, geographic distance, energy and power.

The reduction in latency brought by Edge computing can make certain applications in IOT, AI and ML more achievable. For instance: realtime instruction of autonomous devices, remote surgery and facial recognition are just a few emerging technologies that will leverage Edge computing to some extent.

The rise in Edge computing is further spurred on by the evolution of 5G and faster connectivity and devices in general. While Edge computing is reducing latency by bringing compute closer to user, 5G is reducing the latency of the communication. Together 5G and Edge computing can bring latency down to previously unattainable levels — allowing a new generation of application use cases.

<Chunk>
 For instance: realtime instruction of autonomous devices, remote surgery and facial recognition are just a few emerging technologies that will leverage Edge computing to some extent.

The rise in Edge computing is further spurred on by the evolution of 5G and faster connectivity and devices in general. While Edge computing is reducing latency by bringing compute closer to user, 5G is reducing the latency of the communication. Together 5G and Edge computing can bring latency down to previously unattainable levels — allowing a new generation of application use cases.
## What is Serverless?

Serverless enables us to build applications and services without thinking about the underlying servers. It’s an imperfect name for a broad spectrum of services — but at it’s core it’s an architectural movement to increase agility through reduced Total Cost of Ownership (TCO).

In short, Serverless is an abstraction. We’ve abstracted away the details of the underlying machine to enable a model in which pure application code is sent to the Cloud Provider (e.g. AWS) and run in response to different events. Lambda, the “poster child” for Serverless, is the most widely known Serverless Service — the Function as a Service (FaaS) from AWS. Lambda has changed how we can build application and the range of triggers for Lambda functions has enabled elegant event-driven architectures to flourish.

<Chunk>
 It’s an imperfect name for a broad spectrum of services — but at it’s core it’s an architectural movement to increase agility through reduced Total Cost of Ownership (TCO).

In short, Serverless is an abstraction. We’ve abstracted away the details of the underlying machine to enable a model in which pure application code is sent to the Cloud Provider (e.g. AWS) and run in response to different events. Lambda, the “poster child” for Serverless, is the most widely known Serverless Service — the Function as a Service (FaaS) from AWS. Lambda has changed how we can build application and the range of triggers for Lambda functions has enabled elegant event-driven architectures to flourish.
## Real World Edge Computing

Smart CCTV cameras, IoT devices, medical devices and CDNs (as discussed above) are all use cases of Edge computing. We’re moving processing closer to the place the data is generated. In the case of CCTV this could be the use of instant fall detection through a model trained to recognise a person falling over — there can be almost zero latency in the inference of a fall event and in a factory setting this could halt machinery instantly.

Edge computing is not without its risks. It’s hard to go a day without hearing about the security issues of “smart” devices with the rapid adoption of IoT across all sectors. The scalability of Edge computing services can mitigate some security threats like DDoS, but the core computing model has to adapt to operating in less trusted environments.

<Chunk>
 We’re moving processing closer to the place the data is generated. In the case of CCTV this could be the use of instant fall detection through a model trained to recognise a person falling over — there can be almost zero latency in the inference of a fall event and in a factory setting this could halt machinery instantly.

Edge computing is not without its risks. It’s hard to go a day without hearing about the security issues of “smart” devices with the rapid adoption of IoT across all sectors. The scalability of Edge computing services can mitigate some security threats like DDoS, but the core computing model has to adapt to operating in less trusted environments.
## Worlds Collide — The Serverless Edge… CentreLess?

As stated above, Serverless is all about abstraction. Developers can write application code and ignore the details of the infrastructure. This abstraction, the pay-per-use pricing model and ephemeral (short-lived) execution model of FaaS solutions have a complementary conceptual model to the Edge world.

In the Edge world:
- We don’t control the hardware — so abstraction is key
- It’s an untrusted environment — so we need rigid sandboxing
- We’re often processing unpredictable real-time data at scale — so we need an extremely scalable and elastic model for compute.

The typical computers used at Edge locations need to be small and cheap, therefore we’re operating in a low resource (compared to modern Server sizes) environment when it comes to CPU, RAM and disk — so we need to have compute efficiency and small memory/storage footprints. All these points are challenges that were faced by Serverless, meaning the technology, conceptual model and best practices are complementary to Edge. Serverless has matured extensively since its inception. The tooling, both from the vendor side and the open-source space, has improved massively. Along with this, practitioners have developed best practices and mental models for working in a Serverless environment. All of these advances will enable the rise of Edge computing. Developers have got used to coding in a stateless and abstract runtime environment — as well as integrating with Serverless databases and third parties with elegant design patterns. The paradigm shift from a Serverless compute model to an Edge compute model is marginal, whereas going from a traditional Server or even Kubernetes environment is a much greater leap — both in mindset & tooling.

<Chunk>
 Serverless has matured extensively since its inception. The tooling, both from the vendor side and the open-source space, has improved massively. Along with this, practitioners have developed best practices and mental models for working in a Serverless environment. All of these advances will enable the rise of Edge computing. Developers have got used to coding in a stateless and abstract runtime environment — as well as integrating with Serverless databases and third parties with elegant design patterns. The paradigm shift from a Serverless compute model to an Edge compute model is marginal, whereas going from a traditional Server or even Kubernetes environment is a much greater leap — both in mindset & tooling.
### Lambda@Edge — Serverless services are already at the Edge

Lambda@Edge doesn’t actually sit in the “Serverless” team of AWS. In fact it’s a CloudFront (CDN) product. Lambda@Edge lets customers run application code closer to users by running in the CDN layer. It’s very similar to “traditional” Lambda — we don’t have to manage the infrastructure and we only pay when the code is running. I often make use of Lambda@Edge for security checks, location routing and context specific data modification, SSR of React applications closer to the users, the classic image transformation use-case and even for some basic A/B testing. With Lambda@Edge code must be deployed initially in the US-East-1 region — it’s then distributed across 100s of data centres across the world. Surely distributing code to 100s of locations and running it in the CDN layer in response to network requests is going to complicated?… well no, it’s pretty much the same as deploying a Lambda function — which, thanks to years of Serverless community development, has become a simple task.

```yaml
<Chunk>
 I often make use of Lambda@Edge for security checks, location routing and context specific data modification, SSR of React applications closer to the users, the classic image transformation use-case and even for some basic A/B testing. With Lambda@Edge code must be deployed initially in the US-East-1 region — it’s then distributed across 100s of data centres across the world. Surely distributing code to 100s of locations and running it in the CDN layer in response to network requests is going to complicated?… well no, it’s pretty much the same as deploying a Lambda function — which, thanks to years of Serverless community development, has become a simple task.

```yaml# Serverless.yml file definition for a single Lambda@Edge function — the complete IaC template needed.
```

Using the Serverless Framework we can define our function, state it’s triggered by a CloudFront event and then run a single serverless deploy command. The code itself can be written in Python or JavaScript and there is no manual bootstrapping or virtualisation concerns to deal with. For instance, we could gradually migrate traffic from one S3 bucket to another in a few lines of code. The “handler” function and it’s parameters are very familiar to users of “traditional” Lambda Functions. The developer experience is the same as developing Serverless applications with Lambda. The only difference is that it needs to be deployed to US-East-1 first and there are more constraints on the execution time, CPU and memory. Also only JS or Python (no other supported languages or options for Custom Runtimes) can be used and deploying/rollbacks take longer due to distribution time. This is a great example of how the execution model of Serverless is complementary to the execution environment of the edge.

<Chunk>
 For instance, we could gradually migrate traffic from one S3 bucket to another in a few lines of code. The “handler” function and it’s parameters are very familiar to users of “traditional” Lambda Functions. The developer experience is the same as developing Serverless applications with Lambda. The only difference is that it needs to be deployed to US-East-1 first and there are more constraints on the execution time, CPU and memory. Also only JS or Python (no other supported languages or options for Custom Runtimes) can be used and deploying/rollbacks take longer due to distribution time. This is a great example of how the execution model of Serverless is complementary to the execution environment of the edge.
### Firecracker — solving many challenges for Serverless that Edge computing will face

AWS developed (and open-sourced) Firecracker as an enabling technology to Lambda & Fargate by improving speed and overall efficiency. Firecracker is a virtualisation technology that enables workloads to execute in “microVMs”. microVMs are lightweight VMs (Virtual Machines) that provide security and isolation while maintaining speed and flexibility. Security and low startup-times are enabled via a very minimal design. This ensures the memory footprint is kept low, they run quick and have a minimal attack surface. In short, Firecracker brings greater tenancy density (number of microVMs per machine), improved security, reduced memory requirements and rapid startup times. This is achieved by the leveraging of Linux KVM (virtualization at the kernel level — with the kernel acting as a hypervisor), using Rust as a highly performant implementation language and a very specific and minimal API. Firecracker aimed to improve latency in a resource constrained environment for the purpose of a Serverless FaaS (Lambda). The needs for low latency, strong security isolation and tenant density in a low resource environment are a massive overlap with the needs of Edge computing. Code will need to be run with low latency (especially due to the use cases highlighted above) in a low trust multi-tenant environment on small resourced machines. It’s not surprising then that Lambda@Edge runs on Firecracker. It’s just one example of a technological advance made for Serverless that benefits the development of Edge computing.
