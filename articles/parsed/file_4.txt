['<h1>Understanding Eventual Consistency in DynamoDB</h1>', <p>One of the core complaints I hear about DynamoDB is that it can't be used for critical applications because it only provides eventual consistency.</p>, <p>It's true that eventual consistency can add complications to your application, but I've found these problems can be handled in most situations. Further, even your "strongly consistent" relational databases can result in issues if you're not careful about isolation or your choice of ORM. Finally, the benefits from accepting a bit of eventual consistency can be pretty big.</p>, <p>In this post, I want to dispel some of the fear around eventual consistency in DynamoDB.</p>, <p>We'll walk through this in three different sections:</p>, <p>My hope is that you can use this to make a more informed decision about when and how you can handle eventual consistency in your application.</p>]
['<h2>Background\u200b</h2>', <p>Before we get started, let's set the boundaries for what we're going to discuss today.</p>, <p>There's a lot of confusion around eventual consistency. Part of this is because the area of database and distributed system consistency is confusing -- so much so, that I wrote a huge <a href="https://www.alexdebrie.com/posts/database-consistency/" rel="noopener noreferrer" target="_blank">post on the different concepts of consistency before I could write this one</a>. Amazingly, that post covered three types of consistency, and the idea of 'eventual consistency' was not one of them. If the concepts in this background are confusing to you, you should read that post first.</p>, <p>At a very high level, the notion of "<a href="https://en.wikipedia.org/wiki/Eventual_consistency" rel="noopener noreferrer" target="_blank">eventual consistency</a>" refers to a distributed systems model where, to quote Wikipedia, "if no new updates are made to a given data item, eventually all accesses to that item will return the last updated value."</p>, <p>Note that this definition talks about what eventual consistency can promise, but the interesting part is what it doesn't promise. It doesn't promise that any read of a data item will show the most recent updates to that item. This is the core downside of eventual consistency -- you could update an item with one request, then fail to see those updates with a subsequent request.</p>, <p>With that in mind, let's note three key aspects about eventual consistency. These aspects are broadly applicable, but I will use a DynamoDB focus in discussing them.</p>]
['<h3>Eventual consistency is about replication\u200b</h3>', <p>Fundamentally, eventual consistency is an issue of replication.</p>, <p>In a single-node database, you don't have to worry about eventual consistency. Writes and reads are coming to the same instance, so it should be able to provide a consistent view of a single data item.</p>, <p>Once you start to replicate your data to multiple nodes, the story changes. Now, when a write comes to a node, you need to figure out how to efficiently communicate that write to the other nodes. Further, if you allow reads from the other nodes, then you think about the PACELC tradeoff discussed in the next section.</p>, <p>Replication adds some complexity around eventual consistency, but replication can be a good thing as well! There are a number of reasons you may want to replicate your data to multiple places, such as:</p>, <p><strong>Redundancy.</strong> Adding replicas allows for redundancy in case your primary instance fails. In this case, you're not necessarily reading from your replicas -- you're simply maintaining a hot standby in the event of failure. Thus, there are no eventual consistency implications (outside of failure scenarios) that are implicated solely due to redundancy. This is the <a href="https://www.mongodb.com/docs/manual/replication/#read-operations" rel="noopener noreferrer" target="_blank">default mode for MongoDB replicas</a>, which direct all read requests to the primary node unless otherwise configured.</p>, <p><strong>Increase throughput.</strong> A second reason to add replication is to increase your processing throughput. In this case, you may direct all write operations to your primary node but reads can go to your secondary nodes. This is the approach used when adding read replicas to your relational database. By increasing the nodes available for read traffic, you're able to handle more requests per second. However, you do add the eventual consistency issues discussed in this post.</p>, <p><strong>Get data closer to users.</strong> A third reason for replication is to move data closer to your users. Your users may be global, but often your database is in a central location. If that's the case, the network latency to your database is likely the slowest part of a request. By globally distributing your data, you can vastly reduce client-side latency. This is the approach taken by <a href="https://fly.io/blog/globally-distributed-postgres/" rel="noopener noreferrer" target="_blank">Fly.io with global Postgres</a>. This does implicate eventual consistency issues as well.</p>, <p>Note that these reasons aren't mutually exclusive! All three of these are at play with DynamoDB. Adding replicas in different availability zones enhances availability through redundancy. Because each replica can receive read requests, they also increase throughput. Finally, if you use <a href="https://aws.amazon.com/dynamodb/global-tables/" rel="noopener noreferrer" target="_blank">DynamoDB Global Tables</a>, you can put your data closer to your users.</p>]
['<h3>Eventual consistency is not (only) about CAP\u200b</h3>', <p>When people think about eventual consistency, they often jump straight to the <a href="https://en.wikipedia.org/wiki/CAP_theorem" rel="noopener noreferrer" target="_blank">CAP theorem</a>. And the CAP theorem is relevant when thinking about eventual consistency. If you choose an AP system, you probably have some process to reconcile the data on disparate nodes once the network partition is healed.</p>, <p><a href="https://www.alexdebrie.com/posts/database-consistency/#consistency-in-the-cap-theorem" rel="noopener noreferrer" target="_blank">But the CAP theorem is limited</a>. It only applies during times of failure and only during a specific type of failure -- network partitions between nodes. (See <a href="https://www.alexdebrie.com/posts/when-does-cap-theorem-apply/" rel="noopener noreferrer" target="_blank">this post for more on when CAP applies</a>).</p>, <p>Most of the time, your application is going to be running as intended. During that time, <em>if you're allowing reads from your secondary nodes</em>, you have to think about a different tradeoff -- one between latency and consistency. This is the second half of the <a href="https://en.wikipedia.org/wiki/PACELC_theorem" rel="noopener noreferrer" target="_blank">PACELC theorem</a>, which <a href="https://www.alexdebrie.com/posts/database-consistency/#the-cap-theorem-is-incomplete" rel="noopener noreferrer" target="_blank">I discussed further here</a>.</p>, <p>Let's think about this with a quick example. Imagine you have a three-node system. Node A is the primary and handles the writes, but reads can be handled by any of the nodes.</p>, <p><img alt="Replicated database system" class="img_ev3q" loading="lazy" src="https://user-images.githubusercontent.com/6509926/179993877-a809c319-a671-42e9-8bf5-86a5a489e10b.png"/></p>, <p>When a write comes to your primary node, you have a range of options on how to handle it.</p>, <p>You could require that all three nodes acknowledge durably committing the update before acknowledging the write to the client. This gives you the strongest consistency, as all writes must be agreed upon by all nodes before accepting. However, it also increases latency, as you need to communicate with these remote nodes and wait for the slowest one to respond. You can think of this as comparable to a RDBMS with synchronous replication to read replicas.</p>, <p>On the other end of the spectrum, you could only commit the write to the primary node before acknowledging to the client. The primary node could then send the updates to the replica nodes asynchronously. This has much lower write latency for clients at the cost of more data inconsistencies on reads. This is more comparable to asynchronous read replicas with a relational database or with MongoDB's <code>writeConcern: 1</code> setting.</p>, <p>More in the middle is something like MongoDB's <code>writeConcern: "majority"</code> setting, where the write must be committed to a majority of nodes before returning.</p>, <p><img alt="ELC Spectrum" class="img_ev3q" loading="lazy" src="https://user-images.githubusercontent.com/6509926/179857424-43278539-2d24-4c92-89f8-466eb8ea87eb.png"/></p>, <p>In reviewing the different flavors of eventual consistency below, we'll talk about where each one fits on the spectrum.</p>]
['<h3>Eventual consistency is (usually) about reads\u200b</h3>', <p>A final point to keep in mind is that eventual consistency is <em>almost</em> always a read problem but not a write problem.</p>, <p>Most replicated databases use a primary-replica setup where any piece of data has a primary node and one or more replica nodes. The primary node is responsible for all write operations and will send updates to the replica. This means you don't have an eventual consistency issue when writing data -- you'll always be operating against the latest version of the data when writing.</p>, <p>This is important later on when we discuss strategies for dealing with eventual consistency.</p>, <p>That said, some databases do allow for a multiple primary setup. <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-multi-master.html" rel="noopener noreferrer" target="_blank">Amazon Aurora has multi-master clusters</a> where any of the instances can handle write operations. Similarly, <a href="https://aws.amazon.com/dynamodb/global-tables/" rel="noopener noreferrer" target="_blank">DynamoDB Global Tables</a> are essentially a multi-primary system where each enabled region has a primary node.</p>, <p>Using a multi-primary system increases complexity around eventual consistency and can lead to subtle bugs. We'll talk about this later.</p>]
['<h2>The Three Flavors of Eventual Consistency in DynamoDB\u200b</h2>', <p>With our basic notes in mind, let's understand the three flavors of eventual consistency in DynamoDB. Those flavors are:</p>, <p>We'll handle these in order.</p>]
['<h3>Eventual consistency on your main table\u200b</h3>', <p>First, let's talk about eventual consistency on your main table.</p>, <p>Quick sidebar: DynamoDB tables have a primary key that uniquely identify each item. The DynamoDB API strongly pushes you toward using the primary key for your data access. But often you have items that need to be accessed in multiple distinct ways. With secondary indexes, you accomodate this. You can create secondary indexes with a different primary key, and DynamoDB will replicate your data into the secondary index with this new primary key structure. Finally, note that you can only make reads against your secondary index -- all writes must go through your main table.</p>, <p>Your main table is an interesting case in DynamoDB as you have the option between using strongly consistent reads or eventually consistent reads. To understand why, let's understand DynamoDB's underlying infrastructure and what happens when a write request comes in.</p>, <p>First, you should know that, under the hood, <a href="https://www.alexdebrie.com/posts/dynamodb-partitions/" rel="noopener noreferrer" target="_blank">DynamoDB segments your data onto multiple partitions</a>. DynamoDB will assign a record to a specific partition based on the partition key of your table, and each record belongs to one and only one partition.</p>, <p><img alt="DynamoDB Partitions" class="img_ev3q" loading="lazy" src="https://user-images.githubusercontent.com/6509926/179855105-845bfdcf-200f-42c3-8e87-ed4617f2dfd2.png"/></p>, <p>But that's not the end of the story. Each partition includes three nodes: a primary and two replicas. The primary node is the only one that can handle writes, but any of the replicas can serve reads.</p>, <p><img alt="DynamoDB - Partitions + Replicas" class="img_ev3q" loading="lazy" src="https://user-images.githubusercontent.com/6509926/179855664-a0adb905-eb5e-4d75-b5a9-2dc28a5a350e.png"/></p>, <p>When a write comes to a DynamoDB table, the request router will use the partition key to direct it to the primary node for the given key. The primary node will tentatively apply the write if any write conditions are met. Then it will submit the write to both replica nodes. <em>As soon as one of the replicas responds with a successful write</em>, the primary node will return a successful response to the client.</p>, <p>By ensuring that a majority of the nodes have committed the write, DynamoDB is increasing the durability of the write. Now, even if the primary node fails immediately after the write request, it has been durably committed to another node to preserve the write.</p>, <p>You can also see where the potential for an inconsistent read comes in. Imagine our write request is accepted after the primary and Replica A have recorded it. When a read request comes in, it could go to our primary or either of the two replicas. If it chooses the lagging Replica B, there's a possibility we could receive a stale read that does not include the latest updates to the item.</p>, <p>In effect, DynamoDB chooses an intermediate position on the latency-consistency continuum. It doesn't go for strong consistency by durably writing to all nodes, but it also doesn't take latency to the lowest limit by writing to only one node. Part of this is due to the durability benefits of writing to multiple nodes, but it's relevant nonetheless.</p>, <p><img alt="ELC Spectrum -- DynamoDB writes to main table" class="img_ev3q" loading="lazy" src="https://user-images.githubusercontent.com/6509926/179994442-d4fac8d2-6578-4b1d-94ca-dd4f295c5028.png"/></p>, <p>Two final notes to wrap up this section.</p>, <p>First, while a stale read is possible on your main table, it's fairly unlikely. Remember that DynamoDB will randomly choose one of the three nodes to read from to handle a read request. Because a write will be committed to two of the three nodes before being acknowledged, this means there's a 66% chance you'll hit one of the "strongly consistent" nodes anyway.</p>, <p>Further, the lagging third node probably isn't that far behind in the normal case. The primary sent the write operation to both replicas at the same time. While it won't wait for the second replica to respond, it's probably milliseconds behind at most. This is exactly what <a href="#know-the-general-latency-characteristics-for-your-flavor-of-eventual-consistency">I found in some basic testing as well</a>.</p>, <p>Second, while I've been talking about your main table in this section, the same principles apply to local secondary indexes. Some of the nuances around local secondary indexes require another post, but items in a local secondary index are kept on the same partition as the items in the main table and thus have the same replication behavior.</p>]
['<h3>Eventual consistency on global secondary indexes\u200b</h3>', <p>Now that we've covered the standard case with our main table, let's move on to the second flavor of eventual consistency -- that of global secondary indexes.</p>, <p>Unlike your main table or local secondary indexes, you cannot opt in to strongly consistent reads for your global secondary indexes. To understand why, we'll again look to the infrastructure underlying the implementation.</p>, <p>We talked in the previous section about how DynamoDB partitions your data using the partition key of your data. When you create a global secondary index on your table, DynamoDB creates <em>completely separate partition infrastructure</em> to handle your global secondary index:</p>, <p><img alt="DynamoDB partitions with GSI partitions" class="img_ev3q" loading="lazy" src="https://user-images.githubusercontent.com/6509926/179994987-93d834ee-6e3b-46c7-b903-7f8a515ba72f.png"/></p>, <p>Note that this separate infrastructure for secondary indexes is different than how other NoSQL databases handle it. With MongoDB, for example, additional indexes are implemented on each of the existing shards holding your data.</p>, <p>Why does DynamoDB place global secondary indexes on new partitions? It all flows from <a href="https://www.alexdebrie.com/posts/dynamodb-no-bad-queries/" rel="noopener noreferrer" target="_blank">DynamoDB's laser focus on <em>consistent, predictable queries</em></a>.</p>, <p>DynamoDB wants your response times to be consistent no matter the size of your database or the number of concurrent queries. One key way it does this is via its partitioning scheme using the partition key so that it can quickly find the right ~10GB partition that holds your requested data.</p>, <p>But a global secondary index can have a different partition key than your main table. If DynamoDB didn't reindex your data, then a query to a global secondary index would require querying <em>every single partition in your table</em> to answer the query. This is exactly what MongoDB does -- if you have a query that does not use the shard key on your collection, it will <a href="https://www.mongodb.com/docs/manual/core/sharded-cluster-query-router/#broadcast-operations" rel="noopener noreferrer" target="_blank">broadcast to every shard in your cluster</a>.</p>, <p>Ok, so we know the choice DynamoDB makes in placing your global secondary indexes on separate partitions. Why does this mean we can't get strong consistency from these separate partitions?</p>, <p>Recall our tradeoff between latency and consistency. To get strong consistency from a given node, we must ensure the write is committed to the node before the write is acknowledged. Thus, our table would need to not only write to the primary and one replica of our main table, but also two nodes for each global secondary index as well!</p>, <p>With one global secondary index, that might not sound so bad -- what's a few milliseconds between friends? But you can have <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/ServiceQuotas.html#limits-secondary-indexes" rel="noopener noreferrer" target="_blank">up to 20 global secondary indexes by default</a>, and you can even request a limit increase to have more!</p>, <p>You don't want your writes hung up on acknowledgements from 10+ nodes, each of which has their own provisioned capacity and instance failures. This would vastly reduce the benefit of global secondary indexes.</p>, <p>Instead, DynamoDB uses asynchronous replication to global secondary indexes. When a write comes in, it is not only committed to two of the three main table nodes, but it also adds a record of the operation to an internal queue. At this point, the write can be acknowledged to the client. In the background, a service is processing that queue to update the global secondary indexes.</p>, <p><img alt="Eventual Consistency - Partitions -- GSI with replication queue" class="img_ev3q" loading="lazy" src="https://user-images.githubusercontent.com/6509926/179996029-06ec76eb-250f-4030-9661-bb438c92a21d.png"/></p>, <p>Notice where global secondary indexes fall on the latency-consistency spectrum. They optimize heavily for write latency at the expense of consistency.</p>, <p><img alt="ELC Spectrum -- replication to GSI" class="img_ev3q" loading="lazy" src="https://user-images.githubusercontent.com/6509926/179996241-3da4b907-e7c1-44f0-a247-9f16a35cc6a2.png"/></p>, <p>Further, and more importantly, notice the difference in flavor of eventual consistent reads on your main table as compared to reads from your global secondary index. The replication lag on your main table is likely to be barely perceptible in the normal course of business, whereas the replication lag for global secondary indexes will be more noticeable.</p>, <p>Finally, the replication lag on global secondary indexes can be influenced by <em>your own actions</em>. Global secondary indexes have capacity that is separate from your main table. If you don't provision enough capacity, or if you exceed <a href="https://www.alexdebrie.com/posts/dynamodb-limits/#partition-throughput-limits" rel="noopener noreferrer" target="_blank">partition throughput limits</a> for your index, the lag can be even longer. This is different from your main table where exceeding provisioned capacity or partition throughput limits will result in an error on the write request itself.</p>]
['<h3>Eventual consistency on DynamoDB Global Tables\u200b</h3>', <p>When I first conceived of this post, I was going to discuss the <em>Two</em> Flavors of Eventual Consistency in DynamoDB, as I wanted to make the key point about the difference in replication lag between your main table and global secondary indexes. But as I started to write, I realized there's a <em>third</em> flavor of eventual consistency in DynamoDB -- that of cross-region replication using DynamoDB Global Tables.</p>, <p>As a quick summary: DynamoDB Global Tables are a fully managed way to replicate your tables across multiple regions. This can be used for both the redundancy reason for replication, as you have some resiliency in the face of region failure, as well as the "get data closer to users" reason, as users can be routed to the region nearest them.</p>, <p>In a way, the infrastructure for DynamoDB Global Tables is similar to the infrastructure for global secondary indexes. When a write is received in one region, it is durably committed to two of the three nodes on the main table for the region and the write is acknowledged to the client. Then, it is asynchronously replicated to separate infrastructure. Rather than being a different set of partitions in the same region, the target infrastructure is a separate table in a different region.</p>, <p><img alt="DynamoDB Partitions -- Global Table" class="img_ev3q" loading="lazy" src="https://user-images.githubusercontent.com/6509926/179996637-04251ebb-e897-46d5-9bbc-cbd75b539ab8.png"/></p>, <p>Similar to global secondary indexes, Global Tables optimize for latency on the latency-consistency spectrum. However, there are two additional notes to consider as compared to global secondary indexes.</p>, <p>First, the replication latency to Global Tables is likely to be longer than that to global secondary indexes. Regions are significantly further apart than instances in the same datacenter, and network latency starts to dominate. <a href="https://www.cloudping.co/grid/p_99/timeframe/1D" rel="noopener noreferrer" target="_blank">P99 latencies across regions</a> can easily be 100 - 200 milliseconds, so you should anticipate your replication being affected accordingly.</p>, <p>Second, using Global Tables introduces write-based consistency issues into your application. I mentioned earlier that eventual consistency is mostly a read-based problem, but that's not the case with Global Tables. You can write to both regions, and writes will be replicated from one region to another.</p>, <p>This can result in a few different problems in your application, which we'll talk about in the next section.</p>]
['<h2>Dealing with the effects of eventual consistency\u200b</h2>', <p>Now that we know the different types of eventual consistency in DynamoDB, let's talk about some strategies for dealing with eventual consistency. As always, you must understand the requirements of your application to determine which strategies work best for you.</p>]
['<h3>Know the general latency characteristics for your flavor of eventual consistency\u200b</h3>', <p>As discussed above, there are three flavors of eventual consistency in DynamoDB, and these flavors have very different replication lags. You may find that, practically, you rarely see the effects of eventual consistency in your application.</p>, <p>A common preference around consistency is something called <a href="https://jepsen.io/consistency/models/read-your-writes" rel="noopener noreferrer" target="_blank">"read your writes"</a>. This basically says that if a process makes a write and later performs a read, that read should reflect the previous write. It doesn't offer guarantees about reads from <em>other</em> processes, but it at least offers some sanity for a single workflow.</p>, <p>If your database allows you to direct operations to specific nodes, then a process can handle this by reading from the same server from which it wrote. With a load-balanced NoSQL database like DynamoDB, you don't get a choice from which server you read from. However, we can get a sense of how likely it is that I will get "read your writes" behavior from our different flavors of eventual consistency.</p>, <p>I did some basic testing of "read your writes" behavior against your main table and a global secondary index. You can read the <a href="https://github.com/alexdebrie/dynamodb-performance-testing/tree/master/consistency-performance" rel="noopener noreferrer" target="_blank">full results and methodology in the GitHub repo here</a>, but the high-level takeaways are:</p>, <p>As you can see, even an eventually consistent read against a main table is going to get darn close to 'read your write' consistency in the vast majority of cases.</p>]
["<h3>... but don't count on the general latency characteristics!\u200b</h3>", <p>While understanding the general characteristics is useful, you shouldn't <em>rely</em> on this. As Ben Kehoe notes:</p>, <p dir="ltr" lang="en">Ask me in person sometime about the ticket our devs opened after they built in an assumption of a maximum time for eventual consistency on DynamoDB</p>, <p>The tests above are very basic tests in standard operation. You shouldn't expect these results in all scenarios.</p>, <p>I would expect the main table results to be more bounded than the global secondary index results, given that you still need 2 out of 3 to commit the write, and a flapping third node is likely to be replaced quickly if needed. In fact, a <a href="https://www.usenix.org/system/files/atc22-elhemali.pdf" rel="noopener noreferrer" target="_blank">new paper on DynamoDB</a> talks about how DynamoDB uses log-only replicas to keep write latency low and durability high in the event of partition failures.</p>, <p>A global secondary index, on the other hand, has more potential failure modes, including ones of your own making (underprovisioned throughput or exceeding partition throughput limits).</p>]
['<h3>Use write conditions to guarantee consistency\u200b</h3>', <p>As mentioned in the background, eventual consistency is <em>mostly</em> a read-based problem (check the next point for a caveat here). All writes to DynamoDB are happening against the latest version of the item. When making a write, you can include a ConditionExpression in your operation that must evaluate to True in order for the write to process.</p>, <p>The canonical example of potential problems with eventual consistency is a bank transaction -- what happens if I read a bank balance that is out of date, then approve a transaction based on the stale balance?</p>, <p>You can avoid this problem with the use of ConditionExpressions. Your ConditionExpression could require that the balance of the account is greater than the amount of the requested transaction in order to proceed. You can even combine multiple operations, each with their own ConditionExpressions, into a <a href="https://www.alexdebrie.com/posts/dynamodb-transactions/" rel="noopener noreferrer" target="_blank">DynamoDB transaction</a> in order to make these changes to multiple bank accounts with ACID semantics.</p>, <p>Amazingly, you're not necessarily going to avoid these issues with a relational database! Todd Warszawski and Peter Bailis have a <a href="http://www.bailis.org/papers/acidrain-sigmod2017.pdf" rel="noopener noreferrer" target="_blank">fun paper showing how isolation levels below serializable can result in concurrency bugs during transactions</a>. Peter Bailis and others have another <a href="http://www.bailis.org/papers/feral-sigmod2015.pdf" rel="noopener noreferrer" target="_blank">fun paper showing how ORMs implement concurrency-control mechanisms that fail in subtle ways</a>. I'm not saying this to scare you or raise FUD, but more to iterate that you really need to understand your infrastructure and your dependencies to avoid these issues altogether.</p>, <p>Write conditions won't save you from all your problems with eventual consistency, but they can help you avoid persisting data in inconsistent or invalid states. Look into strategies like pessimistic concurrency (aka locking), optimistic concurrency (aka version checking), or <a href="https://www.alexdebrie.com/posts/dynamodb-condition-expressions/" rel="noopener noreferrer" target="_blank">simply asserting data constraints</a> in your write operations.</p>]
['<h3>Avoid multi-region writes where possible\u200b</h3>', <p>Let's address the caveat from the previous section. While eventual consistency is mostly a read problem, it can become a write problem if you're using DynamoDB Global Tables. Now you have multiple regions, each able to accept writes that are asynchronously replicated to other regions.</p>, <p>There are two sources of potential issues with writing to multiple regions.</p>, <p>First, you could lose writes that occur nearly simultaneously. Imagine a write in Region A that updates attribute X, while a concurrent write in Region B updates attribute Y. In theory, both of these writes can succeed, and DynamoDB will do its best to reconcile concurrent writes. However, you could find yourself losing updates in specific situations.</p>, <p>Second, you could assert write conditions that are true in your region's copy of the data but that are false in another region's (yet-to-be-replicated) copy. Condition expressions and DynamoDB Transactions are consistent in a single region only, so writing in multiple regions can add problems.</p>, <p>When using DynamoDB Global Tables, I generally recommend users only write an item in a particular region. For many, that means choosing a "write" region and directing all writes there, even if the latency is higher. For some use cases, individual items may naturally be correlated with specific regions and thus you can ensure that each write will only ever be written in a single region.</p>]
