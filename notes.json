[
  {
    "neutralised_chunk": "Ensuring security for CI/CD pipelines is generally carried out through the use of dedicated roles rather than IAM users - one role per pipeline. The person or entity allowed to assume these roles has become a matter for consideration. Several 3rd-party providers, including GitHub Actions, offer broad support for identity federation, meaning there's no longer a necessity to store IAM credentials in CI/CD tools, significantly reducing the risk of security breaches.\n\nHowever, despite these precautions, vulnerabilities still exist. For instance, pipelines can be compromised via supply chain attacks, such as through corruption of relied-on Docker images within the CI/CD pipeline, or violation of static analysis tools like eslint. Consequently, the ideal degree of limitation on permissions for a CI/CD role is a recurring concern at workshops such as Production-Ready Serverless. \n\nA typical initial response might be to restrict the CI/CD role to its absolute minimum requirements as part of adhering to the principle of least privilege. Let's examine two typical strategies for achieving this limitation:\n\n1. Beginning with no permissions and continually adding them until the role can deploy the application.\n2. First bestowing the AdministratorAccess policy and allowing the role universal scope. Then, tracking the role's activities using CloudTrail over a certain timeframe. Finally, compiling a specifically tailored IAM permissions set based on the gathered CloudTrail data.\n\nWhile the first method seems exhaustive and inefficient and is suggested to be avoided; Much to their discredit, both strategies possess significant long-term disadvantages.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "One might think to limit the CI/CD role to its necessity in complying with the principle of least privilege. This goal can be attained through various means, here are two prevalent methods:\n\n1. Begin with zero permissions and progressively grant the role permissions until it can deploy the application.\n2. Initiate with an AdministratorAccess policy permitting the role to perform all actions. The actions carried out by the role over a period can be logged using CloudTrail, and this data can be utilized to develop a customized IAM permissions set.\n\nThe initial method is highly labor-intensive and time-consuming, hence best to be avoided. However, both methods are flawed and pivotal drawbacks emerge in the long run. \n\n## Unexpected Rollbacks\n\nRollbacks in CloudFormation seldom occur, particularly for individuals who do not manually script CloudFormation templates. Therefore, when a deployment fails for the first time, it exposes that the CI/CD role lacks many permissions. This situation can create complications leading to stagnation of your stack in the ROLLBACK_FAILED state. You can only resume once the permissions deficiencies are addressed. Essentially, establishing a least privileged CI/CD role demands double effort as it is also necessary to account for all rollback permissions.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more serious. Convert it to a more serious style. Take the fun out of it."
  },
  {
    "neutralised_chunk": "## Unanticipated rollback frustrations \n\nCloudFormation rollbacks do not commonly occur for several individuals, more so for those who do not manually write CloudFormation templates. As such, when a deployment error occurs for the initial time, there emerges the revelation that there are numerous permissions lacking in the CI/CD role, an uncomfortable scenario. This situation can result in the stack being mired in a ROLLBACK_FAILED state, leaving you with a challenge until the permissions' discrepancies are addressed. Ultimately, the conception of a least privilege CI/CD role is double the anticipated effort, as there's a need to consider all the permissions for potential rollbacks.\n## Impact on productivity \n\nArchitectures dynamism is fundamental, where it needs to adapt to the application and business demands. The introduction of any new service necessitates even a revision of the CI/CD role. As an example, prior to utilizing OpenSearch or Bedrock, necessary adjustments to the CI/CD role are required. Where the application team does not have oversight over the CI/CD role within organizations, this process can significantly impact productivity. It can result in frequent interactions between teams, impeding productivity. Moreover, all services may undergo this process, as each has their distinct CI/CD pipeline. This ongoing friction is essentially a burden on productivity, causing issues such as stifling creativity and the lagging integration of new services into the tech stack. All because any new service integrates needs changes to the CI/CD permissions, leading to unnecessary bureaucratic processes.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "In order to utilize OpenSearch, it is necessary to update the CI/CD role. The same requirement applies to the use of Bedrock. Particularly in larger businesses where the application team does not oversee the CI/CD role, this can create a significant drawback to productivity. It often necessitates extensive discussions between teams, slowing down work progress. Additionally, it is typically required for each individual service, as each service has its own CI/CD pipeline. This bureaucratic process imposes a permanent burden on productivity. It stymies innovation and impedes the implementation of new services into the technical stack, as the initiation of a new service calls for new CI/CD permissions.\n\nAdopting least privileged CI/CD roles for security reasoning can create a false sense of safety. These roles serve to minimize the potential damage of a security breach. By safeguarding the CI/CD pipeline, it ultimately limits what a hacker can accomplish. The CI/CD role, nonetheless, usually requires the deployment of IAM roles and Lambda functions. Given a security breach, a least privileged CI/CD role does not provide sufficient protection against a hacker. The attackers could manipulate the CI/CD role to set up confused deputies in order to act according to their instructions. For instance, an attacker may use the CI/CD role to create IAM roles for them to use or Lambda functions to carry out harmful actions. These dangers can be somewhat lessened using permission boundaries, but that demands a considerable amount of work and precision. Once again, it would be needed for every CI/CD role.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "If our CI/CD pipeline gets breached, we want to lessen the potential damage the attacker can inflict. Often, the CI/CD role needs to implement IAM roles and Lambda functions, so if there's a breach, a least privileged CI/CD role may not be effective enough in preventing the attacker's actions. Attackers may exploit the CI/CD role by creating confused deputies, for instance, they could make IAM roles that they then assume control of, or design Lambda functions with harmful actions. A certain amount of risk reduction can be achieved with permission boundaries, but this demands more effort and precision, and it needs to be applied to every CI/CD role.\n\nChoosing a permissive role that's tough to misuse is recommended, instead of strictly adhering to the least privileged CI/CD roles, which with time yields less return on investment. This is because a significant amount of output is needed to manage improbable security risks. Listed here are some suggestions to securitize a CI/CD pipeline.\n\n- Assign a distinct AWS account to each environment. Everyone should practice this, as it provides protection between environments. The damage of a breach is limited to within the account boundaries. If your dev environment is infiltrated, it implies your production data still remains inaccessible to the eavesdropper.\n  \n- Particularly for sizable organizations, further measures may be helpful. For instance, allocating at least one account for every team in each environment can prevent the impact of one team's mistakes on others.\n  \n- Crucial business systems might benefit from having personalized accounts also \u2013 one per service for each controlled environment. It isn\u2019t always required, but it would save important systems from the impact they could suffer due to any problem faced by other systems.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more casual. Consider using a bullet point format."
  },
  {
    "neutralised_chunk": "Environments should be divided from each other for security purposes. In the event that one environment is compromised, it would be restricted from impacting other environments notably your production data, due to the account boundary. In bigger organisations, there should be at least one account for each team per environment serving as a protective measure. For essential business systems, a separate set of accounts is advisable.\n \nUtilise Service Control Policies (SCPs) to prevent access to unused resources. Monitor resources that aren't in use and deny all activities related to them which decreases possible targets for attacks.\n\nAttribute-Based Access Control (ABAC)'s effectiveness continues to progress and several services now support aws:ResourceTag/${TagKey} and aws:PrincipalTag/${TagKey} decisions. They can be used to block attackers from accessing and generating resources. They can be used in the IAM permissions of the CI/CD role or implemented through a permission boundary. \n\nFor added security, the capability of the CI/CD role to self-identify should be denied. This makes it difficult for the attacker to identify the required tags to generate new resources. \n\nA high level of vigilance should be maintained. Specific services that may become subjects to malicious code execution or privilege escalation by the attackers include Lambda, EC2, ECS, IAM, CodePipeline and CloudFormation. Most of these services support the aws:ResourceTag/${TagKey} condition which prevents the attacker from generating new resources.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more casual. Consider using a bullet point format."
  },
  {
    "neutralised_chunk": "The enforcement of my idea occurs via a permission boundary. This could be further improved by making sure that the CI/CD role can't obtain information about itself in order to better protect it from attackers. This prevents them from understanding what tags are attached to the CI/CD role, thereby restricting their ability to create new resources through this role. This greatly reduces the chances of them being able to access our resources. \n\nYet the above only prevents one mode of attack. The attacker still has the ability to execute malicious codes or escalate their privileges through several different services such as Lambda, EC2, ECS, IAM, CodePipeline and CloudFormation. Thankfully though, all these services and quite a number of others support the `aws:ResourceTag/${TagKey}` feature, another line of defense that could help protect against attackers creating fresh resources in their bid to access our systems. \n\nHaving done all this to protect our resources and operations, we actually get quite a good few benefits: \n\n- We cut down on the potential areas from which we could be attacked by eliminating unused regions and services. \n- A possible attack from any direct exploitation becomes limited to specific teams and services. \n- The CI/CD role is far more difficult to misuse by attackers due to the requirement for resources to be correctly tagged. Plus, the CI/CD role's inability to describe itself further obstructs an attacker's ability to easily figure this out. \n\nNow we can permit ourselves to have a more permissive CI/CD role that's less of a hassle to create and even maintain. This is because even with a more open CI/CD role, it becomes less attractive for misuse, as it's more difficult to totally exploit it due to the safeguards in place.\n\nWhen I say 'permissive', I'm not suggesting going fully with 'AdministratorAccess.' Yet, you are permitted to largely allocate read and write access to virtually all services you use, rather than spending a lot of time refining the specifics down to the teeth.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more casual. Consider using a bullet point format."
  },
  {
    "neutralised_chunk": "AWS Step Functions and Lambda functions serve different purposes, although they can be used in conjunction. Step Functions is a service for orchestrating and coordinating multiple AWS services, including Lambda functions, to build serverless applications. Lambda functions, on the other hand, are used for running code in response to events or requests.\n\nUsing Step Functions for orchestration can provide benefits such as:\n\n1. Separating the workflow logic from the business logic, making it easier to maintain and update.\n2. Visualizing and monitoring the workflow execution, which can aid in debugging and troubleshooting.\n3. Handling retries, error handling, and state management, which can be complex to implement in code.\n4. Integrating with various AWS services, allowing for complex workflows spanning multiple services.\n\nHowever, Step Functions may introduce additional complexity and overhead, making it less suitable for simple use cases. In such cases, implementing the entire orchestration logic within a Lambda function might be more straightforward.\n\nThe decision to use Step Functions or handle orchestration within a Lambda function depends on factors like the complexity of the workflow, the need for visibility and monitoring, the number of services involved, and the team's familiarity with the services. Simple workflows with few steps and services involved may be easier to implement and maintain within a Lambda function, while complex workflows with multiple decision points, retries, and service integrations could benefit from using Step Functions.",
    "style": "Style: Vary the original vocabulary, tone and prose. Convert it to a less coherent draft style."
  },
  {
    "neutralised_chunk": "# Comparing Step Functions and Lambda Function \n\nAWS Step Functions offer workflow orchestration solutions for various tasks like payment processing and map-reduce jobs. This service requires additional learning and costs and adds complexity to your systems due to:\n- Difficult testing procedures.\n- Partition of business logic between configuration and code.\n- A necessity to choose between Express Workflows or Standard Workflows.\n\nAn alternative to using Step Functions is incorporating all the orchestration within the Lambda function. Here's an evaluation of the pros of this approach.\n\n## Pros of Using Lambda\n\n1. Performing all orchestration processes in code can be simpler. Familiarity is guaranteed since every aspect one can achieve with Step Functions can be replicated with a few code lines. An example is as follows:\n\n```javascript\nmodule.exports.handler = async (event) => {\n    // error handling\n    try {\n        await doX()\n    } catch (err) {\n        // handle errors\n    }\n    // branching logic\n    if (condition === true) {\n        await doSomething()\n    } else {\n        await doSomethingElse()\n    }\n    // parallelism\n    const promises = event.input.map(x => doY())\n    const results = await Promise.all(promises)\n    return results\n}\n```\n\n2. This may be more economical. Step Functions would typically use Lambda for Task states. Hence, you will end up paying for both Lambda invocations and Step Function state transitions. State transitions cost $25 per million, making it one of the more expensive AWS services. Paying for two services could be costlier than a single one.\n\n3. It may offer better scalability. Using both Step Functions and Lambda functions exposes you to the throughput limits of both services. Step Functions have certain limitations on the cumulation of state transitions and execution starts per second. The Lambda function only limits you by the concurrent executions limit, regular throughput limitations on the no. of concurrent executions. These cost and scalability points, however, depend on the specific architecture. Planning can make them moot. Recent scaling changes on Lambda reinforce this.\n\nThe use of Step Functions versus orchestrating everything in a Lambda function warrants consideration and holistic analysis.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more serious. Convert it to a more serious style. Take the fun out of it."
  },
  {
    "neutralised_chunk": "The constraints of using both Step Functions and Lambda functions lie in the throughput limits of both services. The limits on the number of state transitions and executions initiated per second are modest in Step Functions Standard Workflows. However, these limits can be increased. Proper planning negates these issues. Without Step Functions, the constraining factor is the limit on concurrent executions on Lambda. By default, Lambda has limits on the number of concurrent executions, but these can also be adjusted through planning. Recent scaling adjustments for Lambda helps to achieve desired scalability. The considerations for cost and scalability are situational and influenced by various architectural choices.\n\nDisadvantages of Lambda:\n\n1. Existence of cold starts.\n2. Maximum duration is restricted to 15 minutes.\n3. Paying for execution time by the millisecond makes it a less than optimal solution when the function is waiting for something to finish, as this time and money is wasted.\n\nAdvantages of Step Functions:\n\n1. The ability to visualize workflows simplifies troubleshooting, making it accessible even to non-technical team members such as product owners or customer support teams.\n2. The built-in audit history includes comprehensive data such as dates and times when a state started, inputs, outputs, error messages, and stack traces.\n3. Direct service integration with almost all AWS services makes it possible to implement a full workflow without any Lambda functions.\n4. Not using Lambda avoids cold starts, leading to more consistent performance.\n5. Execution time is extended, and a Standard Workflow can run up to a year.\n6. Callback patterns are beneficial for supporting human interventions in a workflow.\n7. Standard Workflows are a cost-efficient way to wait as you\u2019re charged only for the state transitions and not the duration.\n8. More robust error handling can be implemented, essential for operations critical to the business.",
    "style": "Style: Vary the original vocabulary, tone and prose. Convert it to a less coherent draft style."
  },
  {
    "neutralised_chunk": "A common error people often make when using Kinesis with Lambda",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more casual. Consider using a bullet point format."
  },
  {
    "neutralised_chunk": "Title: Common Mistake in the Use of Kinesis Combined with Lambda\n\nMany developers harness AWS Kinesis and Lambda in tandem to handle considerable data quantities in real time processing. However, a typical error frequently occurs when fusing these two services.\n\nThere exist recommended practices for configuring the EventSourceMapping for Kinesis in Lambda, which include:\n- Setting up an OnFailure destination for unsuccessful records.\n- Turning on the BisectBatchOnFunctionError.\n- Surpassing the MaximumRetryAttempts. Deciding on a value which clerly permits failed messages several retries before transmitting them to the OnFailure destination.\n\nImplementing these suggestions is a forthright solution for creating a robust data processing system and mitigating the impact of unsavoury messages. Nonetheless, an integral piece of information has been overlooked: the exerted payload in the OnFailure location does not encompass the event payload.\n\nThe OnFailure location is anticipated to capture the following type of data:\n\n```json\n{\n  \"requestContext\": {\n    \"requestId\": \"cf6fa2a6-48e2-49a6-bc23-7ed2b154b176\",\n    \"functionArn\": \"arn:aws:lambda:us-east-1:xxx:function:kinesis-processor\",\n    \"condition\": \"RetryAttemptsExhausted\",\n    \"approximateInvokeCount\": 4\n  },\n  \"responseContext\": {\n    \"statusCode\": 200,\n    \"executedVersion\": \"$LATEST\",\n    \"functionError\": \"Unhandled\"\n  },\n  \"version\": \"1.0\",\n  \"timestamp\": \"2023-12-10T22:06:23.446Z\",\n  \"KinesisBatchInfo\": {\n    \"shardId\": \"shardId-000000000000\",\n    \"startSequenceNumber\": \"4964723877401875844914379084747495239034204468389137\",\n    \"endSequenceNumber\": \"496472387740187584491437909411123017485931487851477729\",\n    \"approximateArrivalOfFirstRecord\": \"2023-12-10T22:05:59.280Z\",\n    \"approximateArrivalOfLastRecord\": \"2023-12-10T22:06:19.196Z\",\n    \"batchSize\": 3,\n    \"streamArn\": \"arn:aws:kinesis:us-east-1:xxx:stream/MyKinesisStream\"\n  }\n}\n```\n\nThe KinesisBatchInfo pertains to the shard ID, sequence numbers, and batch size but, it doesn't keep hold of the actual data payload, the very crux of the event needed for data processing and analysis.\n\nSuch an oversight can give rise to several significant problems:\n1. Inability to troubleshoot problems easily: Leaving the actual event data out makes identifying failure modes a challenging task.\n2. Possible data loss: In case the record cannot be retrieved before it gets erased from the stream, the data is lost irrevocably. Kinesis stream's standard retention is set at 24 hours. If a failure arises on a non-working day, the likelihood of developer awareness could take until their return to work, leading to the lacking accessibility to the failure in the stream as it's no longer available.\n3. Extreme complexity levels: Developers have to formulate alternative methodologies to capture and reserve the actual event data.\n\nMany, lacking this knowledge, are taken by surprise when they meet an unsuccessful message attempt for the first time.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "In the year 2024, if you intend to develop a REST API employing serverless technologies, you need to decide between using Lambda Function URLs or API Gateway. Understanding the following benefits and drawbacks can assist you in your decision.\n\nFunction URL has several advantages:\n1. It is naturally compatible with Lambdaliths.\n2. It doesn't have the added latency of API Gateway.\n3. Costs related to API Gateway aren't an issue.\n4. It supports response streaming, which is beneficial when sending large HTTP response data packets.\n5. It has less configuration to consider.\n6. The API can operate for a maximum duration of 15 minutes.\n\nFunction URL also has several disadvantages: \n1. It necessitates the utilization of Lambdaliths and the navigating of associated issues.\n2. There are no per endpoint metrics, although this can be compensated for with custom-made metrics utilizing the embedded metric format (EMF).\n3. It doesn't integrate with WAF directly, though this can be achieved through CloudFront, the original function URL would still be devoid of protection.\n4. It only accommodates AWS_IAM authentication.\n5. Different endpoint authorizations can't be modified.\n\nThe use of Function URL would be a valid approach if you long to construct a Lambdalith voluntarily, and lack a need for the supplementary benefits that API Gateway provides. In this case, Function URLs are more affordable, faster, and entail fewer components.\nApart from these scenarios, if there is a need to transmit large data payloads (greater than 10MB) or operate for durations that exceed 29 seconds, Function URL is also a fitting choice. However, this hinges on the inability to modify the interactions between the client and the server.\nConsidering the limited functionality for authentication and authorization, Function URL wouldn't provide optimal usability for user-end APIs, which often necessitate the use of a Cognito authorizer or a customized Lambda authorizer. Therefore, Function URLs are best suited for public APIs or in-house APIs focusing on a microservices architecture. An \"in-house API\" refers to an API that is employed by other services, but is not used directly by the frontend application. These types of APIs usually require AWS_IAM authentication, as they are frequently sourced by another AWS resource - for example, a Lambda function, an EC2 instance, or an ECS task.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more serious. Convert it to a more serious style. Take the fun out of it."
  },
  {
    "neutralised_chunk": "If you are unable to refactor the interaction between client and server, this might pose a problem. Because of the minimal support for authentication and authorization, this method is not ideal for APIs that are visible to users. Such APIs will need something like a Cognito authorizer or a custom authorizer made using Lambda.\n\nThe best utilization of the function URLs is for public APIs or intra-microservices architecture APIs. When we talk about \"internal APIs,\" it refers to those APIs that are employed by other services and not directly by the frontend application. Such APIs typically need AWS_IAM authorization as the caller is another resource from AWS such as a Lambda function, ECS task or an EC2 instance.\n\n### Advantages of API Gateway\n1. It is more adaptable and works with both a multitude of Lambda functions and a single function per endpoint approach.\n2. Direct integration with the majority of AWS services is possible. This makes a Lambda function unnecessary in many situations.\n3. Proxying to HTTP APIs is feasible. This could be beneficial when integrating with APIs from third parties.\n4. Numerous other features are also offered, like:\n   - Cognito authorizer\n   - Usage plans which suits software as a service applications that have tiered pricing.\n   - Inherent validation of requests with request models.\n   - Detailed metrics for every endpoint.\n   - Creation of mock endpoints which is convenient for endpoints that return static data.\n   - Transformation of request and response which is again beneficial for integration with third part APIs.\n   - Supports WebSockets.\n   - And many more.",
    "style": "Style: Vary the original vocabulary, tone and prose. Convert it to a less coherent draft style."
  },
  {
    "neutralised_chunk": "The system is functional with both Lambdaliths and the singular function per endpoint methodology. This technology offers a direct connection with a significant majority of AWS services, eliminating the need for a Lambda function in many circumstances. It maintains the ability to proxy requests to diverse HTTP APIs, a feature which enhances integration with third-party APIs. The system also delivers numerous other capabilities, though the following are key highlights:\n- Cognito authorizer\n- Usage plans pertinent for Software as a Service applications featuring tiered pricing\n- In-built request assessment through request models\n- Detailed metrics for each endpoint\n- Mock endpoints beneficial for endpoints presenting static information\n- Transformations of requests and responses, additionally advantageous for integration with third-party APIs\n- Support for WebSockets\n\nDespite these benefits, some disadvantages include added latency overhead and increased costs. Other limitations are the inability to stream responses, integration restrictions to 29 seconds and constraint to a 10MB response limit.\n\nIn contexts where the plethora of features provided by the API Gateway is crucial, this system may be appropriate, assuming any additional costs are not restrictive. Its limitations, such as 29-second and 10MB response confines, can pose difficulties depending on the situation, although there are strategies available to address this like Decoupled Invocations and S3 presigned URLs. It should be noted that these solutions often necessitate a revision of client-server communication and won't always be feasible.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "Big businesses are now using more Large Language Models (LLMs) but this is creating issues. Running several LLMs at the same time can be a challenge. AWS is well-placed to make it easier for businesses to use Generative AI. They have the computer resources, strong security and the latest serverless designs. \n\nAWS has launched Bedrock to deal with this growing trend. Bedrock's design, which doesn't need servers and can work with any model, is perfect for this new wave of GenAI. Last week, Bedrock became available to all AWS customers, letting them see how it will can help with their operations.\n\nRunning LLMs on a large scale, especially if you're not an expert in ML/AI, can be hard. You need teams to manage the computers, balance the loads and run the APIs. Not many businesses want to put that much money in up front. \n\nAmazon Bedrock takes these problems away. When you use a serverless service, such as Bedrock, the business only pays for what the LLM uses and creates. For example, your customer service chatbot could have 100 times as many users on Black Friday than on a Monday in March. You don't need to organise extra servers as Bedrock can deal with this increase in demand and then reduce resources afterwards.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more casual. Consider using a bullet point format."
  },
  {
    "neutralised_chunk": "- LLM infrastructure management at large scale is a challenge especially for businesses without machine learning or artificial intelligence expertise. This includes managing compute, load balancers and APIs. \n- Amazon Bedrock provides a solution to these concerns as a serverless service that charges based on the tokens consumed and generated by the LLM.\n- Scaling problems are eliminated with Amazon Bedrock. An example situation would be a customer support bot that serves significantly more users on Black Friday compared to a regular weekday; in such a case, there would be no need to prepare extra servers.\n- Data governance and audit trails are crucial in present times and Amazon Bedrock ensures this with top-notch safety measures.\n- Data is encrypted both during stationary periods and while being moved. Moreover, customers can opt to use their own keys.\n- Amazon Bedrock respects privacy legislations such as HIPAA and GDPR. Importantly, provided data does not get employed for base model improvements nor does it get shared with third-party model providers.\n- AWS PrivateLink is compatible with Amazon Bedrock, allowing users to make private connections between LLMs and their Virtual Private Clouds (VPCs), protecting sensitive traffic from public internet exposure.\n- Companies achieve the security they require to create tools using LLMs, which allows them to use their own classified data archives as context.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more casual. Consider using a bullet point format."
  },
  {
    "neutralised_chunk": "Bedrock LLMs ensure encryption of all provided information during rest as well as transit, giving customers the option to utilize their own keys. It is also compliant with HIPAA eligibility and GDPR, and it never uses provided information for improving base models or sharing with third-party model providers. Amazon Bedrock also allows businesses to maintain privacy via AWS PrivateLink, enabling them to connect their LLMs and VPC\u2019s privately without exposing traffic to the public internet. This allows entities to set up tools employing their own confidential data archives for context with LLMs.\n\nOne primary aspect of Amazon Bedrock is it's developed as a comprehensive suite of tools for builders. Ease in model selection is one strong point in this offering. It supports a wide array of proprietary as well as open-source models, including Amazon\u2019s Titan models. It can meet various use-cases, such as long form text generation, quick summarisation, or conversational discourse. Bedrock further more has in place a playground to test and fine-tune different models. It will also introduce Meta\u2019s Llama 2 model through a Serverless API, which is an attractive feature of Bedrock. In line with AWS's recent partnership with Hugging Face, and its $235 million Series D funding round contribution, it looks promising for Bedrock to include more open-source models in the foreseeable future.",
    "style": "Style: Vary the original vocabulary, tone and prose. Convert it to a less coherent draft style."
  },
  {
    "neutralised_chunk": "You can choose a model based on the task you need to accomplish, such as generating long articles, summarising texts, or engaging in conversation. Bedrock provides a platform that lets teams experiment with different models and find the best prompts for the model they choose. The fact that it integrates Meta's Llama 2 model with a Serverless API makes Bedrock distinct. After AWS invested a lot of money in Hugging Face and their recent funding round, we anticipate that more open-source models will be integrated into Bedrock in the near future. \n\nSelf-sufficient agents for Bedrock are being demoed. Agents are skilled in automating tasks typically performed by humans, like making pitch decks, responding to emails, and coding. Companies such as Canva and Adobe have already incorporated GenAI to modify image sizes, eliminate backgrounds and items. It\u2019s predicted that external style guides will eventually become parameters for these creations. With a handful of notes, these tools can generate slides, flyers, and other documents. Generating codes is also getting simpler with increasingly accurate outcomes for more complicated use-cases. In recent times, there has been remarkable progress in areas including AI code assistants, code generation via test-driven development, and autonomous code reviews. \n\nWhile the results produced by the independent agents might not be flawless, a 70% accuracy rate is still considerably helpful. The custom of paying hefty amounts for analysts to do simple tasks like adjusting the colours on slide decks may soon be a thing of the past.",
    "style": "Style: Vary the original vocabulary, tone and prose. Convert it to a less coherent draft style."
  },
  {
    "neutralised_chunk": "Utilizing a specific set of instructions, these instruments are capable of producing items like presentation slides, flyers and diverse types of materials. The procedure of generating code is progressively becoming more straightforward as there is an increasing emergence of competent single shot accuracy scenarios for more intricate applications. Advances in this particular field have accelerated rapidly because of the prevalence of tools such as AI code facilitators, code generation impelled by test-driven development, and autonomous code assessments that are getting much recognition.\n\nDespite the conceivable imperfections in the results delivered by these systems, an approximate accuracy rate of 70% still effectively reduces the time consumed by the human workforce. With these developing technologies, delegating repetitive tasks such as color adjustments on presentation slides to analysts for large compensations may soon be viewed as unnecessary.\n\nLanguage Lab Model (LLMs) operates at their optimal basics when they receive training based on the generic context of the defined task and have the ability to rephrase accordingly. If they lack the in-depth comprehension of the specific idea\u2014for example, certain exclusive practices peculiar to your establishment\u2014in the ideal scenario, the model would need systematic fine-tuning to cater to your unique needs. Users can conveniently make any necessary modifications to any of the Bedrock LLMs directly in the console using data acquired from S3.\n\nCertain justifiably rational arguments advocate the utilization of Bedrock over OpenAI. Both platforms exhibit a myriad of models and showcase a serverless cost per token pricing model. They equally support the feature of fine-tuning directly from the console. Nevertheless, Bedrock has an added advantage of supporting models from an extensive array of providers coupled with a much more comprehensible data governance policy. Drawing benefits from the massive AWS ecosystem, Bedrock allows seamless integration with ancillary services like Lambda for computation, OpenSearch for vectors, and S3 for object storage. On the aspect of pricing, Bedrock evidently wins over OpenAI. For instance, 1k tokens utilizing Claude 2 through Bedrock would approximately cost $0.01102 for inputs and $0.03268 for outputs, in contrast with OpenAI (GPT4 32k context) that charges $0.06 for inputs and $0.12 for outputs. \n\nIn some rigid situations where a user has a complex requirement, they might opt for OpenAI\u2019s GPT models, particularly if they have significant value for their prompts or use function calling where the LLM explicitly returns JSON. In such special circumstances, considering OpenAI could be a suitable option. Alternately, the transition to Bedrock is seemingly effortless, especially if your application relies on a library function such as LangChain, which inherently possesses drop-in replacement compiler for Bedrock.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "This post is about Lift, an open-source tool for deploying serverless applications more easily. It's a Serverless Framework plugin that aids this deployment process. During operation, an application may need to interact with third party SaaS providers. To facilitate this interaction, a webhook HTTP endpoint can be included in the application. This allows for notification from the network of external applications which the application can take actions upon. In the coming weeks, we plan to release the first beta of Lift, which will include a deep dive on Lift's 4th component, webhooks. This will include details on deploying production-grade HTTP webhook endpoints and how Lift is going to aid in accomplishing this complicated process.\n## The early concepts\n### Elementary webhook endpoint structure\n\nAPI Gateway facilitates serving of HTTP endpoints in the serverless application. It is an Amazon-managed service which can handle tasks such as authorization and input validation, with native support for Lambda integration. Consequently, distinctive HTTP requests can trigger your code via a specific application path. The use of the Serverless Framework to announce a webhook endpoint is straightforward, involving just a minimum lines of code configuration requirement:\n\n```yaml",
    "style": "Style: Vary the original vocabulary, tone and prose. Convert it to a less coherent draft style."
  },
  {
    "neutralised_chunk": "API Gateway is a managed service from AWS that provides functionality for exposing HTTP endpoints in a serverless application. It handles tasks such as authorization, routing, and input validation. API Gateway can be natively integrated with AWS Lambda, allowing your code to be triggered whenever an HTTP request is received at a specific path of your application. To deploy a webhook endpoint using the Serverless Framework, configuration is required in the serverless.yml file, where the function is defined along with the HTTP method and path for the event trigger.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more serious. Convert it to a more serious style. Take the fun out of it."
  },
  {
    "neutralised_chunk": "The architecture of a simple webhook endpoint is integrative with API Gateway serving numerous purposes such as facilitating HTTP endpoints in a Serverless application. It is an AWS-managed system adept at managing authorization, routing, and input validation aspects. API Gateway can be integrated with Lambda that causes the code to be prompted whenever an HTTP request approaches a particular path of the application. Deploying a webhook endpoint with the Framework of Serverless simply requires a few configurations.\n\nDespite the above approach working sufficiently, several characteristics pose to be missing. These criteria are imperative for maximising the efficiency of third-party apps notifiers that expect the webhook endpoints to be available, swiftly responsive, and acknowledge all notifications. Points to necessarily consider include the separation of Transport/Processing tasks which means authenticating, validating, parsing acknowledgments, and processing notifications should be assessed as distinct responsibilities. As long as the message is recognized, a 200 response should inevitably return from the API. In addition, an asynchronous decoupling needs to be ensured such that the application is not overly burdened with the influx of notifications. Implementations to optimise costs service to hold restraint on infrastructure invocation for all arriving webhooks. Thus, reducing the risk of Denials of Wallet, efficiently encouraging the application of filtering to process desired notification types. The management of errors ensures that processing errors aren't reached by the notifier on attribution to Transport/Processing separation. Still, mechanisms must be devised for keeping unsolved message from getting forever lost. Lastly, prompt notice of an unable-to-authorize notification needs to be assured. It indeed could be arising from a malicious endpoint attack or from a wrongful secret configuration causing check notification signature discrepancies.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "It is advisable to not engage all your infrastructure for every incoming webhook. This action will decrease potential wallet denials and allow easier application filtering to process preferred notification types. Error control: Any processing errors should not reach back to the notifier due to the boundary of transportation and processing. Nevertheless, there must be a method present to ensure not all unprocessable messages are irretrievably lost. Observability: You must be alerted if there is a surplus of notifications that you cannot authorize. This could be an erroneous attack on your endpoint or incorrect secret configuration checking the notification signature.\n## An approach suitable for production\n### Serverless webhook endpoint structure ready for production\n\nHere is an intro to a stripped-down `serverless.yml` configuration incorporating these good practices:\n\n```yaml# serverless.yml# ...\nprovider:\n  eventBridge:\n    useCloudFormation: true# You can use native Serverless framework functionalities to define your processors",
    "style": "Style: Vary the original vocabulary, tone and prose. Convert it to a less coherent draft style."
  },
  {
    "neutralised_chunk": "Here's a more casual, neutral version of the text:\n\n- The team had to build a highly scalable chat app in just 4 weeks due to a surge in demand for their client's video conferencing service during the COVID-19 lockdown.\n- The chat app needed to handle 250,000 participants per video event.\n- It had to be released quickly to handle the increased traffic.\n- The existing development team needed to maintain and iterate the chat system.\n- Real-time communication was required, so websockets were used instead of long-polling for better performance and cost-efficiency at scale.\n- AWS AppSync was chosen as it's a serverless GraphQL interface that can connect to different data sources.\n- It offered speed of delivery, scalability, and ease of use.\n- API Gateway websockets weren't viable due to the need for mass message broadcasting.\n- AppSync's GraphQL interface enabled rapid frontend development.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more casual. Consider using a bullet point format."
  },
  {
    "neutralised_chunk": "Websockets were chosen over long-polling due to their advantages in cost efficiency and performance when scaled up. AWS AppSync, a fully-managed Serverless GraphQL interface to a range of different data sources, was used for its speed of delivery, scalability and user-friendliness. API Gateway websockets were also assessed but were not practical due our mass message broadcasting needs. The drawback is that there is no capability to broadcast messages to all connected clients with a single API call- each connection requires an individual API call. Beside this, the GraphQL interface of AppSync facilitates speedy frontend development.\n\nIn relation to the architecture, its simplicity strengthened the delivery speed of our chat application and provided a definitive set of technologies for the existing team to get acquainted with. AppSync is central to the architecture and it integrates with Cognito user pools for authentication purposes. DynamoDB is also utilized as our data source. \n\nThis all connects to our React frontend that employs the Apollo Client to dispatch our requests to the GraphQL server from the frontend. The Serverless Framework was utilized for managing the Infrastructure as Code (IaC), whereas amplify provided the SDK for frontend interactions with backend deployed services.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "## Architecture\nWe chose a simple architecture for our chat application, which allowed for a fast delivery rate, given the technology requirements that needed to be handled by our existing team. \n\nOur architecture uses AppSync, integrated with Cognito user pools for authentication needs. DynamoDB serves as our data source.\n\nThe architecture connects to the React frontend via Apollo Client which sends requests to the GraphQL server. Serverless Framework was utilized to manage the infrastructure as code (IaC), and amplify provided the SDK needed for frontend-backend communication.\n## DynamoDB\nSince DynamoDB is a NoSQL Serverless Database, it satisfied our needs for scalability without server management. It's used by large corporations like Airbnb and Samsung due to its capability to handle high velocity demands, which lends credibility. DynamoDB streams were also useful for analytics integration. \n\nAppSync's integration with DynamoDB is an advantage, as it reduces set-up time and thus speeds up the delivery process. \n\n```\nDynamoDB Schema for basic Serverless chat application\n```",
    "style": "Style: Vary the original vocabulary, tone and prose. Convert it to a less coherent draft style."
  },
  {
    "neutralised_chunk": "## DynamoDB\nDynamoDB, a NoSQL Serverless Database, offers a scalable solution without the requirement of managing servers. Big corporations such as AirBnb and Samsung utilize this service, showcasing its potential for large-scale operations. Furthermore, DynamoDB streams have been used for additional analytics integrations.\n\nThe integration of AppSync and DynamoDB is advantageous as it minimizes set-up time, boosting our rate of project completion. \n\n```\nDynamoDB Schema for basic Serverless chat application\n```\n## Understanding GraphQL\nThe usage of REST requires calls to different endpoints to receive the needed information. However, with GraphQL, one simply calls the same GraphQL endpoint.\n\nIn simpler terms, GraphQL presents a different potential way of connecting your client applications to the backend. It originated from Facebook, aiming to improve bandwidth efficiency due to the inconsistent internet connections of mobile devices. Let's compare GraphQL to REST, which most of us are familiar with, for better comprehension.\n\nWith GraphQL, we avoid calling multiple endpoints as we do with REST.\n## Queries\nQueries within GraphQL provide similar function to REST's GET requests. Queries offer a method of data retrieval (in our context, we refer to collecting data from DynamoDB, our database).\n\n```\nThe above demonstration shows a query to receive messages. A roomId is involved as an argument that we implement in our query. The purpose is to attain messages specific to that certain chat room. We then declare the return type to be an array of messages, where 'Message' is a characterizing type.\n```",
    "style": "Style: Vary the original vocabulary, tone and prose. Convert it to a less coherent draft style."
  },
  {
    "neutralised_chunk": "The following text defines that the return type is an array comprised of 'Messages', a specific type.\n```\n## Explanation of GraphQL\nUsing REST requires to contact multiple endpoints in order to receive the necessary information. However, with the use of GraphQL, there is only need to call one consistent GraphQL endpoint. \n\nTo summarize, GraphQL is simply another method of connecting client applications to the backend system. It was formulated at Facebook to enhance bandwidth efficiency, especially due to the unstable internet on mobile devices. Many people are better acquainted with REST, hence a comparison with GraphQL provides a more comprehensible understanding of the concept.\n\nIn the practice of GraphQL, rather than contacting distinct endpoints as it occurs with REST, there is only need to call upon a single endpoint.\n## About Mutations\nThe concept of mutations within GraphQL has considerable similarity to the REST POST requests. They essentially mutate data as their name suggests.\n\n```\nIn the above example, there is a demonstration of executing a sendMessage mutation, where the roomId and message constituents are fed in as arguments, so they can be added to a DynamoDB table with those specified details. The return then is a 'Message'.\n```\n## Concept of Subscriptions\nAppSync subscriptions are intriguing as they facilitate real-time updates. As for the chat application in question, the need is to \u2018subscribe\u2019 to the \u2018sendMessage\u2019 mutation event so as to ensure that the frontend application updates every time a message is sent, transferring the new message across all users. \n\n```\nExamining here, it can be interpreted that the roomId gets passed into it. This signifies that only users belonging to that specific chat room will monitor messages that are sent.\n```",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "The aim is to 'subscribe' to the mutation event 'sendMessage' in our chat application. This event enables the app to refresh when a user sends a message. The new message is universally disseminated to all active users.\n```\nTo refine it further, participants of a particular chat room get the updates. If you inspect the code, you'll spot that we used roomId variable to achieve this specification.\n```\n## Mutations\nIn GraphQL, mutations mirror REST's POST requests. As their name implies, mutations alter the data.\n```\nIn the demonstration above, we crafted a 'sendMessage' mutation. We input roomId and message as arguments and then append it to our DynamoDB table. Here, our return type is a Message.\n```\n## Resolvers\nWe apply Apache VTL when dealing with our business logic. Communication with the data source comes through resolvers. They bridge between GraphQL and the data source. They operate in Apache Velocity Template Language (VTL). Here, the resolver ingests the request and spews out a JSON document.\n## Pipeline Resolvers\nIn scenarios where numerous operations need to be conducted to decipher the GraphQL field, the pipeline resolvers feature of AppSync comes handy. It comprises 'before' mapping template, continuous functions, and concludes with an 'after' mapping template.\n```\nLike when launching a thread in response to a query, for instance, we apply pipeline resolver. We have designed MESSAGE and THREAD rows in single- table set-up. When someone responds in a thread, we constitute a THREAD row and adjust the MESSAGE \nrow to incorporate a 'repliedAt' timestamp. Each operation needs a separate request.\n```",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more casual. Consider using a bullet point format."
  },
  {
    "neutralised_chunk": "Pipeline Resolvers are utilized when multiple operations are required to resolve the GraphQL field. This feature of AppSync, the pipeline resolvers, encompass a \"before\" mapping template, numerous functions, and culminate with an \"after\" mapping template. \n\nA typical application of the pipeline resolver involves initiating a thread regarding a certain question. Single table design makes use of \"MESSAGE\", and \"THREAD\" rows. When a reply occurs in a thread, a THREAD row must be added and the MESSAGE row should be adjusted to incorporate a \"repliedAt\" timestamp. This entails two distinct requests.\n\nOn the user interface end, AWS Amplify SDK with Cognito is employed to enable user authentication. Pre-designed user interface elements for sign-in and sign-out are provided by Amplify. 'Auth' from aws-amplify along with our Cognito user pool data (deployed via the Serverless Framework) assist in configuring the authentication process. The Apollo Client is used for requests on the frontend, primarily due to its compatibility with TypeScript, creating a positive user experience, and its ability to manage state on the frontend using a cache mechanism.",
    "style": "Style: Vary the original vocabulary, tone and prose. Convert it to a less coherent draft style."
  },
  {
    "neutralised_chunk": "The testing of automated software is crucial for ensuring its quality in a continuous integration setting. Other types of testing such as checking backend logic and web interfaces, although complicated and lengthy when ensuring its precision, is nonetheless thoroughly documented and there are existing guidelines to follow. Features like PDF generation, which are often used in products designed for consumers (like order confirmation, data analysis or personalized planning documents), tend to be overlooked when it comes to testing. However, these features are often of critical importance. \n\nWe tried a new approach to test PDFs by combining Puppeteer (a headless Chrome manipulator), Jest (a testing framework for JavaScript), and a visual regression Jest plugin from American Express. The main concept of visual regression testing is comparing acceptedount screen captures of an interface to those produced following alterations to the underlying code. If deviations of pixels are detected that surpass a set limit, the builds are seen as unsuccessful. \n\nIn order to do this, we require: \n\n- Something to convert a PDF page to an image that can be evaluated using an image comparison tool. The arrangement of PDFs is unsatisfactory for comparison and it does not function well with current image diff instruments. Puppeteer possesses the ability to create a PNG for each PDF page.\n- A framework to conduct testing (Jest).\n- A technic to assess the produced screen captures to the new ones (jest-image-snapshot).",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more casual. Consider using a bullet point format."
  },
  {
    "neutralised_chunk": "Visual regression testing fundamentally involves comparing old versions of a user interface with the new ones created after any changes are made to the underlying code. A build may fail if there's a noticeable pixel difference beyond a certain anticipated range.\n\nTo perform this task, some requirements are:\n\n1. A system for creating image snapshots from PDF pages because the PDF format isn't as effective for comparisons and doesn't work well with available image difference tools. Puppeteer can be used to create a PNG of each PDF page. \n2. A framework for running tests, such as Jest.\n3. A tool to correlate the created snapshots with new ones (like Jest-image-snapshot).\n\n### Creating snapshot pictures from PDF\n\nAs a headless instance of Chrome, Puppeteer can generate visual snapshots of websites for image comparisons using its `screenshot` function.\n\n```javascript\n// Sample usage of Puppeteer's screenshot function\n```\n\nPuppeteer can launch a PDF file using Chrome's built-in PDF viewer and then create screenshots for that page. From the previous example, one could change line 6 to enable the opening of a local PDF file using file protocol.\n\nThe only problem with this approach is when there's a scrollbar on the viewer's right side. It's best to exclude this from the screenshot as Chrome's scrollbars can potentially introduce unexpected issues and delays to the tests. Fortunately, it's possible to crop the screenshot using Puppeteer's `clip` function. \n\n```javascript\n// An example how to use Puppeteer's clip parameter\n```",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more casual. Consider using a bullet point format."
  },
  {
    "neutralised_chunk": "Puppeteer has the ability to open a PDF file and create a screenshot of the page. It can be done by modifying line 6 to use a local PDF file with the file protocol based on the given example.\n\nOne drawback with this method is the scrollbar appearing on the viewer's right side. Removing the scrollbar from the snapshot can be fewer chances of inconsistencies and timing problems in the tests due to the way Chrome loads its scrollbar. Fortunately, Puppeteer has a `clip` parameter for its screenshot feature, thus allowing for screenshot cropping.\n\n```javascript\n// Demonstration of Puppeteer's clip parameter\n```\n## Regarding Jest\n\nJest is an efficient JavaScript testing framework characterized by its simplicity. It's relatively straightforward to add jest to an existing JS project; you just need to follow these commands:\n\n```bash\n// Instructions for integrating jest\n```\n\nA sample test template could be constructed as below:\n\n```javascript\ndescribe('pdf suite', () => {\n   it('works', async () => {\n      expect(1).toBe(1);\n   });\n});\n```\n\nFurthermore, there's a helpful extension provided by American Express for the Jest platform. This extension facilitates image comparison for visual regression tests. It can be added to your project by executing these commands:\n\n```bash\nyarn add -D jest-image-snapshot\nnpm i --save-dev jest-image-snapshot\n```\n\nOnce installed, Jest's built-in 'expect' object can be expanded thus:\n\n```javascript\nconst { toMatchImageSnapshot } = require('jest-image-snapshot');\nexpect.extend({ toMatchImageSnapshot });\n```",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more serious. Convert it to a more serious style. Take the fun out of it."
  },
  {
    "neutralised_chunk": "Serverless technology, among others, has been monumental in accelerating project timelines, reducing the management cost, and generally enhancing the autonomy of developers in the systems they construct. Despite these advantages, its implementation is not devoid of challenges \u2014 particularly regarding corporate management essential for fully leveraging the benefits of Cloud technology. As we increasingly orient towards Cloud-Native technology, including Serverless, certain key components demand reevaluation, such as: management-driven constraints hindering developers, and cognitive load that could be overwhelming.\n\nAddressing these issues extends beyond mere technical refinements \u2014 a blend of CDK can possibly drive the necessary socio-technical transformations.\n\nSecurity Frequently Calls for Modifications\nLarge establishments venturing into Serverless frequently encounter two major obstacles: viewing security as an upper-management hurdle that impedes budding initiative and expanded awareness about steadily rising security incidents over years, requiring a shift in tandem efforts rather than tasks traditionally passed onto the Information Security team. \n\nPrimarily Due to Security Seen As Hierarchical Hinderance\nAs businesses increasingly align towards the paradigms of Cloud and DevOps, developers are mandated to have closer proximity with security measures, though collaboration sometimes encounters resistance. The great symbiosis between the application code and the cloud in Serverless signifies that developers further venture into security-related territories, often causing territorial contradictions during the early stages of Serverless adoption, yet offers a chance to rethink the age-old approach to managing security.\n\nSecurity is best approached as an active facilitator, providing tools and framework, along with multiplying secure by default standards obliging structural and technical modifications in the engineering teams. The integration of approaches like \"Team Topologies\" and CDK could hold the key to drive the requisite amendments. The aim is to shift the perception of security from a hindrance to an enabler enriching development, rather than stifling deployment and eternal inventiveness. To accomplish the same, both security and system engineering sectors require significant changes to support a bottom approach to the secure by design pattern.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "During the early phases of transitioning to a Serverless framework, there are often conflicts regarding territorial ownership. However, these situations offer a chance for reconsidering the conventional methods of ensuring security. \n\nSecurity, ideally, should not be an inhibiting factor but should be an enabler endowing with essential tools and strategies, and empowering teams to create a default secure environment. This demands a paradigm shift in how we perceive security - as an inclusive function facilitating security rather than a restrictive one hindering deployment or innovation.\n\nImplementing this shift necessitates a modification in the structure of engineering teams, and the introduction of technical solutions to vest a secure by design rule from the bottom up. Harnessing the synergies of \"Team Topologies\" and CDK can thrust this transformation.\n\nMoreover, Cognitive load operates as an aspect of friction on teams. The greater the cognitive load, the more ponderous the team's work-rate (often, consequently affecting their gratification negatively). A primary strategy to alleviate cognitive load would be by tailoring our architecture and systems considering the underlying domain and creating services rooted in exclusive subsections of said domain, to allow teams and services to function independently. Such an arrangement permits teams to limit their scope to a certain sector of the total domain complexity.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "This necessitates a restructuring of the engineering team and implementation of a technical solution to facilitate a bottom-up approach to security by design. Both \"Team Topologies\" and CDK can be utilized to achieve this structural change. \n\nThe concept of cognitive load refers to the amount of working memory being used. As the convergence of cloud and application code increases, so too does the reliance on smaller cross-functional teams. Despite the numerous advantages, such as increased autonomy and velocity, improper design of organisational structures and architectures might impose a substantial cognitive load on the teams involved. This can impede productivity and complicate the onboarding of new developers.\n\nWithin team topologies four different type of teams exist:\n1. The Stream-Aligned Team, which ideally segregates the Business Domain into sections (e.g., Bounded Contexts) and delivers a flow of work.\n2. The Enabling Team, which assists Stream-Aligned Teams, removing obstacles and detecting capability gaps.\n3. The Complicated Subsystem Team, which is primarily concerned with complex domain-specific expertise, such as algorithmic and machine learning processes.\n4. The Platform Team, which serves to facilitate the work of the Stream-Aligned Teams via a product-centric approach whilst preserving their independence.\n\nAlong with Domain-Driven Design techniques, Stream-Aligned Teams can focus on separate areas of the domain with services designed accordingly. \n\nPlatform Teams hold the potential to alleviate the cognitive burden imposed on Stream-Aligned Teams by offering foundational services that the latter are no longer tasked with creating. It is beneficial for Platform Teams to visualize themselves as service providers, viewing the Stream-Aligned Teams as their clients.\n\nKey to the success of the Platform Team is its ability to facilitate:\n- Abstraction\n- Encapsulation\n- Composition \n\nThese elements empower the Platform Team to obfuscate complexity through abstraction, assure optimal security via encapsulation, and enable Stream-Aligned Teams to individually combine components into their respective sub-domain cases.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "Complex Subsystem division: Distinct intricate skills for specific areas (like algorithmic, machine learning)\n4. Platform Group: Enhance the flow-oriented teams with a product-related approach keeping their independence intact.\n\nWhen paired with Domain-driven Design methods, Stream-Aligned Teams can manage independent sections of the domain with services structured to match (reference EventBridge Storming).\n\nPlatform Groups could play a key role in decreasing the cognitive burden on Stream-Aligned teams by facilitating fundamental services which don't need to be developed by the Stream-Aligned teams anymore. These Platform groups should consider themselves as service suppliers, treating Stream-Aligned teams as their \"clientele\".\n### What's CDK?\n\nThe AWS Cloud Development Kit, short as AWS CDK, is open-source software framework aimed for defining application resources for cloud using familiar programming languages.\n\nAWS CDK enables programmers to use the programming language they prefer like TypeScript to establish their architecture. It enables programmers to utilize the dynamic expressive attributes of contemporary programming languages, encapsulating principles such as Abstraction, Encapsulation & Composition.\n\nBehind the scenes, CDK formulates recognizable CloudFormation, offering consistent deployment and compatibility with pre-existing deployment tools and practices.",
    "style": "Style: Vary the original vocabulary, tone and prose. Convert it to a less coherent draft style."
  },
  {
    "neutralised_chunk": "The AWS Cloud Development Kit (AWS CDK) is an open-source framework for software development. It is used to define the resources of cloud applications via standard programming languages.\n\nThe AWS CDK allows the use of a preferred programming language, for instance TypeScript, for the definition of the toolkit architecture. Accordingly, it allows the utilisation of modern programming languages characteristics such as Abstraction, Encapsulation and Composition.\n\nAt its core, CDK converts its original structure into cloud formation facilitating repeatable deployments and compatibility with existing tooling and common practices.\n\n\nA Construct in AWS CDK represents the primary elements of AWS architectures. It has all the components needed to generate the CloudFormation essential for the deployment of a specific architecture component.\n\nThere are three levels of constructs and each one builds on top of the other:\n- Level 1: AWS CloudFormation-Only\n- Level 2: Curated\n- Level 3: Patterns\n\n```plaintext\nL1 constructs are directly generated from CloudFormation, which reflects a complete feature interface to generate resources at the lowest level of abstraction.\nconst bucket = new s3.CfnBucket(this, \"MyBucket\", {\n  bucketName: \"MyBucket\"\n});\n\nL2 constructs are based on L1, with added default values, boilerplate and glue.\nimport * as s3 from 'aws-cdk-lib/aws-s3';\n// \"this\" is HelloCdkStack\nnew s3.Bucket(this, 'MyFirstBucket', {\n  versioned: true\n});\n```\n\nLastly, L3 constructs act as patterns for standard use cases, such as a REST API. This often involves multiple resource types composed into an end-to-end framework.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more serious. Convert it to a more serious style. Take the fun out of it."
  },
  {
    "neutralised_chunk": "EventBridge Pipes, developed by Amazon Web Services (AWS), offers an efficient method for transferring data between sources and destinations. In the process, the data can be filtered, enriched, and transformed. Formerly known as the EventBridge Bus, it has recently been extended from being a standalone product to a comprehensive product suite for event-driven application development, inclusive of EventBuses, Pipes, and Schedulers.\n\nWhile constructing high-scale messaging systems that manage vast amounts of parallel messages without losing any due to systems failures can be quite complex, Pipes simplifies this task. It presents a solution that ensures broad horizontal scaling and redundancy with dead letter queues, alongside inbuilt transformations, filtering, and enrichment functionalities.\n\nBy leveraging the benefits of the Cloud Development Kit (CDK), we are able to construct and implement our messaging service right from our favored development environment without needing to navigate the AWS console or compose YAML, as it provides support for Typescript as well.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more serious. Convert it to a more serious style. Take the fun out of it."
  },
  {
    "neutralised_chunk": "The text provided describes the need for a messaging service that can handle high volumes of messages concurrently without losing them due to failures. It mentions that such services often require complex features like routing, filtering, and transforming messages, which can be challenging to implement and maintain. The text then introduces Pipes as a solution that provides industry-leading horizontal scaling, redundancy with dead letter queues, and built-in transformations, filtering, and enrichment capabilities.\n\nThe text also mentions using the Cloud Development Kit (CDK) to build and deploy the messaging service without leaving the chosen development environment (Typescript) and avoiding the AWS console and writing YAML.\n\nAn example application is then described, which is a web app with a fully serverless backend. It requires a microservice that sends an email to the user whenever they change something security-related on their account, such as an address. The account data is stored in a DynamoDB table.\n\nThe messaging service should construct and send an email to the user with identifying information about them and the change that has occurred. The email should include the user's email address, subject line, and body with placeholders for the modified attribute, first name, and last name.\n\nHowever, the DynamoDB table does not store names or email addresses, only a Cognito sub (user id). The Cognito User Pool is used as a single source of truth for security reasons and GDPR compliance. Therefore, the rest of the information, such as name and email address, must be queried from Cognito directly.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more serious. Convert it to a more serious style. Take the fun out of it."
  },
  {
    "neutralised_chunk": "The Cognito User Pool serves as a singular source for both security and GDPR compliance objectives. Consequently, details such as the user's name and email address will need to be elicited from Cognito.\n\nPipes, introduced on December 1, 2022, by AWS, streamlines the process of assembling integrations between event producers and consumers for developers, thus diminishing the requirement for composing integration code. A Pipe's function is to route events automatically from the source to its destination which simplifies the task of creating event-driven applications.\n\nIn the case of EventBridge buses on AWS, these act as a central exchange point for transferring messages or \"events\" between various sources. Envision it as an intermediate dispatcher applying routing rules to direct messages to the desired services. \n\nIn contrast, EventBridge Pipes is purposed to channel events without any interruption from one AWS facility to another. This allows the bridging of event sources aggregated with a specified service without the necessity of any additional connecting code. Moreover, you have the potential to connect a Lambda function or any other equivalent AWS service to refine the events prior to their receiving point.\n\nThe adaptability that Pipes provides makes it an ideal option for use in our messaging service. We have an emerging situation where an event source requires to transfer an event to another AWS service. The particular event demands further information acquisition and incorporation along the course of its flow. With the utilization of Pipes, we hold the capability to assemble every piece of infrastructure with the insertion of minimal additional code.",
    "style": "Style: Vary the original vocabulary, tone and prose. Convert it to a less coherent draft style."
  },
  {
    "neutralised_chunk": "Pipes have the ability to connect each piece of infrastructure with minimal additional code. \n## What is the distinction between EventBridge Pipes and EventBridge Buses?\n\nThink of event buses as post offices for AWS services. Various resources can deliver and obtain messages or \"events\" to each other through this centralised mechanism. Consider it as a third party who uses forwarding plus rules to direct messages to the desired services. \n\nConversely, EventBridge Pipes delivers events from one AWS service directly to another. This feature lets you connect an event origin to a target service without the need for any additional glue code. Further, you have the option to connect a Lambda function or yet another AWS service to modify or \"enrich\" the events prior to their arrival at the target.\n## Architecture Construction\n\nIn order to construct the Pipe, we first need to identify a source that generates the event. In this scenario, the DynamoDB stream of the table serves this purpose.\n### DynamoDB Streams\n\nDynamoDB Streams is an aspect of Amazon DynamoDB that captures alterations to the data in a DynamoDB table almost instantaneously. \n## Infrastructure as Code (IaC)\n\nCDK is our chosen method for IaC for this project. CDK allows us to write IaC in directive programming languages like TypeScript or Python rather than indicative languages like YAML or JSON.",
    "style": "Style: Vary the original vocabulary, tone and prose. Convert it to a less coherent draft style."
  },
  {
    "neutralised_chunk": "We're using the AWS Cloud Development Kit (CDK) as an Infrastructure as Code (IaC) solution for this project. This lets us code in languages like Python and TypeScript instead of YAML or JSON.\n\nOne feature we're using from Amazon is DynamoDB Streams. This captures changes in data from a DynamoDB table in almost real time.\n\nHere's the construction process for our architecture:\n\n- First, decide on the event source. We're using the table's DynamoDB stream.\n\nHere's the sample TypeScript code for creating the table:\n\n```typescript\nconst sourceTable = new Table(this, 'example-table-id', {\n  tableName: 'example-table',\n  partitionKey: { name: 'PK', type: AttributeType.STRING },\n  sortKey: { name: 'SK', type: AttributeType.STRING },\n  stream: StreamViewType.NEW_AND_OLD_IMAGES,\n  billingMode: BillingMode.PAY_PER_REQUEST,\n  removalPolicy: RemovalPolicy.DESTROY,\n  pointInTimeRecovery: true,\n});\n```\nThe Lambda function code for our enrichment handler is below:\n\n```typescript\nexport const handler = async (event: DynamoDBStreamEvent): Promise<string> => {\n  const record: DynamoDBRecord = event.Records[0];\n  if (record.dynamodb?.NewImage == null && record.dynamodb?.OldImage == null) {\n    throw new Error('No NewImage or OldImage found');\n  }\n  const modifiedAttributes: string[] = [];\n  for (const key in record.dynamodb.NewImage) {\n    if (record.dynamodb.NewImage[key].S !== record.dynamodb.OldImage?.[key].S) {\n      modifiedAttributes.push(key);\n    }\n  }\n  if (modifiedAttributes.length === 0) {\n    throw new Error('No changed parameters found');\n  }\n  const userId = record.dynamodb.NewImage?.userId.S;\n  if (userId == null) {\n    throw new Error('No userId found');\n  }\n  const user = await getUser(userId);\n  return JSON.stringify({\n    ...user,\n    modifiedAttributes,\n  } as MessageBody);\n};\n```",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more casual. Consider using a bullet point format."
  },
  {
    "neutralised_chunk": "The management of risks associated with generative artificial intelligence (GenAI) systems is a critical consideration, particularly in regard to the supply chain involved in their development and deployment. The supply chain encompasses various stages, including data acquisition, model training, deployment, and ongoing monitoring and maintenance. Each stage presents unique challenges that necessitate careful attention and mitigation strategies.\n\nDuring the data acquisition phase, it is crucial to ensure the quality, relevance, and diversity of the data used for training GenAI models. Biased or low-quality data can lead to flawed models that perpetuate harmful biases or produce inaccurate outputs. Rigorous data curation processes, including techniques like data cleaning, debiasing, and augmentation, are essential to mitigate these risks.\n\nThe model training stage involves the use of complex algorithms and computational resources to develop GenAI systems. Risks in this stage include unintended model behavior, such as amplifying biases present in the training data or generating outputs that violate ethical or legal norms. Robust model evaluation, testing, and validation procedures, as well as the incorporation of ethical principles and guidelines, are necessary to address these concerns.\n\nDuring deployment, GenAI systems may interact with various stakeholders, including end-users, third-party applications, and other AI systems. Risks at this stage include potential misuse, security vulnerabilities, and unintended consequences arising from the interaction between GenAI systems and their environment. Implementing robust security measures, monitoring mechanisms, and clear guidelines for responsible use are crucial to mitigate these risks.\n\nOngoing monitoring and maintenance are essential to ensure the continued safe and effective operation of GenAI systems. As these systems interact with dynamic environments and new data, their behavior may drift or become outdated, leading to potential risks. Regular model updates, continuous monitoring, and incident response plans are necessary to address these challenges.\n\nEffective risk management in the GenAI supply chain requires a collaborative effort involving developers, researchers, policymakers, and end-users. Establishing clear governance frameworks, fostering transparency and accountability, and promoting responsible innovation are essential steps towards mitigating the risks associated with GenAI systems while harnessing their potential benefits.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "# The Relationship of GenAI Risk and Supply Chain\n\n## Setting the Scene\n\nIn a short time frame, Generative AI (GenAI) has grabbed attention worldwide through high-profile applications such as ChatGPT, Midjourney, and DALL-E. People are applying these pioneering applications after their first encounter and have begun to integrate them into several business areas. One of the tools, known as Copilot, developed by GitHub and built on a GPT model, is now widely used throughout my engineering team. On top of previous databases, we have developed several productivity tools and received inquiries from clients looking to get on board with this technology trend. Current applications are predominantly for customer service and data recovery fields, but there are plenty who are eyeing applications in the creative and knowledge sectors as well.\n\nPublic attention is not only on the advantages of integrating these technologies, but security and regulation issues are also being scrutinised. ChatGPT's creator, open-source AI development company OpenAI, has been reviewed by global regulatory departments. Two major investment banks, JP Morgan and Goldman Sachs, have prohibited the use of ChatGPT due to concerns about data security and commercial rights issues. In response to wrongful assumptions presented by ChatGPT and alleged copyright violations, The Federal Trade Commission (FTC) of the U.S. is now calling for a declaration of how each underlying model was trained to use data by ChatGPT.\n\nThe fundamental technology, Large Language Models (LLMs) from which GenAIs are formed, isn't inherently dangerous. However, due to their need for a large number of data and uninterrupted learning, problems can arise if they are not managed properly. While it's plausible to consider that commercial interests might be gleaned from petabytes of openly accessible material given during public LLM training exercises, it also brings up questions of generated data owner rights. To top it off, any new interactions received during training further improves the model, which proposes any data given by a user can be utilized. For instance, if syncing a free-to-use GPT application, air marketing secrets firsthand to the application itself concerning an undisclosed launch campaign by a multinational soft drinks brand, a senior executive of another competitive brand could also be fishing for the same information to new product launches using the same application itself. Conceivable concerns also arise when individuals begin using GenAI applications to transcribe every business meeting.\n\nA large body of my work within GenAI is helping clients comprehend and scale down associated risks. The straightforward interpretation offers risks that can emerge anywhere throughout the GenAI applications supply chain, from model training, basic model selection right through to perfecting setting questions, settings, hosting, fine tuning, and integrating. Analyzing, declining, and demonstrating these risks will equate to businesses reaping GenAI benefits while bypassing law-suit chances, societal impacts, and brand identity threats.",
    "style": "Style: Vary the original vocabulary, tone and prose. Convert it to a less coherent draft style."
  },
  {
    "neutralised_chunk": "The increasing usage of GenAI application automations for note harvesting during business meetings presents a salient issue.\n\nIn most tasks associated with GenAI, the main objective is to assist clients in both comprehending and reducing potential threats. Potentially threatening situations may occur at various stages of the GenAI software procession, ranging from model instruction and choosing an elementary model, to fashioning prompts, arranging contexts, hosting, delicately fine tuning parameters, and interlocking segments. Recognising, reducing, and chronicling these possible issues will enable corporations to harness the capabilities of GenAI, while simultaneously sidestepping potential legal, social, and status-related hazards.\n\nFundamental Model Instruction/Selection Contemplations\n\nLong-Form Models (LLMs) are honed using profound studying methodologies on immense data repositories, outscaling the standard volumes used previously in profound learning modelling. The initial data source and the corresponding licences and assurances pertaining to it are pivotal factors or 'control points' in averting oblivious violations of Intellectual Property (IP) rights. For the majority of corporations, the application of GenAI means engaging with third-party foundational models (FMs) that were developed externally. Owing to the overwhelming magnitudes and convolument of third party FMs, tracing individual elements back to their original sources becomes a near impossible task, which leads companies into a sphere of obscurity in defending themselves against IP claims. Employing the classic phrase 'input of poor-quality data results in the production of equally lacklustre output', one can extend this notion to IPs as 'Ingress of IP results in a resultant IP threat exponentiation'. Threat reduction can be attained most effectively through conducting rigorous analytical tests for assurances on the third-party foundational models being utilised, or alternatively, initiating a stringent data validation method should there be a need to construct a FM with an element of component specificity.\n\nIn addition to the IP-related potential risks, when one undertakes the development of their own FM, there arises the emergence of data-related threats. For instance, a vast financial entity making use of an internal library of data could decide to train a GPT PM and utilise it to enhance their employees' work efficiency. Data combinations sourced from the CRM or proprietary knowledge banks could be easily roped into the zone of influence of the used datasets for educational purposes. Consequently, an examiner might compare customer request data sets with previously-entered requests of the same nature, thereby potentially gaining unauthorised access to sensitive customer-related data imperceptibly. Crucially, openly festivities nature there's a control bypassing occurring that circumvents standard Client Access Protocols that are instituted into everyday data handling systems. Knowing the pure unaltered progression of the data and comprehending the reasons for its very creation is fundamental to the prerequisite controls needed to guard against erroneous leaks, even in the internal-only information world.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "Large financial institutions may utilize internal knowledge banks and customer relationship management (CRM) data to train language models for assisting staff. However, this process raises concerns about data privacy and unauthorized access. Analysts querying the model could potentially access sensitive client information they should not have access to, bypassing traditional role-based access controls. To mitigate risks, it is crucial to thoroughly vet the provenance, purpose, and required permissions for data used in model training. This ensures that no sensitive or unauthorized information is inadvertently incorporated, preventing the creation of a \"leaky\" model that could expose sensitive data, even if only used internally.\n\nThe process of fine-tuning language models for specific use cases involves further training a pre-trained model on a smaller, domain-specific dataset. While the data volume is typically lower than initial training, making auditing more manageable, it is still essential to implement robust processes for vetting the provenance, purpose, and permission level of the fine-tuning data. Teams should be educated on these processes, and all training activities should be clearly documented for audit and accountability purposes.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more serious. Convert it to a more serious style. Take the fun out of it."
  },
  {
    "neutralised_chunk": "The technique known as \"Parameter Efficient Tuning\" can potentially enhance efficiency by training a subset of weights. The downside to the process, however, is creating an avenue for the model to become tainted with intellectual property-violating data sets or confidential material. The bright side is that the quantity of data employed for refining is usually drastically lesser than that utilized in foundational model training, rendering the auditing process simpler and less prone to errors. Nevertheless, it is a necessity to establish robust procedures concerning the origin, intent, and access level of data employed for refining, along with educating teams and streamlining any training documentation for conceivable auditing and accountability purposes.\n\nRegarding hosting matters, the place where model perches for training and inference sessions is significantly important, particularly when businesses adapt base models by means of high-value intellectual property. In absence of secure hosting, the intellectual property and sensitive details utilized for model training may be jeopardized. Also, there is the potential for prompts and outputs to contain confidential information, which necessitates that the round trip from the application layer to the model, and conversely, should be secure and in accordance with data privateness regulations. It's additionally essential to exercise control over where training and hosting of models occur, to assure compliance with data sovereignty laws. Charting out an application's full route of prompts to the outputs, coupled with the place and security duties of any contracting components, is essential to getting a thorough perspective on the hosting facet. Cloud service providers have responded swiftly, making regional segregation and security controls attainable hosting options.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "Furthermore, the suggestions and results might encompass confidential data, necessitating secure passage from the application phase to the model phase and rewind, satisfying data confidentiality laws. Moreover, managing where models are developed and housed to assure adherence to data sovereignty laws is vital. Drafting a map for the complete journey of application suggestions to outcomes, the places, and safety duties of any third-party elements, is crucial for a comprehensive apprehension of your hosting panorama. Cloud resources have proactively delivered accommodation choices with regional seclusion and security constraints.\n## Implementation\n\nIt's vital while granting users admittance to the LLM's outcomes, to mitigate the threat of illusion. An LLM may convincingly and decisively respond with either a solution, even if it's untrue, resulting in vilification legal trouble if individuals are dishonestly implicated in unlawful activities using fake connections to unreal news stories. This produces a risk of slandering and misinformation affecting crucial business operations. Despite the continuous research on developing new FM that are less predisposed to illusions, it is important to enlighten final users and blend user interface facets that hint users about the potential for incorrect information. Additionally, safeguarding measures must be installed to shield against harmful utilization of public-proxy applications, like producing false news or inappropriate content (profanity, violence, hate speech, etc.). There are also FM under development with inherent safeguard like Amazon's Titan FM's. \n## Surroundings Embeddings\n\nEmbeddings vectors grant LLM's the ability to research for equal data. Vector embedding is spawned from application data and stored as vectors inside a database. Consequently, a user's inquiry can then be switched to a vector, allowing for a similarity quest providing the LLM with more details and framework to the prompt.",
    "style": "Style: Vary the original vocabulary, tone and prose. Convert it to a less coherent draft style."
  },
  {
    "neutralised_chunk": "The AWS Cloud Development Kit (CDK) is a framework that allows developers to define cloud application resources using programming languages like TypeScript or Python. This approach enables developers to leverage their existing knowledge and build cloud infrastructure rapidly. By utilizing provided high-level components designed with best practices, the CDK abstracts complexity away from developers, and by encapsulating resources into constructs, it facilitates scalability and management of complex resources.\n\nInfrastructure as Code (IaC) is a machine-readable representation of all the resources required for a cloud application. It offers benefits such as version control, a single source of truth, reusability across multiple environments, typing for suggestions and autocomplete, and the ability to test the IaC.\n\nThe CDK distinguishes itself from other IaC options like Serverless Framework, Terraform, CloudFormation, and AWS SAM by allowing developers to write IaC in imperative programming languages like TypeScript and Python, rather than declarative formats like YAML or JSON. These languages offer superior development experiences, with robust code editors, formatters, syntax checkers, and linters, making the code more readable and comprehensible to developers.\n\nThe CDK code is transpiled into CloudFormation templates, AWS's YAML or JSON IaC format. During this 'synth' process, additional errors in the IaC can be detected, similar to compile-time errors in compiled languages like C or Rust. This early error detection saves developers time by avoiding failed deployments.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more serious. Convert it to a more serious style. Take the fun out of it."
  },
  {
    "neutralised_chunk": "The development experience is streamlined and efficient, as many errors can be detected through static checks performed by the editor. The code is also more easily readable and comprehensible to developers written in these familiar languages than in YAML.\n\nThe CDK code is transpiled into CloudFormation templates, AWS's YAML or JSON IaC format. During this 'synth' process, further errors in the IaC can be identified \u2014 these can be likened to compile-time errors in compiled languages like C or Rust. This means developers do not have to wait for a failed deployment to catch such errors, saving time.\n\nThis approach differs from Terraform's API-based method, where infrastructure is provisioned in a cloud provider agnostic language, and plugins are used to interact with Cloud and SaaS providers. The benefit of this approach is that migration between cloud providers does not require a complete rewrite of the IaC. However, with serverless architectures, cloud-provider-specific constructs are likely already being utilized. Terraform has also released 'CDK for Terraform', which is an AWS CDK equivalent that transpiles to Terraform rather than CloudFormation. However, this is currently less mature than AWS CDK.\n\nCDK additionally provides libraries to write assertion and validation tests with familiar testing tools such as jest, further moving the point at which errors are caught to earlier in the development cycle. CDK also adopts the paradigm of encapsulation with its 'constructs'. This allows specific resource provisioning to be wrapped into a simple, reusable package that can be used elsewhere.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more serious. Convert it to a more serious style. Take the fun out of it."
  },
  {
    "neutralised_chunk": "The given text discusses serverless architectures, Terraform, and AWS Cloud Development Kit (CDK). It provides an overview of CDK, its installation process, and the basics of working with CDK.\n\nThe text can be neutralized as follows:\n\nServerless architectures often involve adopting cloud-provider-specific constructs. Terraform has introduced 'CDK for Terraform', which is an equivalent to AWS CDK that generates Terraform configurations instead of CloudFormation templates. However, 'CDK for Terraform' is currently less mature compared to AWS CDK.\n\nAWS CDK offers libraries for writing assertion and validation tests using familiar testing tools like Jest, allowing errors to be caught earlier in the development cycle. CDK also follows the encapsulation paradigm with its 'constructs', enabling developers to package specific resource provisioning into reusable components for use elsewhere.\n\nTo install AWS CDK, the AWS CLI must be installed and configured with an appropriate account. After meeting this prerequisite, CDK can be installed globally using 'npm i -g cdk'. A new CDK project can be created from scratch using CDK's TypeScript template by executing 'cdk init app --language typescript' in a new directory.\n\nThe main files in a CDK project include 'lib/cdk-demo-stack.ts', which defines a CDK stack corresponding to a CloudFormation template for provisioning resources, and 'bin/cdk-demo.ts', which defines the CDK app. An app can contain multiple stacks, allowing for modular deployment of services across separate stacks.\n\nThe code snippet 'env: { region: 'eu-west-1' }' is likely part of the CDK configuration, specifying the AWS region for resource deployment.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "The text has been transformed into a neutral and plain version while preserving the core meaning and factual information:\n\nThere are two primary files to discuss. First, `lib/cdk-demo-stack.ts` creates the initial CDK stack. A stack corresponds to a CloudFormation template, which provisions the required resources for applications and services.\n\nNext, `bin/cdk-demo.ts` defines the CDK app. An app can contain multiple stacks, and separating services into stacks decouples their deployments. This means if changes are made to only one stack, only that stack needs to be redeployed.\n\nThe code snippet `env: { region: 'eu-west-1' }` is provided.\n\nTo create a Lambda, a `lambdas` directory is made to store the code. Inside this, `getLunchSpots.ts` is created containing a handler function that returns a 200 response with a serialized lunchSpots object.\n\nA Lambda resource is then created in the stack file directly from the TypeScript handler function. The code snippet `// This creates a Node.js Lambda function with the name 'getLunchSpots', using the handler function found in lambdas/getLunchSpots.ts` is provided.\n\nTo deploy the changes, `cdk deploy` is run. A prompt may ask to confirm the deployment. The terminal output will include an 'Outputs' section with an entry 'CdkDemoStack.sohoLunchSpotsEndpoint...', which is the endpoint of the API Gateway API. Making a GET request to this URL + /lunch-spots will return the JSON object specified in the Lambda function.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "LocalStack: A Brief Overview and Its Benefits for Developers \n\nLocalStack emulates cloud services, particularly those from AWS, to run exclusively on a personal laptop without the need for connection to any remote cloud sources.\n\nThe use of LocalStack may be beneficial for various categories of users:\n\n1. Beginning users in need of practice functionality for AWS utilities but lack the required credit card for AWS registration.\n2. Educational users who wish to gain practical knowledge of AWS services without exposure to financial costs.\n3. Practising professionals who want to examine or trail infrastructure configurations offline or on personal equipment, without setting up a parallel cloud environment for testing and smoothly transferring to the main production environment of AWS once finalisation and optimisation are achieved.\n\nThis last scenario especially presents a clear benefit of LocalStack, as one could unknowingly face considerable financial costs from improve their skills by setting up a separate AWS environment without understanding cost plans or setting up budget alerts.\n\nLocalStack's Emulation of AWS Services \n\nLocalStack at present, replicates over 60 AWS cloud services, most of which are free of charge. An illustrative example would be creating an S3 bucket and simulating the deployment of a rudimentary webpage to demonstrate the (italics)functional similarity(end italics) between an action performed on the AWS S3 service and its emulation within LocalStack.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "LocalStack is a tool that allows users to emulate AWS services locally on their machines. It provides a convenient way for students to gain hands-on experience with AWS services without incurring any costs. Additionally, professionals can utilize LocalStack to troubleshoot or test their infrastructure configurations offline before seamlessly transitioning to the main AWS production environment once everything is optimized.\n\nLocalStack can be installed on a personal computer through two methods: using PIP or running it with Docker. For this article, the first option of installing LocalStack using PIP was chosen. The AWS Command Line Interface (CLI) was also installed to interact with the emulated services.\n\nAfter installing the AWS CLI, it was configured with test credentials and the default region set to us-east-1. LocalStack was then started on the localhost port 4566 using the command 'localstack start -d'.\n\nTo interact with LocalStack and emulate AWS services, the AWS CLI or SDK needs to be configured to point to the LocalStack endpoint URL. Alternatively, a tool called 'awslocal' can be installed, which automatically configures the CLI to use the LocalStack endpoint URL, eliminating the need to manually specify the '--endpoint-url' option for every command.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "Please initiate LocalStack via a terminal. The command below will accomplish this on localhost port 4566.\n\n```shell\nlocalstack start -d\n```\n\nCoordination with LocalStack will be necessary when simulating AWS services. Accessing LocalStack can be eased by adjusting your AWS CLI or SDK to aim for the LocalStack endpoint URL. This renders the need to define the `--endpoint-url` when issuing commands unnecessary. \n\nAn alternative approach would be to implement a tool named \"awslocal\". This tool serves as a shell for the AWS CLI compatible with LocalStack. Usage of this tool results in the automatic configuration of the CLI to operate with the LocalStack endpoint URL, reducing the need for manual input of the `--endpoint-url`.\n\nHere are some examples of LocalStack usage:\n### Example 1 - Creation of S3 Bucket \n\n```shell\naws --endpoint-url=http://localhost:4566 s3api create-bucket --bucket testbucket\n```\n### Example 2 - Hosting a Static Website \n\nIf it becomes necessary to act as a host for a static website through the S3 bucket which has been previously created, one must develop two html files: `index.html` and `error.html`. Store both these files in a directory labeled `website`. \n\nThe `index.html` will act as the principal access gate to your website, establishing its content and structural organization. The `error.html` will be employed to generate personalized error pages that emerge during HTTP errors such as 404 (Not found) or 403 (Forbidden). \n\nIt is possible to assign a bucket policy and grant unrestricted access to its entirety. Thus, establish a file bearing the name `bucket_policy.json` and insert the code provided below.\n\n```json\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"PublicReadGetObject\",\n            \"Effect\": \"Allow\",\n            \"Principal\": \"*\",\n            \"Action\": \"s3:GetObject\",\n            \"Resource\": \"arn:aws:s3:::testbucket/*\"\n        }\n    ]\n}\n```\n\nSynchronize the website directory with the S3 bucket through the application of the command given below.\n\n```shell\naws --endpoint-url=http://localhost:4566 s3 sync .\\website\\ s3://testbucket\n```\n\nIn conclusion, provisioning of the static website hosting through the bucket has been completed and configurations of the index and error documents made. \n\nThe functional, hosted website is now available on the emulated S3 bucket. This site can be accessed via the following URL: `http://testbucket.s3website.localhost.localstack.cloud:4566/`.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more serious. Convert it to a more serious style. Take the fun out of it."
  },
  {
    "neutralised_chunk": "DynamoDB is an AWS service providing a fully managed NoSQL database renowned for its high scalability and minimal latency. While it is accessible publicly, user data is secured via AWS IAM. The principles of zero-trust networking stress the critical need for caller authentication rather than relying heavily on trust based solely on network perimeters. Although security of data on DynamoDB does not strictly need network security, augmenting IAM authentication and authorization with network security has its advantages, and at times, becomes a requirement for regulatory compliance or to meet chief security officer's demands. This discourse details how to restrict DynamoDB table access exclusively to a VPC.\n### Is it possible to simply use a VPC endpoint?\n\nA VPC endpoint for DynamoDB facilitates a connection between your VPC and DynamoDB, making it possible for traffic to move from your VPC to DynamoDB even without a public IP address. It is generally considered secure since it maintains distance from the public internet. However, as far as the present knowledge suggests, the traffic between a VPC and DynamoDB (via a NAT Gateway) unlikely goes via the public internet infrastructure, even though a public IP address is required so that DynamoDB knows the destination for the response. What is significant is that a VPC endpoint does not prevent access to the DynamoDB table from outside your VPC.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more serious. Convert it to a more serious style. Take the fun out of it."
  },
  {
    "neutralised_chunk": "- You can create a VPC endpoint for DynamoDB to connect your VPC to DynamoDB without needing a public IP.\n- Traffic from a VPC to DynamoDB (through a NAT Gateway) doesn't traverse over public internet infrastructure anyway. \n- A VPC endpoint doesn't stop others from accessing the DynamoDB table from outside your VPC.\n\n- DynamoDB doesn't support resource policies. \n- You can use the aws:SourceVpc or aws:SourceVpce conditions in IAM policies to ensure access is only from a VPC or VPC endpoint.\n- But it's difficult to ensure everyone follows this requirement.\n\n- An easy and reliable way is to use Service Control Policies (SCPs) with AWS Organizations.\n- SCPs can enforce rules across your AWS accounts and resources.\n- You can create an SCP that requires the aws:SourceVpc or aws:SourceVpce condition for DynamoDB access.\n- This way, feature teams can create their own IAM roles, but they must comply with your VPC requirements.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more casual. Consider using a bullet point format."
  },
  {
    "neutralised_chunk": "DynamoDB does not have the capability to support resource policies. However, one can employ either aws:SourceVpc or aws:SourceVpce stipulations in IAM policies as a workaround. These conditions restrict IAM user/role to access DynamoDB from only a Virtual Private Cloud (VPC) or via a Virtual Private Cloud endpoint, thus fulfilling the requirement. Ensuring all stakeholders adhere to the necessary IAM principles can be complex. Ideally, teams should be able to establish their individual IAM roles for their corresponding Lambda features or containers. The challenge is preserving the compliance guidelines. Possible solutions include adopting Service Control Policies (SCPs) through AWS organizations.\n\n## Service Control Policies (SCPs)\nSCPs facilitate the prevention of certain actions in member accounts within an AWS organization, apart from the master account. For instance, specific activities can be prohibited in all sectors except us-east-1 or eu-west-1, as in these locations the application is positioned. Creation of EC2 examples can also be restricted. These controls are widely invoked as protective measures against cryptojacking. An illustration would be the use of SCP to repudiate any requests made towards DynamoDB that are not sourced from within a selected Virtual Private Cloud.\n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"DenyDynamoDBOutsideVPC\",\n      \"Effect\": \"Deny\",\n      \"Action\": \"dynamodb:*\",\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringNotEquals\": {\n          \"aws:SourceVpc\": \"vpc-xxxxxxxxx\"\n        }\n      }\n    }\n  ]\n}\n```\n\nAs every AWS account has dedicated VPC, the usage of an account-specific SCP is suggestive. In spite of this, the SCP may cause a hindrance to the development course since all actions against DynamoDB tables need to be executed from within the chosen VPC leaving the DynamoDB console inaccessible.\n\nA better arrangement may be to disconnect the SCP from the development accounts providing developers ample liberty and ensuring the sustenance of adherence in production.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more serious. Convert it to a more serious style. Take the fun out of it."
  },
  {
    "neutralised_chunk": "Queues and workers are essential components in serverless architectures for handling asynchronous tasks. The combination of Amazon Simple Queue Service (SQS) and AWS Lambda provides a scalable and cost-effective solution for processing jobs in a serverless environment.\n\nThe basic approach involves creating an SQS queue to receive and store messages (jobs) and an AWS Lambda function to process these messages. Lambda integrates seamlessly with SQS, automatically invoking the configured function when a new message is available in the queue. This setup eliminates the need for polling and allows for automatic scaling based on the incoming workload.\n\nWhile this approach is straightforward, it lacks proper error handling mechanisms. By default, SQS retries failed jobs indefinitely, which can lead to resource wastage and unnecessary costs, especially if the code contains bugs or errors. To address this issue, it is essential to set up a dead-letter queue and limit the number of retries.\n\nA dead-letter queue is a separate queue designed to store messages that could not be processed successfully after a specified number of retries. This allows for easier monitoring and troubleshooting of failed jobs. Additionally, several other configurations need to be fine-tuned, such as setting the visibility timeout, configuring Lambda's maximum concurrency, enabling message batching, and setting up alarms to notify the team when failed messages are present in the dead-letter queue.\n\nTo deploy production-ready queues and workers on AWS, a certain level of CloudFormation configuration is required. This includes defining the necessary resources, such as the SQS queues, Lambda functions, and their respective configurations, as well as setting up the appropriate integrations and error handling mechanisms.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more serious. Convert it to a more serious style. Take the fun out of it."
  },
  {
    "neutralised_chunk": "Here's a neutral version of the provided text:\n\n- Your code may have a bug that causes a job to be retried repeatedly for days, potentially costing money and wasting resources.\n- To handle errors, you need to set up a \"dead letter queue\" (a queue that stores failed messages) and limit the number of retries.\n- You'll need to configure various details, such as:\n    - Setting the dead letter queue to store failed messages for 14 days (the maximum).\n    - Setting up an alarm to send an email when there are failed messages in the dead letter queue.\n    - Adjusting other settings like the SQS visibility timeout, Lambda max concurrency, and message batching.\n- The text includes a sample serverless.yml configuration with best practices, such as setting the worker function timeout, reserved concurrency, SQS event source, batch size, queue visibility timeout, message retention period, and redrive policy.\n- Adding an alarm for failed messages would require around 30 more lines of YAML configuration.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more casual. Consider using a bullet point format."
  },
  {
    "neutralised_chunk": "A Step Function consists of different types of states, including Task, Choice, Fail/Succeed, Pass, Wait, Parallel, and Map states. Each state performs a specific operation within the state machine.\n\nTesting Step Functions is important to ensure proper configuration and prevent future issues. Both unit testing of individual states and integration testing of the overall state machine are recommended, with integration tests simulating the real production environment.\n\nUnit tests focus on testing individual components, while integration tests verify the interaction between various system components. Both types of tests are necessary. Testing individual states is relatively straightforward, while integration tests are more complex but essential.",
    "style": "Style: Vary the original vocabulary, tone and prose. Convert it to a less coherent draft style."
  },
  {
    "neutralised_chunk": "## Contrast between Unit and Integration Testing\nThe primary focus of unit tests is on evaluating single units of code in isolation, while integration tests are designed to see if different components of the system are interacting as expected. It's critical to run both kinds of tests. Individual state tests can be relatively straightforward, but integration tests, although slightly complicated, are essential for overall system stability. \n\n## The Importance of Step Function Testing\nTesting is an essential part of any AWS infrastructure management to ensure proper configuration and to maintain its robustness against possible future issues. Alongside testing the logic of individual states, the overall state machine testing also contributes to system stability. Testing these scenarios using real infrastructure offers a practical simulation comparable to an actual production environment.\n\n## Testing Step Functions Assertions with sls-test-tools\nThe subsequent code samples will illustrate testing for a Step Function termed 'TestAddAndDouble', developed with the capacity to handle two task states; one for adding two numbers and another for doubling the result. \n\n### Ensuring Correct Execution Status of State Machine \nInitially, check the test that transitions to a 'Succeeded' status upon receipt of a valid input. This can be achieved by applying the newly introduced `toHaveCompletedExecutionWithStatus()` assertion. It lets you provide the state machine's executed name and the envisaged status for the execution. We then inspect the latest execution status of the state machine to check if it actually matches the expected one.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "### Verify the correct execution status of the state machine\nLet's begin by testing whether a valid input results in a Succeeded status. This can be done using the `toHaveCompletedExecutionWithStatus()` assertion. You would need to indicate the state machine that's been executed and the anticipated status. The function then verifies if the most recent execution of the state machine conforms to the expected status.\n## Exploring step function assertions with sls-test-tools\nIn the following code samples, we will examine how to evaluate a step function named 'TestAddAndDouble'. This function incorporates two task states: one to sum two numbers and another to double the result.\n### Let's ensure the output corresponds to a valid input\nFor instance, if we provide inputs {3, 6}, the anticipated output is 18. Once again, we can use an assertion, the `toMatchStateMachineOutput` assertion this time. For this operation, you need to supply the state machine that's been executed and the projected output. The function then verifies if the most recent execution of the state machine generated the expected output. Below is an instance of using this assertion:\n\n```javascript\n// Helper: Execute the Step Function until its completed\n```\n\nIt should be reiterated here that we depended on having the step function executed. Instead of having to execute a manual step or making a complicated API call, we've also included a helper function making execution simpler and allowing for waiting until the execution finished. Here's how you can make a call to the helper:",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more serious. Convert it to a more serious style. Take the fun out of it."
  },
  {
    "neutralised_chunk": "The text discusses the concept of an Anti-Corruption Layer (ACL) in the context of building large distributed systems that interact with external systems. It presents a scenario where a company needs to modify multiple sections of its software to accommodate a new third-party provider due to incompatible protocols. The ACL is proposed as a solution to address this issue and facilitate easier transitions in the future.\n\nThe ACL implements the principles of Domain Driven Design (DDD), a software development approach that models software based on the real-world business domain it serves. DDD emphasizes collaboration between technical and domain experts to ensure the software accurately reflects and evolves with the domain. It advocates for a ubiquitous language among stakeholders and organizes the system into bounded contexts, also known as separate services in a distributed system context.\n\nThe ACL follows DDD principles by providing a separation of these services, allowing for effective structuring of business domains and protecting the system's design and language from third-party influence. When executed correctly, the ACL can prevent vendor lock-in and provide better visibility into the system's architecture.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "Here's a neutral version of the text:\n\n- DDD organizes a system into separate bounded contexts (domains) to manage complexity. In distributed systems, these are often called services.\n\n- ACL follows DDD principles by separating services, allowing effective structuring of business domains. It ensures the system's design and language are protected from external influence. \n\n- When executed correctly, ACL can prevent vendor lock-in and provide better visibility into the system's architecture.\n\nAn Anti-Corruption Layer (ACL) is a design pattern that transforms or translates information passing between systems. It's not just for communicating messages, but ensures misinterpreted data cannot corrupt the primary system. \n\n- ACL can be a reusable package or standalone service. \n- Acting as the communication layer between services, it increases decoupling, allowing independent scaling and portability.\n- External systems can be removed without major changes due to low coupling.\n- Decoupled services can be developed independently, increasing flexibility and reducing build time.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more casual. Consider using a bullet point format."
  },
  {
    "neutralised_chunk": "An Access Control List (ACL) can be designed as a reusable package or a standalone service. By acting as the communication layer between services, an ACL separates and increases decoupling, allowing for independent scaling and greater portability of each service. Due to its low coupling with the rest of the software system, an external system, such as a third-party service, can be removed when necessary without significant changes. This provides the added benefit that decoupled services can now be developed independently of each other, increasing developer flexibility and build time.\n\nImplementing an ACL as a service offers the following advantages:\n\n- It promotes the abstraction of protocols required for communication with external services. Building a new layer to handle communication between services can be extremely complex. By encapsulating the ACL as a standalone service, an organization can dedicate a team of experts to build and maintain this service, protecting developers from other domains from needing to understand the intricacies of the ACL domain.\n- Logic used to communicate with external services can often result in a large codebase. Having this within a package can quickly become difficult to maintain, document, and use by consumers. A service simplifies this complexity by allowing the creation of standalone features, such as dedicated infrastructure and organization of logic, in a way that best fits the ACL rather than its consuming services.\n- The ACL service itself can leverage internal packages to reuse code across the service, and the ACL team will have context on these internal development tools. Ideally, having the ACL as a service, rather than a module or package within a service, will provide easier construction, understanding, and correct visibility of this tool at the levels required from those within and outside the ACL domain.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more serious. Convert it to a more serious style. Take the fun out of it."
  },
  {
    "neutralised_chunk": "Having an Anti-Corruption Layer (ACL) within a package can become challenging to maintain, document, and use by consumers. A service simplifies this complexity by allowing the creation of standalone features, such as dedicated infrastructure and organization of logic, in a way that best fits the ACL rather than its consuming services. Additionally, the ACL service itself can leverage internal packages to reuse code across the service, and the ACL team will have context on these internal development tools. Ideally, having the ACL as a service, not a module or package within a service, will provide easier construction, understanding, and correct visibility of this tool at the levels required from those in and outside the ACL domain.\n\nWhen building an Anti-Corruption Layer, its core functionality can be broken down into three mechanisms: Facades, Adapters, and Translators. A Facade provides a simplified interface on top of a complex set of underlying classes. In terms of the ACL, it simplifies access for the client by making the subsystem easier to use. An Adapter allows the client to use a different protocol than the one understood by the system they are trying to interact with. The final element is the Translator. The Translator's job can be seen as a subset of the Adapter's. The translator's job is to translate the actual logic to be recognizable to the client's system.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "Building event-driven serverless architectures with Domain-Driven Design can typically end in services that are divided by business function and communicate asynchronously with something like Amazon EventBridge. This type of architecture has several pros, including loose coupling, easy deployment, easy testing, and less complexity. However, it is not possible to eliminate infrastructure dependencies like the event channel. \n\nThe degree to which these dependencies are addressed determines whether the service manages to be relatively independent, or if it becomes unnecessarily complex. \n\nThis text will show how using well-defined service ports can solve those last few infrastructure dependencies. In doing so, services gain loose coupling, enhanced deployment/testing, and a quickened development velocity.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more casual. Consider using a bullet point format."
  },
  {
    "neutralised_chunk": "The provided text discusses the challenges of cross-cutting infrastructure dependencies in a loosely coupled, event-driven architecture. It highlights the importance of clearly defined service boundaries and managing shared resources to avoid issues such as deployment order dependencies, independent development difficulties, and complex integration testing.\n\nThe text proposes a solution using the concept of \"Service ports\" to resolve these infrastructure dependencies and achieve true loose coupling, independent deployment/testing, and increased development velocity.\n\nThe core message is conveyed in a straightforward manner, focusing on the technical aspects and potential pitfalls of cross-cutting dependencies in a microservices architecture. The tone is informative and objective, akin to a technical article or a university essay.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "The example provided highlights the challenges that arise when multiple services depend on a shared resource, such as the Finance Service. As the number of dependent services grows, the complexity of managing deployments and ensuring consistency across environments (dev, staging, UAT, production) increases. Additionally, this tight coupling hinders independent development and onboarding of new developers.\n\nTo address these concerns, the concept of Hexagonal Architecture, also known as Ports and Adapters, is introduced. This architectural approach promotes the interchangeability of dependencies, simplifying development and testing processes. By defining clear specifications for ports (interfaces), dependencies can be explicitly defined and implemented through adapters.\n\nThe example illustrates the use of Hexagonal Architecture by defining ports for an EventBus and a UserPool. These ports represent abstract dependencies that can be implemented by various adapters, such as an EventBridge EventBus or a Cognito UserPool. This approach allows for flexibility in swapping out implementations based on the environment or specific requirements.\n\nThe provided YAML file snippet demonstrates how these ports can be defined in a Serverless Framework Infrastructure as Code (IaC) configuration. The environment-dependent configuration allows for easy switching between different implementations, supporting scenarios like temporary CI stacks or local development environments.\n\nBy adopting the Hexagonal Architecture principles, the architecture becomes more modular and loosely coupled. This promotes independent development, as services can be developed and tested independently by mocking the required dependencies. Additionally, the explicit definition of ports and their protocols facilitates better documentation and understanding of the system's dependencies.\n\nOverall, the Hexagonal Architecture approach aims to address the challenges of managing dependencies, promoting flexibility, and enabling independent development and testing within a distributed system architecture.",
    "style": "Style: Vary the original vocabulary, tone and prose. Convert it to a less coherent draft style."
  },
  {
    "neutralised_chunk": "The text has been neutralized to a more neutral and plain version while preserving the core meaning and factual information:\n\nThe dependencies could be made more abstract, but for now, it is expected to have an EventBridge EventBus and a Cognito UserPool. A simple representation of these ports can be shown in a Serverless Framework Infrastructure as Code (IaC) YAML file. The file demonstrates that it is environment-dependent, with a clear switch case on the environment. This can be easily overwritten to support a temporary CI stack as per Serverless flow, and switched out manually during local development as needed.\n\nCreating an explicit ports section makes the interfaces clearer and should be further explained in a consistent README section for the search service of the architecture.\n\nRegarding the location of cross-cutting infrastructure dependencies, if they live with a Service, such as the Finance Service, they will be deployed with it. However, this may not be appropriate, as a change to the Finance Service does not necessarily mean there should be a deployment of the EventBridge or Cognito User Pool.\n\nThe Ports have mitigated some of the interdependence that would be created, but that does not mean a host Service should be arbitrarily chosen. Instead, these should be considered as Services, albeit simple ones due to the abstraction provided by the Cloud provider over Event Bussing and User Authentication.\n\nWhile it may be tempting to create a \"global,\" \"central,\" or \"infra\" repository and place any cross-cutting dependency into it, this should be avoided as it will quickly become a dumping ground for anything needed by more than one Service, leading to a mess. Instead, very small Services should be created for each cross-cutting dependency, containing the Infrastructure as Code configuration, the CI/CD Pipeline, and any integration tests.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "Large Language Models' (LLMs) increasing popularity has led to the development of applications based on Generative AI (GenAI). For more imaginative results, these applications utilise third-party LLM services.  Developers, with the help of GenAI applications, have the ability to use AI to generate content, code, or problem-solve in original methods. Regardless of the differences, developers rate these applications on the basis of basic software development principles, such as version control, code review, and rigorous testing. In this discussion, specific difficulties involved with GenAI applications testing will be explored, with attention drawn to the strategies used to overcome these obstacles.\n\nPrompt-centric applications are different from conventional applications in that the primary value is derived from interactions with third-party LLMs rather than the application's code itself. The primary implications of this different approach include elements of uncertainty within LLMs operations. Identical models could generate slightly different results despite similar prompts. The model's response to any given prompt cannot be accurately predicted by developers. Further adding to the complexity, models often provide responses as informal or unstructured plaintext. Creating efficient prompts requires a short feedback cycle to analyse the quality of an LLM's reply. Adjusting prompts within GenAI applications can potentially result in negative side effects and cause behaviour deviations. Therefore, implementing a rigid testing mechanism is crucial for transitioning from proof of concept to a production application.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "For a given prompt, executing the same model twice may yield slightly dissimilar results. It is impossible for developers to accurately anticipate a model's response to a certain prompt. The model's response is also derived in a form of unstructured or loosely structured plain text. Quick feedback loops are vital for effective prompt engineering, allowing for the better assessment of LLM responses. Modifications to prompts in GenAI applications could also have unexpected effects and potentially lead to a decline in the behaviour of the application, emphasizing the importance of a robust testing method to link the theory and production applications.\n\n## Formulating Test Cases and Developing Test prompts\n\nIdentifying areas in need of significant quality assurance is the initial step before examining an application. There's no universal solution to this query as it is contingent on the specificities of the application in question. In our instance where we worked with Code Review GPT, our attention was primarily placed on the tool's review consistency. We focused on a list of frequent programming errors to observe this consistency, which the tool should recognise. Each entry on this catalogue was designed as a test case, assuring the software would consistently spot and comment on this code in the future. The application generated example code snippets derived from the plain text descriptions of the test cases to run these scenarios.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more serious. Convert it to a more serious style. Take the fun out of it."
  },
  {
    "neutralised_chunk": "The correct answer varies depending on the specific application, and does not have a universal solution. Concerning Code Review GPT, the main interest is the predictable nature of the tool's inspections. This reliability was quantified by concentrating on the standard coding faults that the instrument must identify. Each point on this catalogue served as a test case, with the goal of the appliance systematically spotting and noting these patterns for a long time. These cases were implemented into the application by crafting exemplar code snippets based on written descriptions of these instances.\n## Assessing the Quality of AI Answers\nThe final step involves assessing the produced review's quality by the LLM. Using snapshots or reference points for assessment, it's possible to accurately measure the reviews. By pitting the test results against these snapshots, a judgment on the test's successfulness can be achieved by comparing the semantic similarity of the replies. This method simplifies the challenging task of analyzing the quality of AI responses into side-by-side comparison of two scripts, making the GenAI application's coherence and performance more manageable to gauge.\n## Making the Test Sequence Universal\nThe automatic threshold testing series, including the creation of test prompts, plugging them into the model and calculating the similarity quotient with the blended snapshots, is a concept that could be beneficial in various GenAI implementations.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more serious. Convert it to a more serious style. Take the fun out of it."
  },
  {
    "neutralised_chunk": "Image object detection is a process that identifies and extracts information about entities present in an image. This involves detecting objects, activities, locations, animals, products, and more. Object detection has various applications across different industries like banking, insurance, social media, dating apps, news agencies, and financial technology companies.\n\nA social media startup required an image object detection feature. Users could select and submit their favorite photos to be featured on the startup's social media pages. The marketing team needed a way to search through the image submissions for specific themes like ocean photos, popular landmarks, animals, music concerts, and others.",
    "style": "Style: Vary the original vocabulary, tone and prose. Convert it to a less coherent draft style."
  },
  {
    "neutralised_chunk": "The process of navigating through the image submissions based on particular patterns such as the ocean, famous landmarks, animals, and music concerts is needed by the social media marketing team.  \n## Explanation of Image Object Detection\nIn a straightforward language, image object detection identifies and retrieves data about entities in a specific image. This incorporates detecting objects, activities, places, animals, products. The applications of image object detection can be found in many sectors. It is utilized in important industries like banks, insurance firms, social media and dating applications, news companies, and financial technology in different ways.  \n## Deep Learning in Image Analysis\nClassifying and analyzing images based on their content is not a simple task. Human eyesight is noteworthy, and creating a software that can replicate the human brain's capability to identify objects is extremely difficult. An entire industry of computer vision is committed to this. The process of performing object detection from scratch normally has several steps: gathering and labeling objects within the images, training machine learning models, using the trained models for analysis, optimizing the performance, and then the cycle repeats. Our goal was to develop this function swiftly and evaluate its effectiveness in a production setting as soon as feasible. Furthermore, we did not want to allocate development resources to making a solution from scratch when we could make use of available cloud services. For this, we used Serverless - it powers the entire backend of the startup and works on event-driven principles. This architecture lets us have development teams working solely on creating features that made our social media app different from others. Serverless also assists us in building highly scalable services while only paying for what we use, a crucial factor for an expanding startup. Hence, we used Amazon Rekognition, a complete serverless image and video analysis service, to execute this feature. Rekognition facilitated the development of this complex yet significant workflow in a few hours. Now, let's delve into it.",
    "style": "Style: Vary the original vocabulary, tone and prose. Convert it to a less coherent draft style."
  },
  {
    "neutralised_chunk": "Cue Serverless provides a fully Serverless and event-driven backend architecture for the startup. This approach allows teams of developers to focus solely on features that differentiate the social media app from others. Serverless also enables the development of highly-scalable services, with the added benefit of only paying for the resources used, which is an important consideration for a scaling startup. To achieve this feature, Amazon Rekognition, a fully Serverless image and video analysis service, was utilized. Rekognition enabled the development of this complex and critical workflow in a short period of time.\n\nAmazon Rekognition is an AWS Serverless offering that uses deep learning for image and video analysis. As a fully Serverless service, Rekognition eliminates the need to manage underlying infrastructure complexities, and users only pay for the resources they consume. It provides pre-written software for various image and video analysis tasks. Rekognition offers features such as image label detection, face detection, celebrity detection, content moderation, and text detection. The service abstracts away the complexities of building, training, and analyzing deep learning models, making image and video analysis quick and simple with minimal setup required. There was no need to build and train custom datasets or provision server capacity for scalability, as the integration was the sole focus.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more serious. Convert it to a more serious style. Take the fun out of it."
  },
  {
    "neutralised_chunk": "Rekognition is a service with various features that include image label detection, face detection, celebrity detection, content moderation and text detection. This service takes care of the complex tasks involved in building, training and analysing deep learning models. Set-up is uncomplicated and the analysis of images and video content is efficient. It eliminates the need to create and train our own datasets and provision server capacity.\n\nThe architectural layout is simple. Our mobile application transfers images from user devices into an S3 bucket. This upload in turn triggers a Lambda function, which consequently makes a call to the Rekognition API and stores the results in DynamoDB for future queries.\n\nBelow is a basic representation of our Serverless framework Infrastructure as Code file:\n\n```yaml\n//serverless.yaml\nfunctions:\n  imageLabelDetection:\n    handler: image-label-detection.handler\n    events:\n      - s3:\n          bucket: my-image-bucket\n          event: s3:ObjectCreated:*\n          existing: true\n    iamRoleStatements:\n      - Effect: Allow\n        Action: rekognition:DetectLabels\n        Resource: \"*\"\n      - Effect: Allow\n        Action: s3:GetObject\n        Resource: arn:aws:s3:::my-image-bucket\n```\n\nThe above-mentioned Lambda function invokes the Rekognition API and deposits the results into DynamoDB, but this can be changed according to one's requirements. We retrieve the S3 bucket name and the name of the image from the S3 event and pass those as parameters to the detect labels function of the Rekognition SDK. Two optional parameters (MaxLabels and MinConfidence) specify the requisite confidence level and max number of labels that need to be returned. Therefore, in this instance, a maximum of 20 labels are returned and all these labels comply with a confidence establishment of over 80%.\n\nHere is an example:\n\n```javascript\n//image-label-detecion.js\nconst AWS = require(\"aws-sdk\");\nconst rekognition = new AWS.Rekognition();\nexports.handler = async (event) => {\n  const imageDetails = event.Records[0].s3;\n  const bucketName = imageDetails.bucket.name;\n  const objectKey = imageDetails.object.key;\n  const rekognitionResp = await rekognition\n    .detectLabels({\n      Image: {\n        S3Object: {\n          Bucket: bucketName,\n          Name: objectKey,\n        },\n      },\n      MaxLabels: 20,\n      MinConfidence: 80,\n    })\n    .promise();\n  // Send to data store, e.g. DynamoDB\n  // ...\n};\n```",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "The topic of discussion is serverless observability tools that use custom metrics and dashboards through Cloud Development Kit (CDK). One of the significant parts of serverless architectures is adequate logging. While third-party tools offer help, Cloudwatch Dashboards and Alarms possess unused capabilities. This information proposes to outline methods to obtain advanced project-wide observability from within.\n\n\u200bServerless projects, which are more or less cloud projects, are usually made up of various services doing different roles to create a final product. While such flexibility proves to be strong, it's difficult to track issues due to the vast array of moving parts. Owing to such difficulty in maintaining observability, reluctance is observed in serverless adoption, prompting the usage of many pre-made tools to gain better insights. In the past, establishing strong monitoring via AWS services was time-consuming while offering minimal direct gains.\n\nHowever, the AWS Cloud Development Kit (AWS CDK) now changes this process significantly. It's an open-source platform designed to set up Infrastructure as Code (IaC) and deploy it through AWS CloudFormation. Important programming languages for serverless architectures are compatible with CDK, with Typescript being the most noteworthy.\n\nThe Object Oriented principle is used by CDK effectively making it structurally modular and extendible. With this, creating dashboards that are used across services and teams is simplified. Compared to other IaC platforms that utilize YAML syntax, CDK offers convenient methods for creating custom constructs. The resulting ability of automated observability tool creation encourages in-house observability as a solution.\n\nOther existing observability tools maintain their importance. Ready-made solutions seem to be firmer. However, CloudWatch IaC is considered a viable option owing to its power and flexibility, specifically in environments where external tools cannot be used - albeit requiring an investment of time.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more casual. Consider using a bullet point format."
  },
  {
    "neutralised_chunk": "CloudWatch Infrastructure as Code (IaC) lets you make custom dashboards for different services and teams. It's simpler to build custom setups than with other Foundation-creating IaC that use YAML. It makes creating observability tools straightforward and brings automatic standardization to project observability. Existing tools are still useful and are unbeatable when it comes to ready-to-use solutions. But, if you need special observation in a setup where external tools can't be used, CloudWatch IaC offers a solid and adaptable choice if you can allocate the effort.\n\nCDK is evolving and its documentation is not always easy to follow - sometimes leading to wasted time. But after prolonged use, I have managed to build flexible monitoring tools and set up automatic warnings for any issues. This article should help clarify Cloudwatch in CDK and guide you in maintaining your Serverless applications. \n\nThe items we'll discuss are:\n- Cloudwatch Dashboard infrastructure\n- Publishing your custom metrics\n- Making widgets and a dashboard to portray them\n- Setting an Alarm for a Metric\n- Sending an email with SNS when an Alarm rings\n- Keeping an email list in SSM\n- The components of a Cloudwatch Dashboard\n\nA Cloudwatch Dashboard consists of three elements:\n- Metrics \u2013 these are the data you wish to display on the dashboard\n- Widgets \u2013 they are effectively a 'graph' on the dashboard\n- Dashboards \u2013 collections of widgets \nA Dashboard can accommodate numerous Widgets, and a Widget can present several Metrics. An example of multiple metrics in a Widget is coming up.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more casual. Consider using a bullet point format."
  },
  {
    "neutralised_chunk": "Here's a neutral version of the text:\n\n- Setting up a Cloudwatch Dashboard for monitoring Serverless applications using CDK\n- The infrastructure required for a Cloudwatch Dashboard\n- Publishing custom metrics\n- Creating widgets and a dashboard to represent metrics\n- Configuring an alarm for a metric\n- Sending an email via SNS when an alarm is triggered\n- Storing an email list in SSM Parameter Store\n- Components that make up a Cloudwatch Dashboard\n\nA Cloudwatch Dashboard consists of:\n- Metrics - the actual data to display\n- Widgets - graphical representations of metrics\n- Dashboards - a collection of widgets\n\nA dashboard can have multiple widgets, and a widget can visualize multiple metrics.\n\nThere are two types of metrics:\n- Default metrics (like Lambda errors and invocations)\n- Custom metrics\n\nDefault metrics are easy to reference in CDK, and the official documentation provides guidance.\n\nTo create a custom metric, you need to publish a data point using the SDK's putMetricData function. This creates a data point instance, which can then be aggregated into a metric.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more casual. Consider using a bullet point format."
  },
  {
    "neutralised_chunk": "Publishing data at runtime is not considered Infrastructure as Code (IaC), and therefore requires the utilisation of the Software Development Kit (SDK), specifically the putMetricData function. This creates a datapoint instance which can consequently be aggregated to establish a metric.\n\nThe subsequent example demonstrates a method to construct an Error Count metric, wherein a datapoint is created, followed by a metric based on that.\n\nCreating the Data Point:\n\n```typescript\nconst params = {\n  MetricData: [\n    {\n      MetricName: 'Errors Thrown',\n      Dimensions: [\n        {\n          Name: 'Error Code',\n          Value: request?.error?.code.toString() ?? '500',\n        },\n        {\n          Name: 'Function Name',\n          Value: request.context?.functionName ?? 'Unknown Function',\n        },\n      ],\n      Unit: 'None',\n      Value: 1,\n      Timestamp: new Date(),\n    },\n  ],\n  Namespace: `${process.env['STAGE']}`, //STAGE\n};\nawait cloudwatch.putMetricData(params).promise();\n```\n\nCreating the Metric:\n\nTo reference the published Metric in Infrastructure as Code (for its use in Dashboards and Alarms), the creation of the following would suffice.\n\n```typescript\nconst errorRate = new Metric({\n  namespace: stage,\n  metricName: 'Errors Thrown',\n  region: 'eu-west-1',\n  dimensionsMap: {\n    'Error Code': 500,\n    'Function Name': functionName,\n  },\n  period: Duration.minutes(1),\n  statistic: 'Average',\n});\n```\n\nIt is critical to ensure that the metricName, dimensionsMap, region and namespace are identical with the data that has been inputted.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more serious. Convert it to a more serious style. Take the fun out of it."
  },
  {
    "neutralised_chunk": "Creating the Metric:\n\nTo reference the newly published Metric in Infrastructure as Code (IaC) for inclusion in Dashboards and Alarms, utilize the code below.\n\n```typescript\nconst errorRate = new Metric({\n  namespace: stage,\n  metricName: 'Errors Thrown',\n  region: 'eu-west-1',\n  dimensionsMap: {\n    'Error Code': 500,\n    'Function Name': functionName,\n  },\n  period: Duration.minutes(1),\n  statistic: 'Average',\n});\n```\n\nIt's important to ensure that metricName, dimensionsMap, region, and namespace match their initial settings.\nCreating a Dashboard & Widget\n\nOnce the metric is established, building the widget and dashboard follows. Documentation from Cloud Development Kit (CDK) can be helpful in this process.\n\nA dashboard could be created using the code:\n\n```typescript\nconst lambdaDashboard = new Dashboard(this, 'MyDashboardId', {\n  dashboardName: 'MyDashboardName',\n});\n```\n\nThis is the code for adding a widget:\n\n```typescript\nlambdaDashboard.addWidgets(\n  new GraphWidget({\n    title: `${name} Errors`,\n    width: 24,\n    left: [\n      myMetric\n    ],\n  }),\n);\n```",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more serious. Convert it to a more serious style. Take the fun out of it."
  },
  {
    "neutralised_chunk": "Edge Computing involves relocating computing and storage resources closer to end users, aiming to reduce latency, network traffic, bandwidth consumption, geographic distance, energy usage, and power requirements. It represents a shift from the traditional centralized data center/cloud model, where a significant portion of enterprise data processing occurs remotely.\n\nBy 2025, it is predicted that 75% of enterprise data will be generated and processed outside traditional data centers/clouds, compared to approximately 10% currently. Content Distribution Networks (CDNs) pioneered the concept of Edge Computing by distributing data storage across multiple locations nearer to users, reducing data transport time and enhancing performance.\n\nThe maturation of virtualization technologies has enabled not only data storage but also computing resources to be deployed at Edge locations, extending beyond the confines of centralized clouds and data centers. Edge Computing emphasizes real-time and instant data processing, contrasting with the \"Big Data\" focus of cloud computing.\n\nEdge Computing's reduction in latency can facilitate the realization of various applications in the Internet of Things (IoT), Artificial Intelligence (AI), and Machine Learning (ML). Examples include real-time control of autonomous devices, remote surgery, and facial recognition. The advent of 5G and faster connectivity further accelerates the adoption of Edge Computing by reducing communication latency, enabling a new generation of low-latency applications when combined with Edge Computing capabilities.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more serious. Convert it to a more serious style. Take the fun out of it."
  },
  {
    "neutralised_chunk": "- Edge computing allows technologies like real-time control of autonomous devices, remote surgery, and facial recognition by processing data closer to the source.\n- 5G and faster connectivity are driving the growth of edge computing by reducing communication latency.\n- Together, 5G and edge computing can enable extremely low latency, unlocking new application possibilities.\n\n- Serverless allows building apps and services without managing underlying servers.\n- It's an architectural approach to increase agility and reduce costs.\n- You send code to a cloud provider, which runs it in response to events.\n- AWS Lambda is a popular serverless service, enabling event-driven architectures.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more casual. Consider using a bullet point format."
  },
  {
    "neutralised_chunk": "The term \"serverless\" loosely represents a wide range of services, fundamentally promoting an architectural shift aimed at enhancing agility while minimising Total Cost of Ownership (TCO). \n\nEssentially, serverless represents an abstraction whereby details concerning the physical computer are abstracted to allow for a system wherein pure application code if forwarded to a Cloud Provider (e.g. AWS) and executed in response to various events. AWS's Lambda, often cited as synonymous with Serverless, is the most prominent Serverless Service, thus, presenting the Function as a Service option (FaaS) from AWS. Lambda services have one of the more dramatic impacts on application development as the collection of Lambda function triggers has allowed for the evolution of distinct event-driven architectures.\n\nMoving on to the application of Edge Computing in practical scenarios, examples include Smart CCTV cameras, IoT devices, Medical devices and CDNs. The processing shift from centralized locations to the data source is the prevalent feature of edge computing. For instance, in the domain of CCTV systems, patterns of behaviour like a human falling could be detected almost instantly using models that have been trained for such behaviour recognition. Besides accuracy, this also minimises any delay in the recognition of a fall event, which, in an industrial background could halt machine operation instantaneously.\n\nWhile advantageous, Edge computing does come with its set of risks. The security concerns advocating 'smart' devices have escalated with the pervasive adoption of IoT irrespective of sector or industry. Despite this, the scalability potential of Edge computing systems could counterbalance some vulnerabilities like a Distributed Denial of Service (DDoS; yet, it necessitates pervasive alterations in the core computer model to manage operations in less secure conditions.\n",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "Here are some key points about edge computing and its relation to serverless:\n\n- Processing is moving closer to where data is generated, like CCTV cameras doing instant fall detection with low latency\n- Edge computing has security risks from untrusted devices/environments, but serverless scalability can mitigate threats like DDoS\n- Serverless and edge share similar models:\n    - Abstraction from hardware details\n    - Need for secure sandboxing in untrusted environments  \n    - Handling unpredictable real-time data at scale\n- Edge devices have low resources (CPU/RAM/disk) compared to servers, so efficiency is key\n- Serverless tech and practices are well-suited for edge:\n    - Stateless, abstracted runtime environments\n    - Integration with serverless databases/services\n    - Developer experience with serverless patterns\n- Transitioning from serverless to edge is easier than traditional servers/Kubernetes",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more casual. Consider using a bullet point format."
  },
  {
    "neutralised_chunk": "- Serverless has evolved significantly since its inception, with improved tooling from vendors and open-source communities.\n- Developers have established best practices and mental models for working in a Serverless environment.\n- These advancements will enable the growth of Edge computing.\n- Developers are accustomed to coding in stateless, abstract runtime environments and integrating with Serverless databases and third-party services using elegant design patterns.\n- The shift from Serverless compute to Edge compute is relatively minor, whereas transitioning from traditional Server or Kubernetes environments is a much bigger leap in terms of mindset and tooling.\n\n- Lambda@Edge is a CloudFront (CDN) product, not part of the AWS Serverless team.\n- It allows customers to run application code closer to users by executing in the CDN layer.\n- Lambda@Edge is similar to traditional Lambda, with no infrastructure management and pay-per-use billing.\n- It is commonly used for security checks, location routing, data modification, Server-Side Rendering (SSR) of React applications, image transformations, and A/B testing.\n- Code must be initially deployed in the US-East-1 region and is then distributed across hundreds of data centers worldwide.\n- Despite the distribution across numerous locations, deploying Lambda@Edge functions is relatively simple, thanks to years of Serverless community development.\n\n```yaml\n# This code snippet is preserved as-is\n```",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it more casual. Consider using a bullet point format."
  },
  {
    "neutralised_chunk": "Lambda@Edge is a feature of Amazon CloudFront that allows running Lambda functions at the edge locations of the content delivery network (CDN). It is commonly utilized for various use cases such as security checks, location-based routing, context-specific data modification, server-side rendering (SSR) of React applications closer to users, image transformation, and basic A/B testing. \n\nInitially, Lambda@Edge code must be deployed in the US-East-1 region, after which it is distributed across hundreds of data centers worldwide. Despite the distribution across numerous locations and execution in the CDN layer in response to network requests, the process is relatively straightforward and similar to deploying a traditional Lambda function, thanks to the Serverless community's development efforts.\n\nThe Serverless Framework can be used to define the Lambda@Edge function, specify that it is triggered by a CloudFront event, and then deploy it with a single command. The code can be written in Python or JavaScript, and there is no need for manual bootstrapping or virtualization concerns. For example, traffic migration from one S3 bucket to another can be accomplished with a few lines of code. The \"handler\" function and its parameters are familiar to users of traditional Lambda Functions, providing a similar developer experience.\n\nThe primary differences between Lambda@Edge and traditional Lambda Functions are the initial deployment requirement to US-East-1, constraints on execution time, CPU, and memory, and the limitation to only JavaScript or Python (no other supported languages or options for Custom Runtimes). Additionally, deployments and rollbacks take longer due to the distribution time. This demonstrates how the execution model of Serverless complements the execution environment of the edge.",
    "style": "Style: Vary the original vocabulary, tone and prose. Make it read more like a dull university essay. I want to be bored by the end of it."
  },
  {
    "neutralised_chunk": "We have the option to slowly transition traffic between S3 buckets using a few codes. Those familiar with conventional Lambda Functions will recognize the \"handler\" function and its parameters. The development process is similar to creating Serverless applications with Lambda. The only distinction is that it requires initial deployment in US-East-1 and there are more restrictions on execution time, CPU, and memory. Only JS and Python can be utilized, not other supported languages or custom runtime options, and deployment/rollback process is lengthier because of distribution time. This shows how the Serverless execution model works well with the edge execution environment.\n### Firecracker - addressing various issues in Serverless that Edge computing will encounter\n\nAWS created (and released to the public) Firecracker to enhance speed and efficiency in Lambda & Fargate. Firecracker is a technology that facilitates the running of workloads in \"microVMs\". These lightweight virtual machines offer security and isolation while still providing speed and adaptability. The microVMs have a simple design, permitting security and quick start-up times. This results in a lower memory footprint, faster performance and reduced attack surface. The outcomes of using Firecracker includes increased microVM density per machine, improved security, less memory requirements and faster startup times. This is realized by utilizing Linux KVM (kernel-level virtualization with the kernel acting as a hypervisor), Rust as a fast implementation language, and a precise and minimal API. Firecracker was intended to reduce latency in a resource-limited setting for the Serverless FaaS (Lambda). The requirement for low latency, strong security isolation, and tenant density in a resource-limited environment align closely with the needs of Edge computing. The need to run code with low latency in a low trust multi-tenant setting on minimally resourced machines is quite high. Hence the fact that Lambda@Edge operates on Firecracker is not surprising. It's one instance of tech advancements made for Serverless that help Edge computing's development.",
    "style": "Style: Vary the original vocabulary, tone and prose. Convert it to a less coherent draft style."
  }
]